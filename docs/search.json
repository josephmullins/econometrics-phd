[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to this course! Here you’ll find details on the syllabus."
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Syllabus",
    "section": "Topics",
    "text": "Topics\n\nIdentification, Credible Inference, and Marschak’s Maxim\nWe formally define identification and discuss (via examples) what people really mean when they talk about identification and credible inference. We use the Generalized Roy Model to compare identification via functional form to nonparametric identification.\nWe introduce Marschak’s Maxim as a guide for doing empirical model-based research.\n\nReading\nThe two survey articles by Keane (2010) (link) and Angrist and Pischke (2010) (link) - although aging - provide two important perspectives on the issues of credible inference in economics. Low and Meghir (2017) provide a nice review of the advantages of the structural approach.\nThe original paper by Marschak (1953) may be of interest. Heckman and Vytlacil (2007) provide a nice discussion of Marschak’s Maxim in the context of policy evaluation. They introduce (Heckman and Vytlacil 2005; Carneiro, Heckman, and Vytlacil 2011) the Marginal Treatment Effect as a tool for thinking about quasi-experimental estimators and policy evaluation.\n\n\n\nExtremum Estimators\nWe introduce the concept of an extremum estimator and discuss conditions under which this estimator has good asymptotic properties, with specific applications to maximum likelihood, minimum distance, and generalized method of moments estimators. We discuss optimal weighting of the relative efficiency properties of these estimators.\n\nReading\nThis section relies heavily on the Newey and McFadden (1994) chapter of the Handbook of Econometrics. Although not necessary, Hayashi (2011) provides a very thorough treatment of all of these estimators.\n\n\n\nSimulation Methods\nWe introduce simulation methods for the estimation of structural models, including the Simulated Method of Moments, Indirect Inference, and the Bootstrap method for inference.\n\nReading\nYou may find the Horowitz (2001) handbook chapter useful. Cameron and Trivedi (2005) provide a useful discussion of simulation-based estimators in their textbook.\n\n\n\nPanel Data Methods\nWe talk about individual heterogeneity and discuss the use of panel data for detecting individual heterogeneity in data.\n\n\nDiscrete Choice and Dynamic Discrete Choice\nWe review some of the formalities of discrete choice models and consider estimation of these models in the presence of dynamics."
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThere will be 7 problem sets. Your best 5 of these 7 problem sets will be worth 20%. Hence, you can skip two if you want.\nHere is the proposed timeline of due dates. Submissions must be made through Canvas as a notebook (e.g. jupyter or quarto) formatted to html with printed output.\n\n\n\nAssignment\nDue Date\n\n\n\n\nAssignment 1\nMarch 22\n\n\nAssignment 2\nMarch 29\n\n\nAssignment 3\nApril 5\n\n\nAssignment 4\nApril 12\n\n\nAssignment 5\nApril 19\n\n\nAssignment 6\nApril 26\n\n\nAssignment 7\nMay 3"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\nI will provide a link on Canvas to sign up for my weekly office hours."
  },
  {
    "objectID": "recitations/recitation-1.html",
    "href": "recitations/recitation-1.html",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "We are going to work through an example of “identification via functional form” that we will see in class. The model looks as follows:\n\\[ Y = X\\beta + \\alpha D + \\epsilon - \\varphi\\nu \\]\nand\n\\[ D = \\mathbf{1}\\{X\\beta - \\nu \\geq 0\\} \\]\nwhere \\(\\epsilon\\) and \\(\\nu\\) are independent with \\(\\epsilon\\sim\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\) and \\(\\nu\\sim\\mathcal{N}(0,1)\\). This is equivalent to writing:\n\\[ Y = X\\beta + \\alpha D + \\xi \\]\nwith\n\\[ \\left[\\begin{array}{c}\\xi \\\\ \\nu \\end{array}\\right] \\sim \\mathcal{N}\\left(0,\\left[\\begin{array}{cc}\\sigma^2_{\\epsilon} + \\varphi^2 & -\\varphi \\\\ -\\varphi & 1\\end{array}\\right]\\right).\\]\nLet’s start by writing some code to simulate data from this simple selection model:\n\nusing Random, Distributions\n\nfunction sim_data(X ; β,γ,α,φ,σ_ϵ)\n    N = size(X,1)\n    ν = rand(Normal(),N)\n    ϵ = σ_ϵ * rand(Normal(),N)\n    D = (X * γ .- ν) .&gt; 0\n    Y = X * β .+ α * D .+ ϵ .- φ*ν\n    return Y, D\nend\n\nsim_data (generic function with 1 method)\n\n\nLet’s quickly test this function by selecting some default parameters.\n\nN = 1000\nX = [ones(N) 2*rand(Normal(),N)]\nβ = [0., 1.]\nγ = [0.1, 0.5]\nφ = 1.\nα = 0.6\nσ_ϵ = 0.5\n\nY, D = sim_data(X ; β,γ,α,φ,σ_ϵ)\n\n([1.132860301951354, -3.6466921358145283, -2.7966645365786507, 1.074252425709782, 5.251260632435156, 5.333302836895356, 1.048140172519132, -0.012316247760416288, 1.0187652704332761, 3.398234585007554  …  3.1520102965904524, 6.306622263324812, 3.810961616754668, 0.6280107123658936, 0.23612972322392936, -0.735663499435032, 3.6357910280620107, 1.5955063606600457, 2.4628872110429034, 1.1654458069287195], Bool[1, 0, 0, 1, 1, 1, 1, 0, 1, 1  …  1, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n\n\nTo start, let’s think about estimating the parameters \\(\\gamma\\) using maximum likelihood. We will establish the properties of this estimator in class. Here let’s just focus on numerically how to attack the minimization problem. The log-likelihood of the data D given X is:\n\\[ \\mathcal{L}(\\gamma) = \\sum_{n}l(D_{n}; X_{n},\\gamma) = \\sum_{n=1}^{N}D_{n}\\log(\\Phi(X\\gamma)) + (1-D_{n})\\log(1-\\Phi(X\\gamma)) \\]\nLet’s write up this likelihood function.\n\nfunction log_likelihood(D,X,γ)\n    ll = 0.\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            ll += log(cdf(Normal(),xg))\n        else\n            ll += log(1-cdf(Normal(),xg))\n        end\n    end\n    return ll\nend\nlog_likelihood(D,X,[0.,0.])\n\n-693.1471805599322\n\n\nA necessary condition for the maximum likelihood estimator is:\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{n}s_{n}(\\hat{\\gamma}) = 0\\]\nwhere\n\\[s_{n}(\\gamma) = \\frac{\\partial l(D_{n} ; X_{n},\\gamma)}{\\partial \\gamma} \\]\nis often referred to as the “score” of the likelihood.\n\n\nOptimization is most efficient when we have access to the first and second order derivatives of the function. There is a general class of hill-climbing (or descent in the case of minimization) algorithms that find new guesses \\(\\gamma_{k+1}\\) given \\(\\gamma_{k}\\) as:\n\\[ \\gamma_{k+1} = \\gamma_{k} + \\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\gamma} \\]\nwhere \\(Q\\) is the function being maximized (or minimized). \\(A_{k}\\) defines a direction in which to search (providing weights on the derivatives) and \\(\\lambda_{k}\\) is a scalar variable known as a step-size which is often calculated optimally in each iteration \\(k\\). For Newton’s method, the matrix \\(A_{k}\\) is the inverse of the Hessian of the objective function \\(Q\\). Since the hessian can sometimes be expensive to calculate, other methods use approximations to the Hessian that are cheaper to compute.\nSince we have a simple model, we can calculate derivatives relatively easily. However julia and many other languages now provide packages for Automatic Differentiation, which essentially trace all of the operations inside the function and implement the chain rule. It is very quick! Below we’ll compare a hard-coded derivative to this automatic differentiation.\n\nusing ForwardDiff\n\nfunction deriv_ll(D,X,γ)\n    dll = zeros(2)\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            dl = pdf(Normal(),xg) / cdf(Normal(),xg)\n        else\n            dl = - pdf(Normal(),xg) / (1 - cdf(Normal(),xg))\n        end\n        dll[1] += X[n,1] * dl\n        dll[2] += X[n,2] * dl            \n    end\n    return dll\nend\n\nauto_deriv_ll(D,X,γ) = ForwardDiff.gradient(x-&gt;log_likelihood(D,X,x),γ)\n\nd1 = deriv_ll(D,X,γ)\nd2 = auto_deriv_ll(D,X,γ)\n[d1 d2]\n\n2×2 Matrix{Float64}:\n -31.4069  -31.4069\n -29.5423  -29.5423\n\n\nOk so we’re confident that these functions work as intended, but how do they compare in performance?\n\n@time deriv_ll(D,X,γ);\n@time auto_deriv_ll(D,X,γ);\n\n  0.000028 seconds (1 allocation: 80 bytes)\n  0.000043 seconds (6 allocations: 352 bytes)\n\n\nBoth are quite quick but you can see that we’re not losing much with automatic differentiation. In my experience, the gap between the two methods can narrow for more complicated functions.\n\n\n\nSo now let’s try implementing the maximum likelihood estimator using two different gradient-based algorithms: Newton’s Method (which uses the Hessian), and the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm (which updates search direction using chanegs in the first derivative). For maximum likelihood there is also the Berndt–Hall–Hall–Hausman (BHHH) algorithm which uses the following equality that we will derive in class:\n\\[ \\mathbb{E}[\\nabla{\\gamma'}s(\\gamma)] = -\\mathbb{E}[s(\\gamma)s(\\gamma)'] \\]\nThis formula states that the hessian of the log-likelihood is equal to the negative covariance of the score. This is a method that only works when maximizing the log-likelihood. Unfortunately Optim does not provide an implementation of BHHH but it is worth knowing about.\nWhile Newton’s method requires calculation of the Hessian (second derivatives), BFGS and related methods only require first derivatives. Typically, this makes each iteration quicker but will take more time to converge. Let’s test them.\n\nusing Optim\nmin_objective(x) = -log_likelihood(D,X,x) #&lt;- Optim assumes that we will minimize a function, hence the negative\nγ_guess = zeros(2)\nprintln(\" ---- Using Newton's Method ------ \")\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nprintln(\" ---- Using BFGS ------ \")\nres2 = optimize(min_objective,γ_guess,BFGS(),autodiff=:forward,Optim.Options(show_trace=true))\n[res1.minimizer res2.minimizer γ]\n\n ---- Using Newton's Method ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 4.696846008300781e-5\n     1     5.170501e+02     1.388497e+02\n * time: 0.0003650188446044922\n     2     5.074227e+02     2.790597e+00\n * time: 0.0006308555603027344\n     3     5.074194e+02     1.179917e-04\n * time: 0.0008649826049804688\n     4     5.074194e+02     9.096168e-12\n * time: 0.0010650157928466797\n ---- Using BFGS ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 7.295608520507812e-5\n     1     5.238897e+02     1.767467e+02\n * time: 0.00039386749267578125\n     2     5.236553e+02     1.761320e+02\n * time: 0.0006258487701416016\n     3     5.076641e+02     2.366312e+01\n * time: 0.0008478164672851562\n     4     5.074194e+02     7.988743e-02\n * time: 0.0010349750518798828\n     5     5.074194e+02     4.225729e-03\n * time: 0.0012378692626953125\n     6     5.074194e+02     7.219726e-08\n * time: 0.0014328956604003906\n     7     5.074194e+02     2.127742e-13\n * time: 0.0016629695892333984\n\n\n2×3 Matrix{Float64}:\n 0.0335542  0.0335542  0.1\n 0.472962   0.472962   0.5\n\n\n\n\n\nIn class we will see that\n\\[ \\mathbb{E}[Y|X,D] = X\\beta + \\alpha D - \\varphi\\left[(1-D)\\frac{\\phi(X\\gamma)}{1-\\Phi(X\\gamma)} - D\\frac{\\phi(X\\gamma)}{\\Phi(X\\gamma)}\\right] \\]\nwhich allows \\(\\beta\\) and \\(\\alpha\\) to be estimated by virtue of a functional form assumption on the distribution of unobservables. Numerically we can test this by adding the selection correction term to a regression of \\(Y\\) on \\(X\\) and \\(D\\):\n\nusing LinearAlgebra\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\n4×2 Matrix{Float64}:\n  0.313929   0.0\n  1.10537    1.0\n -0.14238    0.6\n -1.44073   -1.0\n\n\nAt first this seems way off, but if we increase the sample size by an order of magnitude we should (depending on sampling error) see that the estimator is consistent:\n\nN = 10000\nX = [ones(N) 2*rand(Normal(),N)]\nY, D = sim_data(X ; β,γ,α,φ,σ_ϵ)\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\nIter     Function value   Gradient norm \n     0     6.931472e+03     8.736635e+03\n * time: 7.987022399902344e-5\n     1     5.159514e+03     1.376363e+03\n * time: 0.0021448135375976562\n     2     5.054556e+03     3.380701e+01\n * time: 0.00448298454284668\n     3     5.054503e+03     2.126288e-03\n * time: 0.005650043487548828\n     4     5.054503e+03     2.817037e-10\n * time: 0.00681304931640625\n\n\n4×2 Matrix{Float64}:\n -0.0458329   0.0\n  0.987815    1.0\n  0.654354    0.6\n -0.960434   -1.0"
  },
  {
    "objectID": "recitations/recitation-1.html#numerical-optimization-and-automatic-differentiation",
    "href": "recitations/recitation-1.html#numerical-optimization-and-automatic-differentiation",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "Optimization is most efficient when we have access to the first and second order derivatives of the function. There is a general class of hill-climbing (or descent in the case of minimization) algorithms that find new guesses \\(\\gamma_{k+1}\\) given \\(\\gamma_{k}\\) as:\n\\[ \\gamma_{k+1} = \\gamma_{k} + \\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\gamma} \\]\nwhere \\(Q\\) is the function being maximized (or minimized). \\(A_{k}\\) defines a direction in which to search (providing weights on the derivatives) and \\(\\lambda_{k}\\) is a scalar variable known as a step-size which is often calculated optimally in each iteration \\(k\\). For Newton’s method, the matrix \\(A_{k}\\) is the inverse of the Hessian of the objective function \\(Q\\). Since the hessian can sometimes be expensive to calculate, other methods use approximations to the Hessian that are cheaper to compute.\nSince we have a simple model, we can calculate derivatives relatively easily. However julia and many other languages now provide packages for Automatic Differentiation, which essentially trace all of the operations inside the function and implement the chain rule. It is very quick! Below we’ll compare a hard-coded derivative to this automatic differentiation.\n\nusing ForwardDiff\n\nfunction deriv_ll(D,X,γ)\n    dll = zeros(2)\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            dl = pdf(Normal(),xg) / cdf(Normal(),xg)\n        else\n            dl = - pdf(Normal(),xg) / (1 - cdf(Normal(),xg))\n        end\n        dll[1] += X[n,1] * dl\n        dll[2] += X[n,2] * dl            \n    end\n    return dll\nend\n\nauto_deriv_ll(D,X,γ) = ForwardDiff.gradient(x-&gt;log_likelihood(D,X,x),γ)\n\nd1 = deriv_ll(D,X,γ)\nd2 = auto_deriv_ll(D,X,γ)\n[d1 d2]\n\n2×2 Matrix{Float64}:\n -31.4069  -31.4069\n -29.5423  -29.5423\n\n\nOk so we’re confident that these functions work as intended, but how do they compare in performance?\n\n@time deriv_ll(D,X,γ);\n@time auto_deriv_ll(D,X,γ);\n\n  0.000028 seconds (1 allocation: 80 bytes)\n  0.000043 seconds (6 allocations: 352 bytes)\n\n\nBoth are quite quick but you can see that we’re not losing much with automatic differentiation. In my experience, the gap between the two methods can narrow for more complicated functions."
  },
  {
    "objectID": "recitations/recitation-1.html#numerical-optimization-using-optim",
    "href": "recitations/recitation-1.html#numerical-optimization-using-optim",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "So now let’s try implementing the maximum likelihood estimator using two different gradient-based algorithms: Newton’s Method (which uses the Hessian), and the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm (which updates search direction using chanegs in the first derivative). For maximum likelihood there is also the Berndt–Hall–Hall–Hausman (BHHH) algorithm which uses the following equality that we will derive in class:\n\\[ \\mathbb{E}[\\nabla{\\gamma'}s(\\gamma)] = -\\mathbb{E}[s(\\gamma)s(\\gamma)'] \\]\nThis formula states that the hessian of the log-likelihood is equal to the negative covariance of the score. This is a method that only works when maximizing the log-likelihood. Unfortunately Optim does not provide an implementation of BHHH but it is worth knowing about.\nWhile Newton’s method requires calculation of the Hessian (second derivatives), BFGS and related methods only require first derivatives. Typically, this makes each iteration quicker but will take more time to converge. Let’s test them.\n\nusing Optim\nmin_objective(x) = -log_likelihood(D,X,x) #&lt;- Optim assumes that we will minimize a function, hence the negative\nγ_guess = zeros(2)\nprintln(\" ---- Using Newton's Method ------ \")\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nprintln(\" ---- Using BFGS ------ \")\nres2 = optimize(min_objective,γ_guess,BFGS(),autodiff=:forward,Optim.Options(show_trace=true))\n[res1.minimizer res2.minimizer γ]\n\n ---- Using Newton's Method ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 4.696846008300781e-5\n     1     5.170501e+02     1.388497e+02\n * time: 0.0003650188446044922\n     2     5.074227e+02     2.790597e+00\n * time: 0.0006308555603027344\n     3     5.074194e+02     1.179917e-04\n * time: 0.0008649826049804688\n     4     5.074194e+02     9.096168e-12\n * time: 0.0010650157928466797\n ---- Using BFGS ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 7.295608520507812e-5\n     1     5.238897e+02     1.767467e+02\n * time: 0.00039386749267578125\n     2     5.236553e+02     1.761320e+02\n * time: 0.0006258487701416016\n     3     5.076641e+02     2.366312e+01\n * time: 0.0008478164672851562\n     4     5.074194e+02     7.988743e-02\n * time: 0.0010349750518798828\n     5     5.074194e+02     4.225729e-03\n * time: 0.0012378692626953125\n     6     5.074194e+02     7.219726e-08\n * time: 0.0014328956604003906\n     7     5.074194e+02     2.127742e-13\n * time: 0.0016629695892333984\n\n\n2×3 Matrix{Float64}:\n 0.0335542  0.0335542  0.1\n 0.472962   0.472962   0.5"
  },
  {
    "objectID": "recitations/recitation-1.html#identification-via-functional-form",
    "href": "recitations/recitation-1.html#identification-via-functional-form",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "In class we will see that\n\\[ \\mathbb{E}[Y|X,D] = X\\beta + \\alpha D - \\varphi\\left[(1-D)\\frac{\\phi(X\\gamma)}{1-\\Phi(X\\gamma)} - D\\frac{\\phi(X\\gamma)}{\\Phi(X\\gamma)}\\right] \\]\nwhich allows \\(\\beta\\) and \\(\\alpha\\) to be estimated by virtue of a functional form assumption on the distribution of unobservables. Numerically we can test this by adding the selection correction term to a regression of \\(Y\\) on \\(X\\) and \\(D\\):\n\nusing LinearAlgebra\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\n4×2 Matrix{Float64}:\n  0.313929   0.0\n  1.10537    1.0\n -0.14238    0.6\n -1.44073   -1.0\n\n\nAt first this seems way off, but if we increase the sample size by an order of magnitude we should (depending on sampling error) see that the estimator is consistent:\n\nN = 10000\nX = [ones(N) 2*rand(Normal(),N)]\nY, D = sim_data(X ; β,γ,α,φ,σ_ϵ)\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\nIter     Function value   Gradient norm \n     0     6.931472e+03     8.736635e+03\n * time: 7.987022399902344e-5\n     1     5.159514e+03     1.376363e+03\n * time: 0.0021448135375976562\n     2     5.054556e+03     3.380701e+01\n * time: 0.00448298454284668\n     3     5.054503e+03     2.126288e-03\n * time: 0.005650043487548828\n     4     5.054503e+03     2.817037e-10\n * time: 0.00681304931640625\n\n\n4×2 Matrix{Float64}:\n -0.0458329   0.0\n  0.987815    1.0\n  0.654354    0.6\n -0.960434   -1.0"
  },
  {
    "objectID": "models/search-model.html",
    "href": "models/search-model.html",
    "title": "Simple Search Model",
    "section": "",
    "text": "In class we discuss the solution and identification of this simple model of undirected search. Time is discrete and indexed by \\(t\\) over an infinite horizon. Workers move between employment and unemployment, have linear utility and cannot save. Let us review the parameters of the model:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n\\(\\lambda\\)\nThe probability an unemployed worker receives a job offer\n\n\n\\(\\delta\\)\nThe probability an employed worker loses their job\n\n\n\\(F_{W}\\)\nThe distribution of wage offers\n\n\n\\(1-\\beta\\)\nThe exponential rate of discounting\n\n\n\\(b\\)\nPer-period utility when unemployed\n\n\n\n\n\nIn class we showed that the optimal decision rule of the worker is characterized by a reservation wage. We derived the reservation wage equation:\n\\[ w^* = b + \\beta\\lambda\\int_{w^*}\\frac{1-F_{W}(w)}{1 - \\beta(1-\\delta)}dw \\]\nand we characterized the steady state rate of unemployment as:\n\\[ P[E = 0] = \\frac{h}{h+\\delta} \\]\nwhere \\(h = \\lambda(1-F_{W}(w^*))\\) is the rate at which workers exit unemployment.\nSimilarly, we showed that the steady state fraction of unemployment durations \\(t\\) is\n\\[ P[t_{U}=t] = h(1-h)^{t} \\]\nLet’s write some code to solve the reservation wage equation, starting with code to evalute the equation below:\n\nusing Distributions, QuadGK\n\n# this function evaluates the reservation wage equation\ndS(x ; F,β,δ) = (1-cdf(F,x)) / (1-β*(1-δ)) #&lt;- this function defines S'(x)\nres_wage(wres ; b,λ,δ,β,F::Distribution) = wres - b - β * λ * quadgk(x-&gt;dS(x;F,β,δ),wres,Inf)[1]\npars = (;b = -5.,λ = 0.45,δ = 0.03,β = 0.99,F = LogNormal(1,1))\nres_wage(1. ; pars...)\n\n-33.70656559385876\n\n\nThe most straightforward way to solve for the reservation wage would be to use a root-finding method here. Since we have used Optim already, let’s just go ahead and use that package:\n\nusing Optim\nfunction solve_res_wage(pars)\n    (;F) = pars\n    w_lb = quantile(F,0.001) #&lt;- get a lower and upper bound for the solution\n    w_ub = quantile(F,0.999)\n    r = optimize(x-&gt;res_wage(x;pars...)^2,w_lb,w_ub)\n    return r.minimizer\nend\nrwage = solve_res_wage(pars)\n\n7.2471320591661295"
  },
  {
    "objectID": "models/search-model.html#model-solution",
    "href": "models/search-model.html#model-solution",
    "title": "Simple Search Model",
    "section": "",
    "text": "In class we showed that the optimal decision rule of the worker is characterized by a reservation wage. We derived the reservation wage equation:\n\\[ w^* = b + \\beta\\lambda\\int_{w^*}\\frac{1-F_{W}(w)}{1 - \\beta(1-\\delta)}dw \\]\nand we characterized the steady state rate of unemployment as:\n\\[ P[E = 0] = \\frac{h}{h+\\delta} \\]\nwhere \\(h = \\lambda(1-F_{W}(w^*))\\) is the rate at which workers exit unemployment.\nSimilarly, we showed that the steady state fraction of unemployment durations \\(t\\) is\n\\[ P[t_{U}=t] = h(1-h)^{t} \\]\nLet’s write some code to solve the reservation wage equation, starting with code to evalute the equation below:\n\nusing Distributions, QuadGK\n\n# this function evaluates the reservation wage equation\ndS(x ; F,β,δ) = (1-cdf(F,x)) / (1-β*(1-δ)) #&lt;- this function defines S'(x)\nres_wage(wres ; b,λ,δ,β,F::Distribution) = wres - b - β * λ * quadgk(x-&gt;dS(x;F,β,δ),wres,Inf)[1]\npars = (;b = -5.,λ = 0.45,δ = 0.03,β = 0.99,F = LogNormal(1,1))\nres_wage(1. ; pars...)\n\n-33.70656559385876\n\n\nThe most straightforward way to solve for the reservation wage would be to use a root-finding method here. Since we have used Optim already, let’s just go ahead and use that package:\n\nusing Optim\nfunction solve_res_wage(pars)\n    (;F) = pars\n    w_lb = quantile(F,0.001) #&lt;- get a lower and upper bound for the solution\n    w_ub = quantile(F,0.999)\n    r = optimize(x-&gt;res_wage(x;pars...)^2,w_lb,w_ub)\n    return r.minimizer\nend\nrwage = solve_res_wage(pars)\n\n7.2471320591661295"
  },
  {
    "objectID": "assignments/Assignment-1.html",
    "href": "assignments/Assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Consider the following simple model of time allocation. Individual utility is given by:\n\\[ U(C,L) = (\\phi C^{\\rho} + (1-\\phi) L^{\\rho})^{1/\\rho} \\]\nwhere \\(L\\) is an aggregate leisure good composed of \\(K\\) different activities:\n\\[ L = \\prod_{k=1}^{K}l_{k}^{\\delta_{k}},\\qquad \\sum_{k}\\delta_{k} = 1 \\]\nIn addition to these leisure activities, the agent may supply labor to the market at a wage rate of \\(w\\). Letting \\(h_{k}\\) be hours, the time constraint is:\n\\[ h_{k} + \\sum_{k}l_{k} = 1 \\]\nThe model is static and the individual solves the following problem:\n\\[ \\max_{C,\\{l_{k}\\}_{k=1}^{K}} U(C,L) \\]\nsubject to the constraint:\n\\[ C + w \\left(\\sum_{k} l_{k}\\right) \\leq w \\]\n\n\n\nSuppose you are interested in using this model to study the effects of a wage subsidy on labor supply.\nNotice that the model can be written as\n\\[ \\max_{C,h} U(wh,L^*(1-h)) \\]\nwhere\n\\[ L^*(1-h) = \\max_{\\{l_{k}\\}_{k=1}^{K}} \\prod_{k=1}^{K}l_{k}^{\\delta_{k}} \\]\nsubject to \\(\\sum_{k}l_{k} = 1-h\\). Given this simplification, what does Marschak’s Maxim (and common sense) suggest about what parameters need to be estimated here?\n\n\n\nBased on your answer to the above, you simplify the model to the following specification:\n\\[ h^* = \\arg\\max (\\phi (wh)^{\\rho} + (1-\\phi) (1-h)^{\\rho})^{1/\\rho} \\]\nand you derive the following relationship:\n\\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi}{1-\\phi}\\right) + \\frac{1}{1-\\rho}\\log(w) \\]\nwhere \\(C=wh^*\\) is total labor income and \\(L=1-h^*\\) is non-market time.\nSuppose you have a cross-section of data \\((C_{n},L_{n},W_{n})\\) where \\(C_{n}\\) is labor market earnings, \\(L_{n}\\) is non-market time, and \\(W_{n}\\) is the wage-rate for person \\(n\\). This could be taken (for example) from the Outgoing Rotation Group of the CPS monthly survey.\nHow does this model compare to what you are likely to see in the data?\n\n\n\nSuppose now you augment the model to acommodate some randomness in how much individuals work by allowing for heterogeneity in preferences (\\(\\phi\\)):\n\\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi_{n}}{1-\\phi_{n}}\\right) + \\frac{1}{1-\\rho}\\log(w) \\]\nWhat assumption do you need for an OLS regression of \\(\\log(C_{n}/L_{n})\\) on \\(\\log(W_{n})\\) to consistently recover the elasticity of labor supply, \\(1/(1-\\rho)\\)? Do you consider this credible? Why/why not?"
  },
  {
    "objectID": "assignments/Assignment-1.html#setup",
    "href": "assignments/Assignment-1.html#setup",
    "title": "Assignment 1",
    "section": "",
    "text": "Consider the following simple model of time allocation. Individual utility is given by:\n\\[ U(C,L) = (\\phi C^{\\rho} + (1-\\phi) L^{\\rho})^{1/\\rho} \\]\nwhere \\(L\\) is an aggregate leisure good composed of \\(K\\) different activities:\n\\[ L = \\prod_{k=1}^{K}l_{k}^{\\delta_{k}},\\qquad \\sum_{k}\\delta_{k} = 1 \\]\nIn addition to these leisure activities, the agent may supply labor to the market at a wage rate of \\(w\\). Letting \\(h_{k}\\) be hours, the time constraint is:\n\\[ h_{k} + \\sum_{k}l_{k} = 1 \\]\nThe model is static and the individual solves the following problem:\n\\[ \\max_{C,\\{l_{k}\\}_{k=1}^{K}} U(C,L) \\]\nsubject to the constraint:\n\\[ C + w \\left(\\sum_{k} l_{k}\\right) \\leq w \\]"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-1",
    "href": "assignments/Assignment-1.html#question-1",
    "title": "Assignment 1",
    "section": "",
    "text": "Suppose you are interested in using this model to study the effects of a wage subsidy on labor supply.\nNotice that the model can be written as\n\\[ \\max_{C,h} U(wh,L^*(1-h)) \\]\nwhere\n\\[ L^*(1-h) = \\max_{\\{l_{k}\\}_{k=1}^{K}} \\prod_{k=1}^{K}l_{k}^{\\delta_{k}} \\]\nsubject to \\(\\sum_{k}l_{k} = 1-h\\). Given this simplification, what does Marschak’s Maxim (and common sense) suggest about what parameters need to be estimated here?"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-2",
    "href": "assignments/Assignment-1.html#question-2",
    "title": "Assignment 1",
    "section": "",
    "text": "Based on your answer to the above, you simplify the model to the following specification:\n\\[ h^* = \\arg\\max (\\phi (wh)^{\\rho} + (1-\\phi) (1-h)^{\\rho})^{1/\\rho} \\]\nand you derive the following relationship:\n\\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi}{1-\\phi}\\right) + \\frac{1}{1-\\rho}\\log(w) \\]\nwhere \\(C=wh^*\\) is total labor income and \\(L=1-h^*\\) is non-market time.\nSuppose you have a cross-section of data \\((C_{n},L_{n},W_{n})\\) where \\(C_{n}\\) is labor market earnings, \\(L_{n}\\) is non-market time, and \\(W_{n}\\) is the wage-rate for person \\(n\\). This could be taken (for example) from the Outgoing Rotation Group of the CPS monthly survey.\nHow does this model compare to what you are likely to see in the data?"
  },
  {
    "objectID": "models/savings.html",
    "href": "models/savings.html",
    "title": "Life-Cycle Savings Model",
    "section": "",
    "text": "Life-Cycle Savings Model\nDetails IOU."
  },
  {
    "objectID": "assignments/Assignment-1.html#question-3",
    "href": "assignments/Assignment-1.html#question-3",
    "title": "Assignment 1",
    "section": "",
    "text": "Suppose now you augment the model to acommodate some randomness in how much individuals work by allowing for heterogeneity in preferences (\\(\\phi\\)):\n\\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi_{n}}{1-\\phi_{n}}\\right) + \\frac{1}{1-\\rho}\\log(w) \\]\nWhat assumption do you need for an OLS regression of \\(\\log(C_{n}/L_{n})\\) on \\(\\log(W_{n})\\) to consistently recover the elasticity of labor supply, \\(1/(1-\\rho)\\)? Do you consider this credible? Why/why not?"
  },
  {
    "objectID": "models/entry-exit.html",
    "href": "models/entry-exit.html",
    "title": "Symmetric Duopoly Model of Entry/Exit",
    "section": "",
    "text": "Here are the basic ingredients of the model:\n\nThere are two firms indexed by \\(f\\in\\{0,1\\}\\)\nThere are \\(M\\) markets indexed by \\(m\\)\nTime is discrete and indexed by \\(t\\)\nEach firm makes an entry decision every period. We let \\(j\\in\\{0,1\\}\\) index this decision to enter or not. Let \\(j(f,m,t)\\) indicate the choice of firm \\(f\\) in market \\(m\\) in period \\(t\\).\nWe let \\(a_{f,m,t}=j(f,m,t-1)\\) indicate whether firm \\(f\\) is active in market \\(m\\) in period \\(t\\), which means they entered in the previous period.\nLet \\(x_{m}\\) be a market-level unobservable that shifts the profitability of operations in market \\(m\\).\nIn addition to the observed states, each firm draws a pair of idiosyncatic shocks to payoffs in each period, \\(\\epsilon_{f}=[\\epsilon_{f0},\\epsilon_{f1}]\\) that is private information to the firm and is iid over markets, firms, and time periods.\nFirms make their decisions in each period simultaneously\n\nTo simplify notation, suppress dependance of outcomes on the market \\(m\\) and time period \\(t\\). Because we are writing a symmetric model, we will also suppress dependence on \\(f\\). The deterministic component of the payoff to entering is a function of the market primitives (\\(x\\)), the firm’s activity status (\\(a\\)), and the other firm’s entry decision \\(j^\\prime\\):\n\\[ u_{1}(x,a,j^{\\prime}) = \\phi_{0} + \\phi_{1}x - \\phi_{2}j^\\prime - \\phi_{3}(1-a) \\]\nThe payoff to not entering is simply:\n\\[{u}_{0}(x,a) = \\phi_{4}a \\]\nBefore characterizing the solution to the firm’s problem, let’s code up these payoff functions:\n\n\nu1(x,a,j′,ϕ) = ϕ[1] + ϕ[2]*x - ϕ[3]j′ + ϕ[4]*(1-a)\nu0(a,ϕ) = a * ϕ[5]\n\nu0 (generic function with 1 method)\n\n\n\n\n\nLet \\(j^*(x,a,a',\\epsilon)\\) be the firm’s optimal decision given the state and the idiosyncratic shock. We will focus on symmetric equilibria so this policy function is sufficient to describe the behavior of both firms.\nThe value to either firm of arriving in a period with state \\((x,a,a')\\) can be written recursively as:\n\\[\n\\begin{split}\nV(x,a,a') = \\mathbb{E}_{\\epsilon,\\epsilon'}\\max\\{u_{1}(x,a,j^*(x,a,a',\\epsilon'))+\\epsilon_{1} + \\beta V(x,1,j^*(x,a,a',\\epsilon')), \\\\ u_{0}(x,a) + \\epsilon_{0} + \\beta V(x,0,j^*(x,a,a',\\epsilon'))\\}\n\\end{split}\n\\]\nDefine the optimal choice probability in equilibrium as:\n\\[ p(x,a,a') = \\int_{\\epsilon}j^*(x,a,a',\\epsilon)dF(\\epsilon) \\]\nWith this in hand we can integrate out the other firm’s shocks \\(\\epsilon\\)’ to get:\n\\[\n\\begin{split}\nV(x,a,a') = \\mathbb{E}_{\\epsilon}\\max\\{\\phi_{0}+\\phi_{1}x - \\phi_{2}p(x,a',a) +\\epsilon_{1} + \\beta [p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)], \\\\\na \\phi_{4} + \\epsilon_{0} + \\beta [p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)]\\}\n\\end{split}\n\\]\nDefine the choice-specific values as:\n\\[ v_{1}(x,a,a') = \\phi_{0}+\\phi_{1}x - \\phi_{2}p(x,a',a) + \\beta [p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)] \\]\nand\n\\[ v_{0}(x,a,a') = a \\phi_{4} + \\beta [p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)] \\]\nSo assuming that \\(\\epsilon\\) is distributed as type I extreme value random variable with location parameter 0 and scale parameter 1 we get analytical expressions for the choice probabilities and the expected value of the maximum:\n\\[ V(x) = \\gamma + \\log\\left(\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))\\right)\\]\nwhere \\(\\gamma\\) is the Euler-Mascheroni constant and\n\\[ p(x,a,a') = \\frac{\\exp(v_{1}(x,a,a'))}{\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))} \\]\nBefore we define equilibrium and think about solving the model, let’s quickly write up the mapping between the other firm’s choice probabilities and the choice values:\n\n# Fixing x, assume that V is stored as a 2 x 2 array\n# The argument p is the current guess of p(x,a',a)\nfunction choice_values(x,a,p,V,ϕ,β)\n    v0 = u0(a,ϕ) + β * p * V[1,2] + β * (1-p) * V[1,1]\n    v1 = u1(x,a,p,ϕ) + β * p * V[2,2] + β * (1-p) * V[2,1]\n    return v0,v1\nend\n\nchoice_values (generic function with 1 method)\n\n\nIn principle we could iterate on this mapping to find (for a fixed \\(p\\)), the firm’s optimal solution. But that won’t be an efficient way to try and solve for the equilibrium.\n\n\n\nThe solution concept for this model is Markov Perfect Equilibrium. Fixing the market \\(x\\), here the equilibrium be characterized as a fixed point in the value function \\(V\\) and choice probabilities, \\(p\\). In words, equilibrium is summarized by a \\(V\\) and a \\(p\\) such that:\n\nGiven \\(p\\), \\(V\\) is a fixed point in the recursive formulation of values; and\n\\(p\\) are the optimal choice probabilities of each firm given \\(V\\) and given the other firm’s choice probabilities are \\(p\\).\n\nHow should we solve for this symmetric equilibrium? We could try iterating on \\(V\\) and \\(p\\) as follows:\n\n# V is a 2x2 array with values\n# p is a 2x2 array with choice probabilities\nfunction iterate_model(V,p,x,ϕ,β)\n    Vnew = copy(V)\n    pnew = copy(p)\n    for a′ in axes(V,2)\n        for a in axes(V,1)\n            p′ = p[a′,a]\n            v0,v1 = choice_values(x,a-1,p′,V,ϕ,β)\n            pnew[a,a′] = exp(v1) / (exp(v0)+exp(v1))\n            Vnew[a,a′] = log(exp(v0)+exp(v1))\n        end\n    end\n    return Vnew,pnew\nend\n\nfunction solve_by_iteration(x,ϕ,β; max_iter = 1000, verbose = false)\n    V0 = zeros(2,2)\n    p0 = fill(0.1,2,2)\n    err = Inf\n    iter = 1\n    while err&gt;1e-10 && iter&lt;max_iter\n        V1,p1 = iterate_model(V0,p0,x,ϕ,β)\n        err = maximum(abs.(V1 .- V0))\n        if mod(iter,100)==0 && verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        V0 = V1\n        p0 = p1\n        iter += 1\n    end\n    return V0,p0\nend\n\nβ = 0.95\nϕ = 2 * [1.,0.1,0.5,2.,0.5]\nsolve_by_iteration(0.,ϕ,β; verbose = true)\n\nIteration 100, error is 0.04823924738592211\nIteration 200, error is 0.001242310554474102\nIteration 300, error is 5.032709619001707e-5\nIteration 400, error is 2.123540213005981e-6\nIteration 500, error is 8.863101186307176e-8\nIteration 600, error is 3.693557459882868e-9\nIteration 700, error is 1.538467131467769e-10\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nThis seems to work! But notice that it takes a while for the iteration to converge. Also, unlike the single agent case, there is no guarantee that this iteration is always a contraction.\nWe can also solve this model relatively easily using Newton’s Method and the magic of Automatic Differentiation. To do this, we’ll solve over the pair of choice-specific values \\(v_{0}\\) and \\(v_{1}\\) (these encode both values and choice probabilities) and store these values as a vector instead of an array:\n\nusing ForwardDiff, LinearAlgebra\n\n# this function returns V as a 2 x 2 array given the vector of choice specific values in V\nfunction calc_V(v)\n    idx = LinearIndices((2,2,2))\n    [log(exp(v[idx[1,1+a,1+a′]]) + exp(v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n# this function returns choice probabilities as a 2x2 array given the vector v\nfunction calc_p(v)\n    idx = LinearIndices((2,2,2))\n    [1 / (1+exp(v[idx[1,1+a,1+a′]] - v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n\nfunction iterate_model_v(v,x,ϕ,β)\n    idx = LinearIndices((2,2,2)) #&lt;- this is for convenient indexing over v\n    vnew = copy(v)\n    V = calc_V(v)\n    for a′ in axes(idx,3)\n        for a in axes(idx,2)\n            i0′ = idx[1,a′,a] #&lt;- this locates the position in v for v_{0}(x,a',a)\n            i1′ = idx[2,a′,a] #&lt;- this locates the position in v for v_{1}(x,a',a)\n            p = 1 / (1 + exp(v[i0′] - v[i1′]))\n            v0,v1 = choice_values(x,a-1,p,V,ϕ,β)\n            vnew[idx[1,a,a′]] = v0\n            vnew[idx[2,a,a′]] = v1\n        end\n    end\n    return vnew\nend\n\nF(v,x,ϕ,β) = v .- iterate_model_v(v,x,ϕ,β)\nfunction solve_model_newton(x,ϕ,β;max_iter = 10, verbose = false)\n    v = zeros(8)\n    dF(v) = ForwardDiff.jacobian(y-&gt;F(y,x,ϕ,β),v)\n    err = Inf\n    iter = 1\n    while (err&gt;1e-10) && (iter&lt;max_iter)\n        Fv = F(v,x,ϕ,β)\n        dFv = dF(v)\n        vnew = v - inv(dFv) * Fv\n        err = maximum(abs.(Fv))\n        if verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        iter += 1\n        v = vnew\n    end\n    return v\nend\n\nsolve_model_newton(0.,ϕ,β;verbose = true);\n\nIteration 1, error is 6.158489821531948\nIteration 2, error is 1.7766463237555712\nIteration 3, error is 0.056247498263360285\nIteration 4, error is 0.00028434628951856666\nIteration 5, error is 3.4473004006940755e-8\nIteration 6, error is 1.4210854715202004e-14\n\n\nLet’s try timing each solution method to quickly compare:\n\nsolve_model_newton(0.,ϕ,β)\nsolve_by_iteration(0.,ϕ,β)\n\n@time solve_model_newton(0.,ϕ,β)\n@time solve_by_iteration(0.,ϕ,β)\n\n  0.000099 seconds (157 allocations: 58.250 KiB)\n  0.000251 seconds (2.15 k allocations: 201.031 KiB)\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nIn this case Newton’s method is faster. Let’s double check that both methods return the same answer:\n\nv0 = solve_model_newton(0.,ϕ,β)\nV0,p = solve_by_iteration(0.,ϕ,β)\np1 = calc_p(v0)\n[p p1]\n\n2×4 Matrix{Float64}:\n 0.910765   0.99055   0.910765   0.99055\n 0.0524759  0.277297  0.0524759  0.277297\n\n\nLooks good! We can re-use this code when we get to thinking about estimation later on. To do this we will have to solve the model for different values of \\(x_{m}\\), but that can be done by using this code and iterating (potentially in parallel) over different values of \\(x\\).\nIf you play around with parameters, you will see how convergence times may change and that solution methods are not always stable, especially when choice probabilities in equilibrium are very close to one or zero."
  },
  {
    "objectID": "models/entry-exit.html#model-ingredients",
    "href": "models/entry-exit.html#model-ingredients",
    "title": "Symmetric Duopoly Model of Entry/Exit",
    "section": "",
    "text": "Here are the basic ingredients of the model:\n\nThere are two firms indexed by \\(f\\in\\{0,1\\}\\)\nThere are \\(M\\) markets indexed by \\(m\\)\nTime is discrete and indexed by \\(t\\)\nEach firm makes an entry decision every period. We let \\(j\\in\\{0,1\\}\\) index this decision to enter or not. Let \\(j(f,m,t)\\) indicate the choice of firm \\(f\\) in market \\(m\\) in period \\(t\\).\nWe let \\(a_{f,m,t}=j(f,m,t-1)\\) indicate whether firm \\(f\\) is active in market \\(m\\) in period \\(t\\), which means they entered in the previous period.\nLet \\(x_{m}\\) be a market-level unobservable that shifts the profitability of operations in market \\(m\\).\nIn addition to the observed states, each firm draws a pair of idiosyncatic shocks to payoffs in each period, \\(\\epsilon_{f}=[\\epsilon_{f0},\\epsilon_{f1}]\\) that is private information to the firm and is iid over markets, firms, and time periods.\nFirms make their decisions in each period simultaneously\n\nTo simplify notation, suppress dependance of outcomes on the market \\(m\\) and time period \\(t\\). Because we are writing a symmetric model, we will also suppress dependence on \\(f\\). The deterministic component of the payoff to entering is a function of the market primitives (\\(x\\)), the firm’s activity status (\\(a\\)), and the other firm’s entry decision \\(j^\\prime\\):\n\\[ u_{1}(x,a,j^{\\prime}) = \\phi_{0} + \\phi_{1}x - \\phi_{2}j^\\prime - \\phi_{3}(1-a) \\]\nThe payoff to not entering is simply:\n\\[{u}_{0}(x,a) = \\phi_{4}a \\]\nBefore characterizing the solution to the firm’s problem, let’s code up these payoff functions:\n\n\nu1(x,a,j′,ϕ) = ϕ[1] + ϕ[2]*x - ϕ[3]j′ + ϕ[4]*(1-a)\nu0(a,ϕ) = a * ϕ[5]\n\nu0 (generic function with 1 method)"
  },
  {
    "objectID": "models/entry-exit.html#solving-the-firms-problem",
    "href": "models/entry-exit.html#solving-the-firms-problem",
    "title": "Symmetric Duopoly Model of Entry/Exit",
    "section": "",
    "text": "Let \\(j^*(x,a,a',\\epsilon)\\) be the firm’s optimal decision given the state and the idiosyncratic shock. We will focus on symmetric equilibria so this policy function is sufficient to describe the behavior of both firms.\nThe value to either firm of arriving in a period with state \\((x,a,a')\\) can be written recursively as:\n\\[\n\\begin{split}\nV(x,a,a') = \\mathbb{E}_{\\epsilon,\\epsilon'}\\max\\{u_{1}(x,a,j^*(x,a,a',\\epsilon'))+\\epsilon_{1} + \\beta V(x,1,j^*(x,a,a',\\epsilon')), \\\\ u_{0}(x,a) + \\epsilon_{0} + \\beta V(x,0,j^*(x,a,a',\\epsilon'))\\}\n\\end{split}\n\\]\nDefine the optimal choice probability in equilibrium as:\n\\[ p(x,a,a') = \\int_{\\epsilon}j^*(x,a,a',\\epsilon)dF(\\epsilon) \\]\nWith this in hand we can integrate out the other firm’s shocks \\(\\epsilon\\)’ to get:\n\\[\n\\begin{split}\nV(x,a,a') = \\mathbb{E}_{\\epsilon}\\max\\{\\phi_{0}+\\phi_{1}x - \\phi_{2}p(x,a',a) +\\epsilon_{1} + \\beta [p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)], \\\\\na \\phi_{4} + \\epsilon_{0} + \\beta [p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)]\\}\n\\end{split}\n\\]\nDefine the choice-specific values as:\n\\[ v_{1}(x,a,a') = \\phi_{0}+\\phi_{1}x - \\phi_{2}p(x,a',a) + \\beta [p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)] \\]\nand\n\\[ v_{0}(x,a,a') = a \\phi_{4} + \\beta [p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)] \\]\nSo assuming that \\(\\epsilon\\) is distributed as type I extreme value random variable with location parameter 0 and scale parameter 1 we get analytical expressions for the choice probabilities and the expected value of the maximum:\n\\[ V(x) = \\gamma + \\log\\left(\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))\\right)\\]\nwhere \\(\\gamma\\) is the Euler-Mascheroni constant and\n\\[ p(x,a,a') = \\frac{\\exp(v_{1}(x,a,a'))}{\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))} \\]\nBefore we define equilibrium and think about solving the model, let’s quickly write up the mapping between the other firm’s choice probabilities and the choice values:\n\n# Fixing x, assume that V is stored as a 2 x 2 array\n# The argument p is the current guess of p(x,a',a)\nfunction choice_values(x,a,p,V,ϕ,β)\n    v0 = u0(a,ϕ) + β * p * V[1,2] + β * (1-p) * V[1,1]\n    v1 = u1(x,a,p,ϕ) + β * p * V[2,2] + β * (1-p) * V[2,1]\n    return v0,v1\nend\n\nchoice_values (generic function with 1 method)\n\n\nIn principle we could iterate on this mapping to find (for a fixed \\(p\\)), the firm’s optimal solution. But that won’t be an efficient way to try and solve for the equilibrium."
  },
  {
    "objectID": "models/entry-exit.html#equilibrium",
    "href": "models/entry-exit.html#equilibrium",
    "title": "Symmetric Duopoly Model of Entry/Exit",
    "section": "",
    "text": "The solution concept for this model is Markov Perfect Equilibrium. Fixing the market \\(x\\), here the equilibrium be characterized as a fixed point in the value function \\(V\\) and choice probabilities, \\(p\\). In words, equilibrium is summarized by a \\(V\\) and a \\(p\\) such that:\n\nGiven \\(p\\), \\(V\\) is a fixed point in the recursive formulation of values; and\n\\(p\\) are the optimal choice probabilities of each firm given \\(V\\) and given the other firm’s choice probabilities are \\(p\\).\n\nHow should we solve for this symmetric equilibrium? We could try iterating on \\(V\\) and \\(p\\) as follows:\n\n# V is a 2x2 array with values\n# p is a 2x2 array with choice probabilities\nfunction iterate_model(V,p,x,ϕ,β)\n    Vnew = copy(V)\n    pnew = copy(p)\n    for a′ in axes(V,2)\n        for a in axes(V,1)\n            p′ = p[a′,a]\n            v0,v1 = choice_values(x,a-1,p′,V,ϕ,β)\n            pnew[a,a′] = exp(v1) / (exp(v0)+exp(v1))\n            Vnew[a,a′] = log(exp(v0)+exp(v1))\n        end\n    end\n    return Vnew,pnew\nend\n\nfunction solve_by_iteration(x,ϕ,β; max_iter = 1000, verbose = false)\n    V0 = zeros(2,2)\n    p0 = fill(0.1,2,2)\n    err = Inf\n    iter = 1\n    while err&gt;1e-10 && iter&lt;max_iter\n        V1,p1 = iterate_model(V0,p0,x,ϕ,β)\n        err = maximum(abs.(V1 .- V0))\n        if mod(iter,100)==0 && verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        V0 = V1\n        p0 = p1\n        iter += 1\n    end\n    return V0,p0\nend\n\nβ = 0.95\nϕ = 2 * [1.,0.1,0.5,2.,0.5]\nsolve_by_iteration(0.,ϕ,β; verbose = true)\n\nIteration 100, error is 0.04823924738592211\nIteration 200, error is 0.001242310554474102\nIteration 300, error is 5.032709619001707e-5\nIteration 400, error is 2.123540213005981e-6\nIteration 500, error is 8.863101186307176e-8\nIteration 600, error is 3.693557459882868e-9\nIteration 700, error is 1.538467131467769e-10\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nThis seems to work! But notice that it takes a while for the iteration to converge. Also, unlike the single agent case, there is no guarantee that this iteration is always a contraction.\nWe can also solve this model relatively easily using Newton’s Method and the magic of Automatic Differentiation. To do this, we’ll solve over the pair of choice-specific values \\(v_{0}\\) and \\(v_{1}\\) (these encode both values and choice probabilities) and store these values as a vector instead of an array:\n\nusing ForwardDiff, LinearAlgebra\n\n# this function returns V as a 2 x 2 array given the vector of choice specific values in V\nfunction calc_V(v)\n    idx = LinearIndices((2,2,2))\n    [log(exp(v[idx[1,1+a,1+a′]]) + exp(v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n# this function returns choice probabilities as a 2x2 array given the vector v\nfunction calc_p(v)\n    idx = LinearIndices((2,2,2))\n    [1 / (1+exp(v[idx[1,1+a,1+a′]] - v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n\nfunction iterate_model_v(v,x,ϕ,β)\n    idx = LinearIndices((2,2,2)) #&lt;- this is for convenient indexing over v\n    vnew = copy(v)\n    V = calc_V(v)\n    for a′ in axes(idx,3)\n        for a in axes(idx,2)\n            i0′ = idx[1,a′,a] #&lt;- this locates the position in v for v_{0}(x,a',a)\n            i1′ = idx[2,a′,a] #&lt;- this locates the position in v for v_{1}(x,a',a)\n            p = 1 / (1 + exp(v[i0′] - v[i1′]))\n            v0,v1 = choice_values(x,a-1,p,V,ϕ,β)\n            vnew[idx[1,a,a′]] = v0\n            vnew[idx[2,a,a′]] = v1\n        end\n    end\n    return vnew\nend\n\nF(v,x,ϕ,β) = v .- iterate_model_v(v,x,ϕ,β)\nfunction solve_model_newton(x,ϕ,β;max_iter = 10, verbose = false)\n    v = zeros(8)\n    dF(v) = ForwardDiff.jacobian(y-&gt;F(y,x,ϕ,β),v)\n    err = Inf\n    iter = 1\n    while (err&gt;1e-10) && (iter&lt;max_iter)\n        Fv = F(v,x,ϕ,β)\n        dFv = dF(v)\n        vnew = v - inv(dFv) * Fv\n        err = maximum(abs.(Fv))\n        if verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        iter += 1\n        v = vnew\n    end\n    return v\nend\n\nsolve_model_newton(0.,ϕ,β;verbose = true);\n\nIteration 1, error is 6.158489821531948\nIteration 2, error is 1.7766463237555712\nIteration 3, error is 0.056247498263360285\nIteration 4, error is 0.00028434628951856666\nIteration 5, error is 3.4473004006940755e-8\nIteration 6, error is 1.4210854715202004e-14\n\n\nLet’s try timing each solution method to quickly compare:\n\nsolve_model_newton(0.,ϕ,β)\nsolve_by_iteration(0.,ϕ,β)\n\n@time solve_model_newton(0.,ϕ,β)\n@time solve_by_iteration(0.,ϕ,β)\n\n  0.000099 seconds (157 allocations: 58.250 KiB)\n  0.000251 seconds (2.15 k allocations: 201.031 KiB)\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nIn this case Newton’s method is faster. Let’s double check that both methods return the same answer:\n\nv0 = solve_model_newton(0.,ϕ,β)\nV0,p = solve_by_iteration(0.,ϕ,β)\np1 = calc_p(v0)\n[p p1]\n\n2×4 Matrix{Float64}:\n 0.910765   0.99055   0.910765   0.99055\n 0.0524759  0.277297  0.0524759  0.277297\n\n\nLooks good! We can re-use this code when we get to thinking about estimation later on. To do this we will have to solve the model for different values of \\(x_{m}\\), but that can be done by using this code and iterating (potentially in parallel) over different values of \\(x\\).\nIf you play around with parameters, you will see how convergence times may change and that solution methods are not always stable, especially when choice probabilities in equilibrium are very close to one or zero."
  },
  {
    "objectID": "recitations/recitation-2.html",
    "href": "recitations/recitation-2.html",
    "title": "Recitation 2",
    "section": "",
    "text": "Review the code and solution method for the search model."
  },
  {
    "objectID": "recitations/recitation-2.html#part-1",
    "href": "recitations/recitation-2.html#part-1",
    "title": "Recitation 2",
    "section": "",
    "text": "Review the code and solution method for the search model."
  },
  {
    "objectID": "recitations/recitation-2.html#part-2",
    "href": "recitations/recitation-2.html#part-2",
    "title": "Recitation 2",
    "section": "Part 2",
    "text": "Part 2\nReview the code and solution method for the entry/exit model."
  },
  {
    "objectID": "recitations/recitation-2.html#part-3",
    "href": "recitations/recitation-2.html#part-3",
    "title": "Recitation 2",
    "section": "Part 3",
    "text": "Part 3\nLet’s take a look at data from the CPS on wages, employment status, and labor market transitions. Here is code to read in the data:\n\nusing CSV, DataFrames, DataFramesMeta\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\n\n183277×22 DataFrame183252 rows omitted\n\n\n\nRow\nYEAR\nSERIAL\nMONTH\nHWTFINL\nCPSID\nASECFLAG\nPERNUM\nWTFINL\nCPSIDP\nAGE\nSEX\nRACE\nMARST\nEMPSTAT\nLABFORCE\nUHRSWORKT\nDURUNEMP\nEDUC\nHOURWAGE\nPAIDHOUR\nEARNWEEK\nUHRSWORKORG\n\n\n\nInt64\nInt64\nInt64\nFloat64\nInt64\nInt64?\nInt64\nFloat64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\nInt64\nFloat64\nInt64\n\n\n\n\n1\n2018\n2\n1\n1609.49\n20161200000200\nmissing\n1\n1420.75\n20161200000201\n72\n1\n100\n1\n10\n2\n55\n999\n81\n99.99\n0\n9999.99\n999\n\n\n2\n2018\n3\n1\n1797.04\n20180100000300\nmissing\n1\n2053.27\n20180100000301\n66\n1\n100\n1\n10\n2\n997\n999\n111\n99.99\n0\n9999.99\n999\n\n\n3\n2018\n3\n1\n1797.04\n20180100000300\nmissing\n2\n1797.04\n20180100000302\n61\n2\n100\n1\n10\n2\n997\n999\n111\n99.99\n0\n9999.99\n999\n\n\n4\n2018\n9\n1\n1735.76\n20171000000400\nmissing\n1\n1735.76\n20171000000401\n52\n2\n200\n4\n10\n2\n40\n999\n73\n20.84\n2\n903.0\n999\n\n\n5\n2018\n9\n1\n1735.76\n20171000000400\nmissing\n4\n3069.4\n20171000000404\n19\n2\n200\n6\n10\n2\n40\n999\n73\n10.0\n2\n400.0\n40\n\n\n6\n2018\n10\n1\n1582.77\n20171000000600\nmissing\n1\n1582.77\n20171000000601\n56\n2\n200\n4\n10\n2\n40\n999\n111\n25.0\n2\n1250.0\n999\n\n\n7\n2018\n10\n1\n1582.77\n20171000000600\nmissing\n2\n2409.42\n20171000000602\n22\n2\n200\n6\n10\n2\n30\n999\n81\n9.5\n2\n70.0\n999\n\n\n8\n2018\n11\n1\n1795.64\n20170100000900\nmissing\n1\n1795.64\n20170100000901\n23\n2\n100\n6\n10\n2\n40\n999\n124\n99.99\n0\n9999.99\n999\n\n\n9\n2018\n11\n1\n1795.64\n20170100000900\nmissing\n2\n1795.64\n20170100000902\n24\n2\n100\n6\n10\n2\n40\n999\n124\n99.99\n0\n9999.99\n999\n\n\n10\n2018\n12\n1\n1927.69\n20170100001000\nmissing\n1\n1927.69\n20170100001001\n59\n2\n200\n1\n10\n2\n55\n999\n111\n99.99\n0\n9999.99\n999\n\n\n11\n2018\n12\n1\n1927.69\n20170100001000\nmissing\n2\n2151.5\n20170100001002\n53\n1\n200\n1\n10\n2\n58\n999\n81\n99.99\n0\n9999.99\n999\n\n\n12\n2018\n14\n1\n2926.96\n20171200001200\nmissing\n1\n2926.96\n20171200001201\n24\n2\n200\n6\n10\n2\n40\n999\n73\n99.99\n0\n9999.99\n999\n\n\n13\n2018\n15\n1\n1861.24\n20161200000800\nmissing\n1\n1557.36\n20161200000801\n60\n1\n100\n1\n10\n2\n40\n999\n124\n99.99\n0\n9999.99\n999\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n183266\n2018\n72283\n3\n275.748\n20170107444500\n2\n3\n280.118\n20170107444503\n33\n1\n100\n6\n21\n2\n999\n61\n73\n99.99\n0\n9999.99\n999\n\n\n183267\n2018\n72286\n3\n332.029\n20170307448900\n2\n1\n332.029\n20170307448901\n44\n2\n100\n1\n10\n2\n30\n999\n111\n99.99\n0\n9999.99\n999\n\n\n183268\n2018\n72286\n3\n332.029\n20170307448900\n2\n2\n334.955\n20170307448902\n46\n1\n100\n1\n10\n2\n45\n999\n91\n99.99\n0\n9999.99\n999\n\n\n183269\n2018\n72288\n3\n341.227\n20171207232200\n2\n1\n341.227\n20171207232201\n42\n1\n100\n6\n10\n2\n40\n999\n111\n99.99\n1\n365.0\n999\n\n\n183270\n2018\n72288\n3\n341.227\n20171207232200\n2\n2\n281.741\n20171207232202\n41\n2\n100\n4\n10\n2\n40\n999\n73\n14.0\n2\n560.0\n40\n\n\n183271\n2018\n72289\n3\n256.804\n20170107445500\n2\n1\n256.804\n20170107445501\n42\n1\n100\n4\n10\n2\n45\n999\n73\n99.99\n0\n9999.99\n999\n\n\n183272\n2018\n72291\n3\n281.741\n20171207232600\n2\n1\n507.056\n20171207232601\n42\n1\n100\n1\n10\n2\n40\n999\n111\n99.99\n1\n519.23\n999\n\n\n183273\n2018\n72291\n3\n281.741\n20171207232600\n2\n2\n281.741\n20171207232602\n43\n2\n100\n1\n10\n2\n50\n999\n123\n99.99\n1\n1442.3\n999\n\n\n183274\n2018\n72291\n3\n281.741\n20171207232600\n2\n4\n377.923\n20171207232604\n18\n1\n100\n6\n10\n2\n15\n999\n60\n9.0\n2\n108.0\n12\n\n\n183275\n2018\n72292\n3\n288.99\n20171207232700\n2\n1\n288.99\n20171207232701\n32\n2\n100\n6\n10\n2\n40\n999\n81\n25.0\n2\n1000.0\n40\n\n\n183276\n2018\n72292\n3\n288.99\n20171207232700\n2\n2\n288.99\n20171207232702\n30\n2\n100\n1\n10\n2\n20\n999\n81\n99.99\n0\n9999.99\n999\n\n\n183277\n2018\n72292\n3\n288.99\n20171207232700\n2\n3\n336.057\n20171207232703\n31\n1\n100\n1\n10\n2\n997\n999\n91\n99.99\n1\n1346.0\n999\n\n\n\n\n\n\nAs you can see from the preview of the data, the data is taken from January-March 2018."
  },
  {
    "objectID": "recitations/recitation-2.html#part-3-cps-data",
    "href": "recitations/recitation-2.html#part-3-cps-data",
    "title": "Recitation 2",
    "section": "Part 3: CPS data",
    "text": "Part 3: CPS data\n\nReading the data\nLet’s take a look at data from the CPS on wages, employment status, and labor market transitions. Here is code to read in the data:\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\n\n183277×22 DataFrame183252 rows omitted\n\n\n\nRow\nYEAR\nSERIAL\nMONTH\nHWTFINL\nCPSID\nASECFLAG\nPERNUM\nWTFINL\nCPSIDP\nAGE\nSEX\nRACE\nMARST\nEMPSTAT\nLABFORCE\nUHRSWORKT\nDURUNEMP\nEDUC\nHOURWAGE\nPAIDHOUR\nEARNWEEK\nUHRSWORKORG\n\n\n\nInt64\nInt64\nInt64\nFloat64\nInt64\nInt64?\nInt64\nFloat64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\nInt64\nFloat64\nInt64\n\n\n\n\n1\n2018\n2\n1\n1609.49\n20161200000200\nmissing\n1\n1420.75\n20161200000201\n72\n1\n100\n1\n10\n2\n55\n999\n81\n99.99\n0\n9999.99\n999\n\n\n2\n2018\n3\n1\n1797.04\n20180100000300\nmissing\n1\n2053.27\n20180100000301\n66\n1\n100\n1\n10\n2\n997\n999\n111\n99.99\n0\n9999.99\n999\n\n\n3\n2018\n3\n1\n1797.04\n20180100000300\nmissing\n2\n1797.04\n20180100000302\n61\n2\n100\n1\n10\n2\n997\n999\n111\n99.99\n0\n9999.99\n999\n\n\n4\n2018\n9\n1\n1735.76\n20171000000400\nmissing\n1\n1735.76\n20171000000401\n52\n2\n200\n4\n10\n2\n40\n999\n73\n20.84\n2\n903.0\n999\n\n\n5\n2018\n9\n1\n1735.76\n20171000000400\nmissing\n4\n3069.4\n20171000000404\n19\n2\n200\n6\n10\n2\n40\n999\n73\n10.0\n2\n400.0\n40\n\n\n6\n2018\n10\n1\n1582.77\n20171000000600\nmissing\n1\n1582.77\n20171000000601\n56\n2\n200\n4\n10\n2\n40\n999\n111\n25.0\n2\n1250.0\n999\n\n\n7\n2018\n10\n1\n1582.77\n20171000000600\nmissing\n2\n2409.42\n20171000000602\n22\n2\n200\n6\n10\n2\n30\n999\n81\n9.5\n2\n70.0\n999\n\n\n8\n2018\n11\n1\n1795.64\n20170100000900\nmissing\n1\n1795.64\n20170100000901\n23\n2\n100\n6\n10\n2\n40\n999\n124\n99.99\n0\n9999.99\n999\n\n\n9\n2018\n11\n1\n1795.64\n20170100000900\nmissing\n2\n1795.64\n20170100000902\n24\n2\n100\n6\n10\n2\n40\n999\n124\n99.99\n0\n9999.99\n999\n\n\n10\n2018\n12\n1\n1927.69\n20170100001000\nmissing\n1\n1927.69\n20170100001001\n59\n2\n200\n1\n10\n2\n55\n999\n111\n99.99\n0\n9999.99\n999\n\n\n11\n2018\n12\n1\n1927.69\n20170100001000\nmissing\n2\n2151.5\n20170100001002\n53\n1\n200\n1\n10\n2\n58\n999\n81\n99.99\n0\n9999.99\n999\n\n\n12\n2018\n14\n1\n2926.96\n20171200001200\nmissing\n1\n2926.96\n20171200001201\n24\n2\n200\n6\n10\n2\n40\n999\n73\n99.99\n0\n9999.99\n999\n\n\n13\n2018\n15\n1\n1861.24\n20161200000800\nmissing\n1\n1557.36\n20161200000801\n60\n1\n100\n1\n10\n2\n40\n999\n124\n99.99\n0\n9999.99\n999\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n183266\n2018\n72283\n3\n275.748\n20170107444500\n2\n3\n280.118\n20170107444503\n33\n1\n100\n6\n21\n2\n999\n61\n73\n99.99\n0\n9999.99\n999\n\n\n183267\n2018\n72286\n3\n332.029\n20170307448900\n2\n1\n332.029\n20170307448901\n44\n2\n100\n1\n10\n2\n30\n999\n111\n99.99\n0\n9999.99\n999\n\n\n183268\n2018\n72286\n3\n332.029\n20170307448900\n2\n2\n334.955\n20170307448902\n46\n1\n100\n1\n10\n2\n45\n999\n91\n99.99\n0\n9999.99\n999\n\n\n183269\n2018\n72288\n3\n341.227\n20171207232200\n2\n1\n341.227\n20171207232201\n42\n1\n100\n6\n10\n2\n40\n999\n111\n99.99\n1\n365.0\n999\n\n\n183270\n2018\n72288\n3\n341.227\n20171207232200\n2\n2\n281.741\n20171207232202\n41\n2\n100\n4\n10\n2\n40\n999\n73\n14.0\n2\n560.0\n40\n\n\n183271\n2018\n72289\n3\n256.804\n20170107445500\n2\n1\n256.804\n20170107445501\n42\n1\n100\n4\n10\n2\n45\n999\n73\n99.99\n0\n9999.99\n999\n\n\n183272\n2018\n72291\n3\n281.741\n20171207232600\n2\n1\n507.056\n20171207232601\n42\n1\n100\n1\n10\n2\n40\n999\n111\n99.99\n1\n519.23\n999\n\n\n183273\n2018\n72291\n3\n281.741\n20171207232600\n2\n2\n281.741\n20171207232602\n43\n2\n100\n1\n10\n2\n50\n999\n123\n99.99\n1\n1442.3\n999\n\n\n183274\n2018\n72291\n3\n281.741\n20171207232600\n2\n4\n377.923\n20171207232604\n18\n1\n100\n6\n10\n2\n15\n999\n60\n9.0\n2\n108.0\n12\n\n\n183275\n2018\n72292\n3\n288.99\n20171207232700\n2\n1\n288.99\n20171207232701\n32\n2\n100\n6\n10\n2\n40\n999\n81\n25.0\n2\n1000.0\n40\n\n\n183276\n2018\n72292\n3\n288.99\n20171207232700\n2\n2\n288.99\n20171207232702\n30\n2\n100\n1\n10\n2\n20\n999\n81\n99.99\n0\n9999.99\n999\n\n\n183277\n2018\n72292\n3\n288.99\n20171207232700\n2\n3\n336.057\n20171207232703\n31\n1\n100\n1\n10\n2\n997\n999\n91\n99.99\n1\n1346.0\n999\n\n\n\n\n\n\nAs you can see from the preview of the data, the data is taken from January-March 2018. Here is a quick snippet of code to see how many observations we have on average per person:\n\n@chain data begin\n    groupby(:CPSIDP)\n    @combine :T = length(:EMPSTAT)\n    @combine :average = mean(:T) :frac_panel = mean(:T.&gt;1)\nend\n\n1×2 DataFrame\n\n\n\nRow\naverage\nfrac_panel\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n1.83191\n0.582806\n\n\n\n\n\n\nSo we see that more than half of the individuals in this sample can be found in more than one month of the data.\nThe @chain macro comes from the package DataFramesMeta and is a convenient syntax for composing operations into one block. For example:\n\n@chain x begin\n    func1(y1)\n    func2(y2)\n    func3(y3)\nend\n\nis equivalent to\n\nfunc3(func2(func1(x,y1),y2),y3)\n\n\n\nCalculating some moments\nYou may find the codebook useful for understanding particular variables. We have already limited the data to individuals who are working (EMPSTAT=10), have a job but did not work last week (EMPSTAT==12), or are unemployed (EMPSTAT==21).\nSuppose we wanted to use the panel dimension to measure transition rates. Here is a simple way to do that by simply measuring transitions between January and Feburary.\n\ndata[!,:E] .= data.EMPSTAT.&lt;21 #&lt;- code the employment variable\n\ndata_jan = @chain data begin\n    @subset :MONTH.==1\n    @select :CPSIDP :AGE :SEX :EDUC :RACE :E\n    @rename :E_lag = :E\nend\n\ndata_merged = @chain data begin\n    @subset :MONTH.==2\n    @select :CPSIDP :E\n    innerjoin(data_jan,on=:CPSIDP)\nend\n\n41262×7 DataFrame41237 rows omitted\n\n\n\nRow\nCPSIDP\nE\nAGE\nSEX\nEDUC\nRACE\nE_lag\n\n\n\nInt64\nBool\nInt64\nInt64\nInt64\nInt64\nBool\n\n\n\n\n1\n20161200000201\ntrue\n72\n1\n81\n100\ntrue\n\n\n2\n20180100000301\ntrue\n66\n1\n111\n100\ntrue\n\n\n3\n20180100000302\ntrue\n61\n2\n111\n100\ntrue\n\n\n4\n20170100000901\ntrue\n23\n2\n124\n100\ntrue\n\n\n5\n20170100000902\ntrue\n24\n2\n124\n100\ntrue\n\n\n6\n20170100001001\ntrue\n59\n2\n111\n200\ntrue\n\n\n7\n20170100001002\ntrue\n53\n1\n81\n200\ntrue\n\n\n8\n20171200001201\ntrue\n24\n2\n73\n200\ntrue\n\n\n9\n20161200000801\ntrue\n60\n1\n124\n100\ntrue\n\n\n10\n20161200000802\ntrue\n57\n2\n123\n100\ntrue\n\n\n11\n20170100001401\nfalse\n50\n2\n73\n200\nfalse\n\n\n12\n20170100001403\ntrue\n18\n1\n81\n200\ntrue\n\n\n13\n20170100001405\ntrue\n29\n1\n50\n200\ntrue\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n41251\n20161107451001\ntrue\n59\n1\n92\n100\ntrue\n\n\n41252\n20161107451901\ntrue\n59\n1\n91\n100\ntrue\n\n\n41253\n20171107237801\ntrue\n45\n1\n91\n100\ntrue\n\n\n41254\n20171107237802\ntrue\n37\n2\n123\n100\ntrue\n\n\n41255\n20171207232201\ntrue\n41\n1\n111\n100\ntrue\n\n\n41256\n20171207232202\ntrue\n41\n2\n73\n100\ntrue\n\n\n41257\n20161107452301\ntrue\n38\n1\n73\n100\ntrue\n\n\n41258\n20161107452302\ntrue\n29\n2\n73\n100\ntrue\n\n\n41259\n20170107445501\ntrue\n41\n1\n73\n100\ntrue\n\n\n41260\n20171207232601\ntrue\n42\n1\n111\n100\ntrue\n\n\n41261\n20171207232602\ntrue\n43\n2\n123\n100\ntrue\n\n\n41262\n20171207232604\ntrue\n17\n1\n60\n100\ntrue\n\n\n\n\n\n\nSo now we can calculate the overall transition rate out of unemployment:\n\n@combine data_merged begin\n    :EU =  1-mean(:E[:E_lag.==1])\n    :UE = mean(:E[:E_lag.==0])\nend \n\n1×2 DataFrame\n\n\n\nRow\nEU\nUE\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n0.00997824\n0.377255\n\n\n\n\n\n\nSo here we’re estimating a very low separation rate and a pretty high hazard rate out of unemployment.\n\n\nObservable heterogeneity\nNext we’ll define a very simple education classification (Bachelor’s degree or not) and race classification (white vs non-white), and use groupby to calculate rates separately by demographics:\n\n@chain data_merged begin\n    @transform begin\n        :bachelors = :EDUC.&gt;=111\n        :nonwhite = :RACE.!=100 \n    end\n    groupby([:bachelors,:nonwhite,:SEX])\n    @combine begin\n       :EU =  1-mean(:E[:E_lag.==1])\n       :UE = mean(:E[:E_lag.==0])\n    end \nend\n\n8×5 DataFrame\n\n\n\nRow\nbachelors\nnonwhite\nSEX\nEU\nUE\n\n\n\nBool\nBool\nInt64\nFloat64\nFloat64\n\n\n\n\n1\nfalse\nfalse\n1\n0.0131\n0.40257\n\n\n2\nfalse\nfalse\n2\n0.0110061\n0.385417\n\n\n3\nfalse\ntrue\n1\n0.0159176\n0.338346\n\n\n4\nfalse\ntrue\n2\n0.0150977\n0.300885\n\n\n5\ntrue\nfalse\n1\n0.00447284\n0.315315\n\n\n6\ntrue\nfalse\n2\n0.00601388\n0.489362\n\n\n7\ntrue\ntrue\n1\n0.00547303\n0.342105\n\n\n8\ntrue\ntrue\n2\n0.00836237\n0.290323\n\n\n\n\n\n\nWhat do these differences in transition rates tell you about how we should extend the simple model with homogenous parameters?"
  },
  {
    "objectID": "assignments/Assignment-2.html",
    "href": "assignments/Assignment-2.html",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "Consider the following extension of the undirected search model. Let \\(X_{n}\\) be a vector of demographics for person \\(n\\):\n\\[ X_{n} = [1,\\ C_{n},\\ F_{n},\\ R_{n}] \\]\nwhere \\(C_{n}\\) is a dummy variable that indicates if an individual has a college degree, \\(F_{n}\\) is a dummy variable indicating that an individual is female, and \\(R_{n}\\) is a dummy that indicates if person \\(n\\) reports their race as not “white”. Define a new set of parameters that depend on these observables:\n\nThe flow value of unemployment is \\(b(X) = X\\beta_{b}\\)\nThe probability of job destruction is \\[ \\delta(X) = \\frac{\\exp(X\\gamma_{\\delta})}{1+\\exp(X\\gamma_{\\delta})} \\]\nThe probability of a job offer is \\[ \\lambda(X) = \\frac{\\exp(X\\gamma_{\\lambda})}{1+\\exp(X\\gamma_{\\lambda})} \\]\n\\(\\beta\\) takes a value of 0.995.\nWage offers are drawn from a log normal distribution with mean \\(\\mu(X) = X\\gamma_{\\mu}\\) and standard deviation \\(\\sigma(X) = \\exp(X\\gamma_{\\sigma})\\)\nLog wages are observed with measurement error: \\[ \\log(W^{o}_{n}) = \\log(W_{n}) + \\zeta_{n} \\] where \\(\\zeta_{n}\\sim\\mathcal{N}(0,\\sigma^2_{\\zeta})\\).\n\nSo the parameters of the model are:\n\\[ \\theta = (\\gamma_{b},\\gamma_{\\delta},\\gamma_{\\lambda},\\gamma_{\\mu},\\gamma_{\\sigma},\\sigma^2_{\\zeta}) \\]\nWe are going to estimate this model on CPS data. Here is code to import the data and impute wages for workers who are not paid by the hour. This code also limits to observations in January so that it is a single cross-section, although you could choose a different month if you wanted. I also convert weekly unemployment durations to monthly.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\ndata = @chain data begin\n    @transform :E = :EMPSTAT.&lt;21\n    @transform @byrow :wage = begin\n        if :PAIDHOUR==0\n            return missing\n        elseif :PAIDHOUR==2\n            if :HOURWAGE&lt;99.99 && :HOURWAGE&gt;0\n                return :HOURWAGE\n            else\n                return missing\n            end\n        elseif :PAIDHOUR==1\n            if :EARNWEEK&gt;0 && :UHRSWORKT.&lt;997\n                return :EARNWEEK / :UHRSWORKT\n            else\n                return missing\n            end\n        end\n    end\n    @subset :MONTH.==1\n    @select :AGE :SEX :RACE :EDUC :wage :E :DURUNEMP\n    @transform :DURUNEMP = round.(:DURUNEMP .* 12/52) #&lt;- we convert weekly unemployment durations to monthly since we have a monthly model\nend\n\n61364×7 DataFrame61339 rows omitted\n\n\n\nRow\nAGE\nSEX\nRACE\nEDUC\nwage\nE\nDURUNEMP\n\n\n\nInt64\nInt64\nInt64\nInt64\nFloat64?\nBool\nFloat64\n\n\n\n\n1\n72\n1\n100\n81\nmissing\ntrue\n231.0\n\n\n2\n66\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n3\n61\n2\n100\n111\nmissing\ntrue\n231.0\n\n\n4\n52\n2\n200\n73\n20.84\ntrue\n231.0\n\n\n5\n19\n2\n200\n73\n10.0\ntrue\n231.0\n\n\n6\n56\n2\n200\n111\n25.0\ntrue\n231.0\n\n\n7\n22\n2\n200\n81\n9.5\ntrue\n231.0\n\n\n8\n23\n2\n100\n124\nmissing\ntrue\n231.0\n\n\n9\n24\n2\n100\n124\nmissing\ntrue\n231.0\n\n\n10\n59\n2\n200\n111\nmissing\ntrue\n231.0\n\n\n11\n53\n1\n200\n81\nmissing\ntrue\n231.0\n\n\n12\n24\n2\n200\n73\nmissing\ntrue\n231.0\n\n\n13\n60\n1\n100\n124\nmissing\ntrue\n231.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n61353\n41\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n61354\n41\n2\n100\n73\nmissing\ntrue\n231.0\n\n\n61355\n38\n1\n100\n73\nmissing\ntrue\n231.0\n\n\n61356\n29\n2\n100\n73\nmissing\ntrue\n231.0\n\n\n61357\n71\n2\n100\n73\n12.0\ntrue\n231.0\n\n\n61358\n45\n1\n100\n92\n21.25\ntrue\n231.0\n\n\n61359\n41\n1\n100\n73\nmissing\ntrue\n231.0\n\n\n61360\n42\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n61361\n43\n2\n100\n123\nmissing\ntrue\n231.0\n\n\n61362\n17\n1\n100\n60\nmissing\ntrue\n231.0\n\n\n61363\n32\n2\n100\n81\nmissing\ntrue\n231.0\n\n\n61364\n30\n2\n100\n81\nmissing\ntrue\n231.0\n\n\n\n\n\n\n\n\n\nFollowing your notes from class, write a function that, given a set of parameters, solves the reservation wage for each unique combination of the variables in \\(X\\) (there are 8 total).\n\n\n\nWrite a function that takes a single observation from the cross-section and calculates the log-likelihood of that observation given the model solution, current parameters, and observables \\(X_{n}\\).\n\n\n\nWrite a function that iterates over every observation in the data and calculates the log-likelihood of the data given parameters.\n\n\nYou may find that these functions work faster if you pull the data you need out of DataFrame format and save it as arrays or vectors with known type. For example, I would recommend creating a flag for missing wage data and a default value for those missing wages, and iterating over those objects:\n\nwage_missing = ismissing.(data.wage)\nwage = coalesce.(data.wage,1.)\n# creat a named tuple with all variables to conveniently pass to the log-likelihood:\nd = (;logwage = log.(wage),wage_missing,E = data.E) #&lt;- you will need to add your demographics as well.\n\n(logwage = [0.0, 0.0, 0.0, 3.0368742168851663, 2.302585092994046, 3.2188758248682006, 2.2512917986064953, 0.0, 0.0, 0.0  …  0.0, 0.0, 2.4849066497880004, 3.056356895370426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 0, 0, 0, 0, 1, 1, 1  …  1, 1, 0, 0, 1, 1, 1, 1, 1, 1], E = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n\n\n\n\nThese data are a subsample of the IPUMS CPS data available from cps.ipums.org. Any use of these data should be cited as follows:\nSarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D030.V11.0\nThe CPS data file is intended only for exercises as part of ECON8208. Individuals are not to redistribute the data without permission. Contact ipums@umn.edu for redistribution requests. For all other uses of these data, please access data directly via cps.ipums.org."
  },
  {
    "objectID": "assignments/Assignment-2.html#setup",
    "href": "assignments/Assignment-2.html#setup",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "Consider the following extension of the undirected search model. Let \\(X_{n}\\) be a vector of demographics for person \\(n\\):\n\\[ X_{n} = [1,\\ C_{n},\\ F_{n},\\ R_{n}] \\]\nwhere \\(C_{n}\\) is a dummy variable that indicates if an individual has a college degree, \\(F_{n}\\) is a dummy variable indicating that an individual is female, and \\(R_{n}\\) is a dummy that indicates if person \\(n\\) reports their race as not “white”. Define a new set of parameters that depend on these observables:\n\nThe flow value of unemployment is \\(b(X) = X\\beta_{b}\\)\nThe probability of job destruction is \\[ \\delta(X) = \\frac{\\exp(X\\gamma_{\\delta})}{1+\\exp(X\\gamma_{\\delta})} \\]\nThe probability of a job offer is \\[ \\lambda(X) = \\frac{\\exp(X\\gamma_{\\lambda})}{1+\\exp(X\\gamma_{\\lambda})} \\]\n\\(\\beta\\) takes a value of 0.995.\nWage offers are drawn from a log normal distribution with mean \\(\\mu(X) = X\\gamma_{\\mu}\\) and standard deviation \\(\\sigma(X) = \\exp(X\\gamma_{\\sigma})\\)\nLog wages are observed with measurement error: \\[ \\log(W^{o}_{n}) = \\log(W_{n}) + \\zeta_{n} \\] where \\(\\zeta_{n}\\sim\\mathcal{N}(0,\\sigma^2_{\\zeta})\\).\n\nSo the parameters of the model are:\n\\[ \\theta = (\\gamma_{b},\\gamma_{\\delta},\\gamma_{\\lambda},\\gamma_{\\mu},\\gamma_{\\sigma},\\sigma^2_{\\zeta}) \\]\nWe are going to estimate this model on CPS data. Here is code to import the data and impute wages for workers who are not paid by the hour. This code also limits to observations in January so that it is a single cross-section, although you could choose a different month if you wanted. I also convert weekly unemployment durations to monthly.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\ndata = @chain data begin\n    @transform :E = :EMPSTAT.&lt;21\n    @transform @byrow :wage = begin\n        if :PAIDHOUR==0\n            return missing\n        elseif :PAIDHOUR==2\n            if :HOURWAGE&lt;99.99 && :HOURWAGE&gt;0\n                return :HOURWAGE\n            else\n                return missing\n            end\n        elseif :PAIDHOUR==1\n            if :EARNWEEK&gt;0 && :UHRSWORKT.&lt;997\n                return :EARNWEEK / :UHRSWORKT\n            else\n                return missing\n            end\n        end\n    end\n    @subset :MONTH.==1\n    @select :AGE :SEX :RACE :EDUC :wage :E :DURUNEMP\n    @transform :DURUNEMP = round.(:DURUNEMP .* 12/52) #&lt;- we convert weekly unemployment durations to monthly since we have a monthly model\nend\n\n61364×7 DataFrame61339 rows omitted\n\n\n\nRow\nAGE\nSEX\nRACE\nEDUC\nwage\nE\nDURUNEMP\n\n\n\nInt64\nInt64\nInt64\nInt64\nFloat64?\nBool\nFloat64\n\n\n\n\n1\n72\n1\n100\n81\nmissing\ntrue\n231.0\n\n\n2\n66\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n3\n61\n2\n100\n111\nmissing\ntrue\n231.0\n\n\n4\n52\n2\n200\n73\n20.84\ntrue\n231.0\n\n\n5\n19\n2\n200\n73\n10.0\ntrue\n231.0\n\n\n6\n56\n2\n200\n111\n25.0\ntrue\n231.0\n\n\n7\n22\n2\n200\n81\n9.5\ntrue\n231.0\n\n\n8\n23\n2\n100\n124\nmissing\ntrue\n231.0\n\n\n9\n24\n2\n100\n124\nmissing\ntrue\n231.0\n\n\n10\n59\n2\n200\n111\nmissing\ntrue\n231.0\n\n\n11\n53\n1\n200\n81\nmissing\ntrue\n231.0\n\n\n12\n24\n2\n200\n73\nmissing\ntrue\n231.0\n\n\n13\n60\n1\n100\n124\nmissing\ntrue\n231.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n61353\n41\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n61354\n41\n2\n100\n73\nmissing\ntrue\n231.0\n\n\n61355\n38\n1\n100\n73\nmissing\ntrue\n231.0\n\n\n61356\n29\n2\n100\n73\nmissing\ntrue\n231.0\n\n\n61357\n71\n2\n100\n73\n12.0\ntrue\n231.0\n\n\n61358\n45\n1\n100\n92\n21.25\ntrue\n231.0\n\n\n61359\n41\n1\n100\n73\nmissing\ntrue\n231.0\n\n\n61360\n42\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n61361\n43\n2\n100\n123\nmissing\ntrue\n231.0\n\n\n61362\n17\n1\n100\n60\nmissing\ntrue\n231.0\n\n\n61363\n32\n2\n100\n81\nmissing\ntrue\n231.0\n\n\n61364\n30\n2\n100\n81\nmissing\ntrue\n231.0"
  },
  {
    "objectID": "assignments/Assignment-2.html#part-1",
    "href": "assignments/Assignment-2.html#part-1",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "Following your notes from class, write a function that, given a set of parameters, solves the reservation wage for each unique combination of the variables in \\(X\\) (there are 8 total)."
  },
  {
    "objectID": "assignments/Assignment-2.html#part-2",
    "href": "assignments/Assignment-2.html#part-2",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "Write a function that takes a single observation from the cross-section and calculates the log-likelihood of that observation given the model solution, current parameters, and observables \\(X_{n}\\)."
  },
  {
    "objectID": "assignments/Assignment-2.html#part-3",
    "href": "assignments/Assignment-2.html#part-3",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "Write a function that iterates over every observation in the data and calculates the log-likelihood of the data given parameters.\n\n\nYou may find that these functions work faster if you pull the data you need out of DataFrame format and save it as arrays or vectors with known type. For example, I would recommend creating a flag for missing wage data and a default value for those missing wages, and iterating over those objects:\n\nwage_missing = ismissing.(data.wage)\nwage = coalesce.(data.wage,1.)\n# creat a named tuple with all variables to conveniently pass to the log-likelihood:\nd = (;logwage = log.(wage),wage_missing,E = data.E) #&lt;- you will need to add your demographics as well.\n\n(logwage = [0.0, 0.0, 0.0, 3.0368742168851663, 2.302585092994046, 3.2188758248682006, 2.2512917986064953, 0.0, 0.0, 0.0  …  0.0, 0.0, 2.4849066497880004, 3.056356895370426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 0, 0, 0, 0, 1, 1, 1  …  1, 1, 0, 0, 1, 1, 1, 1, 1, 1], E = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
  },
  {
    "objectID": "assignments/Assignment-3.html",
    "href": "assignments/Assignment-3.html",
    "title": "Assignment 3: Estimating a Search Model",
    "section": "",
    "text": "You are going to build on the work you did last week so you will need all of that code.\n\n\nEstimate the parameters of the model using maximum likelihood on a single month of the CPS data. You will find a package like Optim helpful.\n\n\n\nWrite code to calculate the standard errors and create a table that reports parameter estimates with standard errors. Remember from class that you can calculate standard errors two different ways with MLE."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-1",
    "href": "assignments/Assignment-3.html#part-1",
    "title": "Assignment 3: Estimating a Search Model",
    "section": "",
    "text": "Estimate the parameters of the model using maximum likelihood on a single month of the CPS data. You will find a package like Optim helpful."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-2",
    "href": "assignments/Assignment-3.html#part-2",
    "title": "Assignment 3: Estimating a Search Model",
    "section": "",
    "text": "Write code to calculate the standard errors and create a table that reports parameter estimates with standard errors. Remember from class that you can calculate standard errors two different ways with MLE."
  },
  {
    "objectID": "assignments/Assignment-2.html#a-disclaimer-for-ipums-cps-data",
    "href": "assignments/Assignment-2.html#a-disclaimer-for-ipums-cps-data",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "These data are a subsample of the IPUMS CPS data available from cps.ipums.org. Any use of these data should be cited as follows:\nSarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D030.V11.0\nThe CPS data file is intended only for exercises as part of ECON8208. Individuals are not to redistribute the data without permission. Contact ipums@umn.edu for redistribution requests. For all other uses of these data, please access data directly via cps.ipums.org."
  },
  {
    "objectID": "recitations/recitation-2.html#a-disclaimer-for-ipums-cps-data",
    "href": "recitations/recitation-2.html#a-disclaimer-for-ipums-cps-data",
    "title": "Recitation 2",
    "section": "A Disclaimer for IPUMS CPS data",
    "text": "A Disclaimer for IPUMS CPS data\nThese data are a subsample of the IPUMS CPS data available from cps.ipums.org. Any use of these data should be cited as follows:\nSarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D030.V11.0\nThe CPS data file is intended only for exercises as part of ECON8208. Individuals are not to redistribute the data without permission. Contact ipums@umn.edu for redistribution requests. For all other uses of these data, please access data directly via cps.ipums.org."
  }
]