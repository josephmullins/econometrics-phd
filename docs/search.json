[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Datasets",
    "section": "",
    "text": "For the practical applications in this course we will use three datasets:\n\nMonthly CPS data from IPUMS.\nCounty-level panel data on entry and exit of club stores from Dearing and Blevins (2024)\nPSID panel data on earnings, consumption, and assets from Arellano, Blundell, and Bonhomme (2018)\n\nThese data are included in the course git repo, and you should be able to run all of the example code in this class using the relative file paths in the git repo if you clone it. Alternatively you can download these datasets and save them wherever you wish, but you will need to edit file paths accordingly.\n\n\n\n\nReferences\n\nArellano, Manuel, Richard Blundell, and Stephane Bonhomme. 2018. “Nonlinear Persistence and Partial Insurance: Income and Consumption Dynamics in the PSID.” AEA Papers and Proceedings 108 (May): 281–86. https://doi.org/10.1257/pandp.20181049.\n\n\nDearing, Adam, and Jason R. Blevins. 2024. “Efficient and Convergent Sequential Pseudo-Likelihood Estimation of Dynamic Discrete Games.” Review of Economic Studies."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to this course! Here you’ll find details on the syllabus."
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Syllabus",
    "section": "Topics",
    "text": "Topics\n\nIdentification, Credible Inference, and Marschak’s Maxim\nWe formally define identification and discuss (via examples) what people really mean when they talk about identification and credible inference. We use the Generalized Roy Model to compare identification via functional form to nonparametric identification.\nWe introduce Marschak’s Maxim as a guide for doing empirical model-based research.\n\nReading\nThe two survey articles by Keane (2010) (link) and Angrist and Pischke (2010) (link) - although aging - provide two important perspectives on the issues of credible inference in economics. Low and Meghir (2017) provide a nice review of the advantages of the structural approach.\nThe original paper by Marschak (1953) may be of interest. Heckman and Vytlacil (2007) provide a nice discussion of Marschak’s Maxim in the context of policy evaluation. They introduce (Heckman and Vytlacil 2005; Carneiro, Heckman, and Vytlacil 2011) the Marginal Treatment Effect as a tool for thinking about quasi-experimental estimators and policy evaluation.\n\n\n\nExtremum Estimators\nWe introduce the concept of an extremum estimator and discuss conditions under which this estimator has good asymptotic properties, with specific applications to maximum likelihood, minimum distance, and generalized method of moments estimators. We discuss optimal weighting of the relative efficiency properties of these estimators.\n\nReading\nThis section relies heavily on the Newey and McFadden (1994) chapter of the Handbook of Econometrics. Although not necessary, Hayashi (2011) provides a very thorough treatment of all of these estimators.\n\n\n\nSimulation Methods\nWe introduce simulation methods for the estimation of structural models, including the Simulated Method of Moments, Indirect Inference, and the Bootstrap method for inference.\n\nReading\nYou may find the Horowitz (2001) handbook chapter useful. Cameron and Trivedi (2005) provide a useful discussion of simulation-based estimators in their textbook.\n\n\n\nPanel Data Methods\nWe talk about individual heterogeneity and discuss the use of panel data for detecting individual heterogeneity in data.\n\n\nDiscrete Choice and Dynamic Discrete Choice\nWe review some of the formalities of discrete choice models and consider estimation of these models in the presence of dynamics."
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThere will be 7 problem sets. Your best 5 of these 7 problem sets will be worth 20%. Hence, you can skip two if you want.\nHere is the proposed timeline of due dates. Submissions must be made through Canvas as a notebook (e.g. jupyter or quarto) formatted to html with printed output.\n\n\n\nAssignment\nDue Date\n\n\n\n\nAssignment 1\nMarch 22\n\n\nAssignment 2\nMarch 29\n\n\nAssignment 3\nApril 5\n\n\nAssignment 4\nApril 12\n\n\nAssignment 5\nApril 19\n\n\nAssignment 6\nApril 26\n\n\nAssignment 7\nMay 3"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\nI will provide a link on Canvas to sign up for my weekly office hours."
  },
  {
    "objectID": "models/entry-exit.html",
    "href": "models/entry-exit.html",
    "title": "Symmetric Duopoly Model of Entry/Exit",
    "section": "",
    "text": "Here are the basic ingredients of the model:\n\nThere are two firms indexed by \\(f\\in\\{0,1\\}\\)\nThere are \\(M\\) markets indexed by \\(m\\)\nTime is discrete and indexed by \\(t\\)\nEach firm makes an entry decision every period. We let \\(j\\in\\{0,1\\}\\) index this decision to enter or not. Let \\(j(f,m,t)\\) indicate the choice of firm \\(f\\) in market \\(m\\) in period \\(t\\).\nWe let \\(a_{f,m,t}=j(f,m,t-1)\\) indicate whether firm \\(f\\) is active in market \\(m\\) in period \\(t\\), which means they entered in the previous period.\nLet \\(x_{m}\\) be a market-level unobservable that shifts the profitability of operations in market \\(m\\).\nIn addition to the observed states, each firm draws a pair of idiosyncatic shocks to payoffs in each period, \\(\\epsilon_{f}=[\\epsilon_{f0},\\epsilon_{f1}]\\) that is private information to the firm and is iid over markets, firms, and time periods.\nFirms make their decisions in each period simultaneously\n\nTo simplify notation, suppress dependance of outcomes on the market \\(m\\) and time period \\(t\\). Because we are writing a symmetric model, we will also suppress dependence on \\(f\\). The deterministic component of the payoff to entering is a function of the market primitives (\\(x\\)), the firm’s activity status (\\(a\\)), and the other firm’s entry decision \\(j^\\prime\\):\n\\[ u_{1}(x,a,j^{\\prime}) = \\phi_{0} + \\phi_{1}x - \\phi_{2}j^\\prime - \\phi_{3}(1-a) \\]\nThe payoff to not entering is simply:\n\\[{u}_{0}(x,a) = \\phi_{4}a \\]\nBefore characterizing the solution to the firm’s problem, let’s code up these payoff functions:\n\n\nu1(x,a,j′,ϕ) = ϕ[1] + ϕ[2]*x - ϕ[3]j′ + ϕ[4]*(1-a)\nu0(a,ϕ) = a * ϕ[5]\n\nu0 (generic function with 1 method)\n\n\n\n\n\nLet \\(j^*(x,a,a',\\epsilon)\\) be the firm’s optimal decision given the state and the idiosyncratic shock. We will focus on symmetric equilibria so this policy function is sufficient to describe the behavior of both firms.\nThe value to either firm of arriving in a period with state \\((x,a,a')\\) can be written recursively as:\n\\[\n\\begin{split}\nV(x,a,a') = \\mathbb{E}_{\\epsilon,\\epsilon'}\\max\\{u_{1}(x,a,j^*(x,a,a',\\epsilon'))+\\epsilon_{1} + \\beta V(x,1,j^*(x,a,a',\\epsilon')), \\\\ u_{0}(x,a) + \\epsilon_{0} + \\beta V(x,0,j^*(x,a,a',\\epsilon'))\\}\n\\end{split}\n\\]\nDefine the optimal choice probability in equilibrium as:\n\\[ p(x,a,a') = \\int_{\\epsilon}j^*(x,a,a',\\epsilon)dF(\\epsilon) \\]\nWith this in hand we can integrate out the other firm’s shocks \\(\\epsilon\\)’ to get:\n\\[\n\\begin{split}\nV(x,a,a') = \\mathbb{E}_{\\epsilon}\\max\\{\\phi_{0}+\\phi_{1}x - \\phi_{2}p(x,a',a) +\\epsilon_{1} + \\beta [p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)], \\\\\na \\phi_{4} + \\epsilon_{0} + \\beta [p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)]\\}\n\\end{split}\n\\]\nDefine the choice-specific values as:\n\\[ v_{1}(x,a,a') = \\phi_{0}+\\phi_{1}x - \\phi_{2}p(x,a',a) + \\beta [p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)] \\]\nand\n\\[ v_{0}(x,a,a') = a \\phi_{4} + \\beta [p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)] \\]\nSo assuming that \\(\\epsilon\\) is distributed as type I extreme value random variable with location parameter 0 and scale parameter 1 we get analytical expressions for the choice probabilities and the expected value of the maximum:\n\\[ V(x) = \\gamma + \\log\\left(\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))\\right)\\]\nwhere \\(\\gamma\\) is the Euler-Mascheroni constant and\n\\[ p(x,a,a') = \\frac{\\exp(v_{1}(x,a,a'))}{\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))} \\]\nBefore we define equilibrium and think about solving the model, let’s quickly write up the mapping between the other firm’s choice probabilities and the choice values:\n\n# Fixing x, assume that V is stored as a 2 x 2 array\n# The argument p is the current guess of p(x,a',a)\nfunction choice_values(x,a,p,V,ϕ,β)\n    v0 = u0(a,ϕ) + β * p * V[1,2] + β * (1-p) * V[1,1]\n    v1 = u1(x,a,p,ϕ) + β * p * V[2,2] + β * (1-p) * V[2,1]\n    return v0,v1\nend\n\nchoice_values (generic function with 1 method)\n\n\nIn principle we could iterate on this mapping to find (for a fixed \\(p\\)), the firm’s optimal solution. But that won’t be an efficient way to try and solve for the equilibrium.\n\n\n\nThe solution concept for this model is Markov Perfect Equilibrium. Fixing the market \\(x\\), here the equilibrium be characterized as a fixed point in the value function \\(V\\) and choice probabilities, \\(p\\). In words, equilibrium is summarized by a \\(V\\) and a \\(p\\) such that:\n\nGiven \\(p\\), \\(V\\) is a fixed point in the recursive formulation of values; and\n\\(p\\) are the optimal choice probabilities of each firm given \\(V\\) and given the other firm’s choice probabilities are \\(p\\).\n\nHow should we solve for this symmetric equilibrium? We could try iterating on \\(V\\) and \\(p\\) as follows:\n\n# V is a 2x2 array with values\n# p is a 2x2 array with choice probabilities\nfunction iterate_model(V,p,x,ϕ,β)\n    Vnew = copy(V)\n    pnew = copy(p)\n    for a′ in axes(V,2)\n        for a in axes(V,1)\n            p′ = p[a′,a]\n            v0,v1 = choice_values(x,a-1,p′,V,ϕ,β)\n            pnew[a,a′] = exp(v1) / (exp(v0)+exp(v1))\n            Vnew[a,a′] = log(exp(v0)+exp(v1))\n        end\n    end\n    return Vnew,pnew\nend\n\nfunction solve_by_iteration(x,ϕ,β; max_iter = 1000, verbose = false)\n    V0 = zeros(2,2)\n    p0 = fill(0.1,2,2)\n    err = Inf\n    iter = 1\n    while err&gt;1e-10 && iter&lt;max_iter\n        V1,p1 = iterate_model(V0,p0,x,ϕ,β)\n        err = maximum(abs.(V1 .- V0))\n        if mod(iter,100)==0 && verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        V0 = V1\n        p0 = p1\n        iter += 1\n    end\n    return V0,p0\nend\n\nβ = 0.95\nϕ = 2 * [1.,0.1,0.5,2.,0.5]\nsolve_by_iteration(0.,ϕ,β; verbose = true)\n\nIteration 100, error is 0.04823924738592211\nIteration 200, error is 0.001242310554474102\nIteration 300, error is 5.032709619001707e-5\nIteration 400, error is 2.123540213005981e-6\nIteration 500, error is 8.863101186307176e-8\nIteration 600, error is 3.693557459882868e-9\nIteration 700, error is 1.538467131467769e-10\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nThis seems to work! But notice that it takes a while for the iteration to converge. Also, unlike the single agent case, there is no guarantee that this iteration is always a contraction.\nWe can also solve this model relatively easily using Newton’s Method and the magic of Automatic Differentiation. To do this, we’ll solve over the pair of choice-specific values \\(v_{0}\\) and \\(v_{1}\\) (these encode both values and choice probabilities) and store these values as a vector instead of an array:\n\nusing ForwardDiff, LinearAlgebra\n\n# this function returns V as a 2 x 2 array given the vector of choice specific values in V\nfunction calc_V(v)\n    idx = LinearIndices((2,2,2))\n    [log(exp(v[idx[1,1+a,1+a′]]) + exp(v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n# this function returns choice probabilities as a 2x2 array given the vector v\nfunction calc_p(v)\n    idx = LinearIndices((2,2,2))\n    [1 / (1+exp(v[idx[1,1+a,1+a′]] - v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n\nfunction iterate_model_v(v,x,ϕ,β)\n    idx = LinearIndices((2,2,2)) #&lt;- this is for convenient indexing over v\n    vnew = copy(v)\n    V = calc_V(v)\n    for a′ in axes(idx,3)\n        for a in axes(idx,2)\n            i0′ = idx[1,a′,a] #&lt;- this locates the position in v for v_{0}(x,a',a)\n            i1′ = idx[2,a′,a] #&lt;- this locates the position in v for v_{1}(x,a',a)\n            p = 1 / (1 + exp(v[i0′] - v[i1′]))\n            v0,v1 = choice_values(x,a-1,p,V,ϕ,β)\n            vnew[idx[1,a,a′]] = v0\n            vnew[idx[2,a,a′]] = v1\n        end\n    end\n    return vnew\nend\n\nF(v,x,ϕ,β) = v .- iterate_model_v(v,x,ϕ,β)\nfunction solve_model_newton(x,ϕ,β;max_iter = 10, verbose = false)\n    v = zeros(8)\n    dF(v) = ForwardDiff.jacobian(y-&gt;F(y,x,ϕ,β),v)\n    err = Inf\n    iter = 1\n    while (err&gt;1e-10) && (iter&lt;max_iter)\n        Fv = F(v,x,ϕ,β)\n        dFv = dF(v)\n        vnew = v - inv(dFv) * Fv\n        err = maximum(abs.(Fv))\n        if verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        iter += 1\n        v = vnew\n    end\n    return v\nend\n\nsolve_model_newton(0.,ϕ,β;verbose = true);\n\nIteration 1, error is 6.158489821531948\nIteration 2, error is 1.7766463237555712\nIteration 3, error is 0.056247498263360285\nIteration 4, error is 0.00028434628951856666\nIteration 5, error is 3.4473004006940755e-8\nIteration 6, error is 1.4210854715202004e-14\n\n\nLet’s try timing each solution method to quickly compare:\n\nsolve_model_newton(0.,ϕ,β)\nsolve_by_iteration(0.,ϕ,β)\n\n@time solve_model_newton(0.,ϕ,β)\n@time solve_by_iteration(0.,ϕ,β)\n\n  0.000099 seconds (157 allocations: 58.250 KiB)\n  0.000251 seconds (2.15 k allocations: 201.031 KiB)\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nIn this case Newton’s method is faster. Let’s double check that both methods return the same answer:\n\nv0 = solve_model_newton(0.,ϕ,β)\nV0,p = solve_by_iteration(0.,ϕ,β)\np1 = calc_p(v0)\n[p p1]\n\n2×4 Matrix{Float64}:\n 0.910765   0.99055   0.910765   0.99055\n 0.0524759  0.277297  0.0524759  0.277297\n\n\nLooks good! We can re-use this code when we get to thinking about estimation later on. To do this we will have to solve the model for different values of \\(x_{m}\\), but that can be done by using this code and iterating (potentially in parallel) over different values of \\(x\\).\nIf you play around with parameters, you will see how convergence times may change and that solution methods are not always stable, especially when choice probabilities in equilibrium are very close to one or zero."
  },
  {
    "objectID": "models/entry-exit.html#model-ingredients",
    "href": "models/entry-exit.html#model-ingredients",
    "title": "Symmetric Duopoly Model of Entry/Exit",
    "section": "",
    "text": "Here are the basic ingredients of the model:\n\nThere are two firms indexed by \\(f\\in\\{0,1\\}\\)\nThere are \\(M\\) markets indexed by \\(m\\)\nTime is discrete and indexed by \\(t\\)\nEach firm makes an entry decision every period. We let \\(j\\in\\{0,1\\}\\) index this decision to enter or not. Let \\(j(f,m,t)\\) indicate the choice of firm \\(f\\) in market \\(m\\) in period \\(t\\).\nWe let \\(a_{f,m,t}=j(f,m,t-1)\\) indicate whether firm \\(f\\) is active in market \\(m\\) in period \\(t\\), which means they entered in the previous period.\nLet \\(x_{m}\\) be a market-level unobservable that shifts the profitability of operations in market \\(m\\).\nIn addition to the observed states, each firm draws a pair of idiosyncatic shocks to payoffs in each period, \\(\\epsilon_{f}=[\\epsilon_{f0},\\epsilon_{f1}]\\) that is private information to the firm and is iid over markets, firms, and time periods.\nFirms make their decisions in each period simultaneously\n\nTo simplify notation, suppress dependance of outcomes on the market \\(m\\) and time period \\(t\\). Because we are writing a symmetric model, we will also suppress dependence on \\(f\\). The deterministic component of the payoff to entering is a function of the market primitives (\\(x\\)), the firm’s activity status (\\(a\\)), and the other firm’s entry decision \\(j^\\prime\\):\n\\[ u_{1}(x,a,j^{\\prime}) = \\phi_{0} + \\phi_{1}x - \\phi_{2}j^\\prime - \\phi_{3}(1-a) \\]\nThe payoff to not entering is simply:\n\\[{u}_{0}(x,a) = \\phi_{4}a \\]\nBefore characterizing the solution to the firm’s problem, let’s code up these payoff functions:\n\n\nu1(x,a,j′,ϕ) = ϕ[1] + ϕ[2]*x - ϕ[3]j′ + ϕ[4]*(1-a)\nu0(a,ϕ) = a * ϕ[5]\n\nu0 (generic function with 1 method)"
  },
  {
    "objectID": "models/entry-exit.html#solving-the-firms-problem",
    "href": "models/entry-exit.html#solving-the-firms-problem",
    "title": "Symmetric Duopoly Model of Entry/Exit",
    "section": "",
    "text": "Let \\(j^*(x,a,a',\\epsilon)\\) be the firm’s optimal decision given the state and the idiosyncratic shock. We will focus on symmetric equilibria so this policy function is sufficient to describe the behavior of both firms.\nThe value to either firm of arriving in a period with state \\((x,a,a')\\) can be written recursively as:\n\\[\n\\begin{split}\nV(x,a,a') = \\mathbb{E}_{\\epsilon,\\epsilon'}\\max\\{u_{1}(x,a,j^*(x,a,a',\\epsilon'))+\\epsilon_{1} + \\beta V(x,1,j^*(x,a,a',\\epsilon')), \\\\ u_{0}(x,a) + \\epsilon_{0} + \\beta V(x,0,j^*(x,a,a',\\epsilon'))\\}\n\\end{split}\n\\]\nDefine the optimal choice probability in equilibrium as:\n\\[ p(x,a,a') = \\int_{\\epsilon}j^*(x,a,a',\\epsilon)dF(\\epsilon) \\]\nWith this in hand we can integrate out the other firm’s shocks \\(\\epsilon\\)’ to get:\n\\[\n\\begin{split}\nV(x,a,a') = \\mathbb{E}_{\\epsilon}\\max\\{\\phi_{0}+\\phi_{1}x - \\phi_{2}p(x,a',a) +\\epsilon_{1} + \\beta [p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)], \\\\\na \\phi_{4} + \\epsilon_{0} + \\beta [p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)]\\}\n\\end{split}\n\\]\nDefine the choice-specific values as:\n\\[ v_{1}(x,a,a') = \\phi_{0}+\\phi_{1}x - \\phi_{2}p(x,a',a) + \\beta [p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)] \\]\nand\n\\[ v_{0}(x,a,a') = a \\phi_{4} + \\beta [p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)] \\]\nSo assuming that \\(\\epsilon\\) is distributed as type I extreme value random variable with location parameter 0 and scale parameter 1 we get analytical expressions for the choice probabilities and the expected value of the maximum:\n\\[ V(x) = \\gamma + \\log\\left(\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))\\right)\\]\nwhere \\(\\gamma\\) is the Euler-Mascheroni constant and\n\\[ p(x,a,a') = \\frac{\\exp(v_{1}(x,a,a'))}{\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))} \\]\nBefore we define equilibrium and think about solving the model, let’s quickly write up the mapping between the other firm’s choice probabilities and the choice values:\n\n# Fixing x, assume that V is stored as a 2 x 2 array\n# The argument p is the current guess of p(x,a',a)\nfunction choice_values(x,a,p,V,ϕ,β)\n    v0 = u0(a,ϕ) + β * p * V[1,2] + β * (1-p) * V[1,1]\n    v1 = u1(x,a,p,ϕ) + β * p * V[2,2] + β * (1-p) * V[2,1]\n    return v0,v1\nend\n\nchoice_values (generic function with 1 method)\n\n\nIn principle we could iterate on this mapping to find (for a fixed \\(p\\)), the firm’s optimal solution. But that won’t be an efficient way to try and solve for the equilibrium."
  },
  {
    "objectID": "models/entry-exit.html#equilibrium",
    "href": "models/entry-exit.html#equilibrium",
    "title": "Symmetric Duopoly Model of Entry/Exit",
    "section": "",
    "text": "The solution concept for this model is Markov Perfect Equilibrium. Fixing the market \\(x\\), here the equilibrium be characterized as a fixed point in the value function \\(V\\) and choice probabilities, \\(p\\). In words, equilibrium is summarized by a \\(V\\) and a \\(p\\) such that:\n\nGiven \\(p\\), \\(V\\) is a fixed point in the recursive formulation of values; and\n\\(p\\) are the optimal choice probabilities of each firm given \\(V\\) and given the other firm’s choice probabilities are \\(p\\).\n\nHow should we solve for this symmetric equilibrium? We could try iterating on \\(V\\) and \\(p\\) as follows:\n\n# V is a 2x2 array with values\n# p is a 2x2 array with choice probabilities\nfunction iterate_model(V,p,x,ϕ,β)\n    Vnew = copy(V)\n    pnew = copy(p)\n    for a′ in axes(V,2)\n        for a in axes(V,1)\n            p′ = p[a′,a]\n            v0,v1 = choice_values(x,a-1,p′,V,ϕ,β)\n            pnew[a,a′] = exp(v1) / (exp(v0)+exp(v1))\n            Vnew[a,a′] = log(exp(v0)+exp(v1))\n        end\n    end\n    return Vnew,pnew\nend\n\nfunction solve_by_iteration(x,ϕ,β; max_iter = 1000, verbose = false)\n    V0 = zeros(2,2)\n    p0 = fill(0.1,2,2)\n    err = Inf\n    iter = 1\n    while err&gt;1e-10 && iter&lt;max_iter\n        V1,p1 = iterate_model(V0,p0,x,ϕ,β)\n        err = maximum(abs.(V1 .- V0))\n        if mod(iter,100)==0 && verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        V0 = V1\n        p0 = p1\n        iter += 1\n    end\n    return V0,p0\nend\n\nβ = 0.95\nϕ = 2 * [1.,0.1,0.5,2.,0.5]\nsolve_by_iteration(0.,ϕ,β; verbose = true)\n\nIteration 100, error is 0.04823924738592211\nIteration 200, error is 0.001242310554474102\nIteration 300, error is 5.032709619001707e-5\nIteration 400, error is 2.123540213005981e-6\nIteration 500, error is 8.863101186307176e-8\nIteration 600, error is 3.693557459882868e-9\nIteration 700, error is 1.538467131467769e-10\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nThis seems to work! But notice that it takes a while for the iteration to converge. Also, unlike the single agent case, there is no guarantee that this iteration is always a contraction.\nWe can also solve this model relatively easily using Newton’s Method and the magic of Automatic Differentiation. To do this, we’ll solve over the pair of choice-specific values \\(v_{0}\\) and \\(v_{1}\\) (these encode both values and choice probabilities) and store these values as a vector instead of an array:\n\nusing ForwardDiff, LinearAlgebra\n\n# this function returns V as a 2 x 2 array given the vector of choice specific values in V\nfunction calc_V(v)\n    idx = LinearIndices((2,2,2))\n    [log(exp(v[idx[1,1+a,1+a′]]) + exp(v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n# this function returns choice probabilities as a 2x2 array given the vector v\nfunction calc_p(v)\n    idx = LinearIndices((2,2,2))\n    [1 / (1+exp(v[idx[1,1+a,1+a′]] - v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n\nfunction iterate_model_v(v,x,ϕ,β)\n    idx = LinearIndices((2,2,2)) #&lt;- this is for convenient indexing over v\n    vnew = copy(v)\n    V = calc_V(v)\n    for a′ in axes(idx,3)\n        for a in axes(idx,2)\n            i0′ = idx[1,a′,a] #&lt;- this locates the position in v for v_{0}(x,a',a)\n            i1′ = idx[2,a′,a] #&lt;- this locates the position in v for v_{1}(x,a',a)\n            p = 1 / (1 + exp(v[i0′] - v[i1′]))\n            v0,v1 = choice_values(x,a-1,p,V,ϕ,β)\n            vnew[idx[1,a,a′]] = v0\n            vnew[idx[2,a,a′]] = v1\n        end\n    end\n    return vnew\nend\n\nF(v,x,ϕ,β) = v .- iterate_model_v(v,x,ϕ,β)\nfunction solve_model_newton(x,ϕ,β;max_iter = 10, verbose = false)\n    v = zeros(8)\n    dF(v) = ForwardDiff.jacobian(y-&gt;F(y,x,ϕ,β),v)\n    err = Inf\n    iter = 1\n    while (err&gt;1e-10) && (iter&lt;max_iter)\n        Fv = F(v,x,ϕ,β)\n        dFv = dF(v)\n        vnew = v - inv(dFv) * Fv\n        err = maximum(abs.(Fv))\n        if verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        iter += 1\n        v = vnew\n    end\n    return v\nend\n\nsolve_model_newton(0.,ϕ,β;verbose = true);\n\nIteration 1, error is 6.158489821531948\nIteration 2, error is 1.7766463237555712\nIteration 3, error is 0.056247498263360285\nIteration 4, error is 0.00028434628951856666\nIteration 5, error is 3.4473004006940755e-8\nIteration 6, error is 1.4210854715202004e-14\n\n\nLet’s try timing each solution method to quickly compare:\n\nsolve_model_newton(0.,ϕ,β)\nsolve_by_iteration(0.,ϕ,β)\n\n@time solve_model_newton(0.,ϕ,β)\n@time solve_by_iteration(0.,ϕ,β)\n\n  0.000099 seconds (157 allocations: 58.250 KiB)\n  0.000251 seconds (2.15 k allocations: 201.031 KiB)\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nIn this case Newton’s method is faster. Let’s double check that both methods return the same answer:\n\nv0 = solve_model_newton(0.,ϕ,β)\nV0,p = solve_by_iteration(0.,ϕ,β)\np1 = calc_p(v0)\n[p p1]\n\n2×4 Matrix{Float64}:\n 0.910765   0.99055   0.910765   0.99055\n 0.0524759  0.277297  0.0524759  0.277297\n\n\nLooks good! We can re-use this code when we get to thinking about estimation later on. To do this we will have to solve the model for different values of \\(x_{m}\\), but that can be done by using this code and iterating (potentially in parallel) over different values of \\(x\\).\nIf you play around with parameters, you will see how convergence times may change and that solution methods are not always stable, especially when choice probabilities in equilibrium are very close to one or zero."
  },
  {
    "objectID": "recitations/recitation-2.html",
    "href": "recitations/recitation-2.html",
    "title": "Recitation 2",
    "section": "",
    "text": "Review the code and solution method for the search model."
  },
  {
    "objectID": "recitations/recitation-2.html#part-1",
    "href": "recitations/recitation-2.html#part-1",
    "title": "Recitation 2",
    "section": "",
    "text": "Review the code and solution method for the search model."
  },
  {
    "objectID": "recitations/recitation-2.html#part-2",
    "href": "recitations/recitation-2.html#part-2",
    "title": "Recitation 2",
    "section": "Part 2",
    "text": "Part 2\nReview the code and solution method for the entry/exit model."
  },
  {
    "objectID": "recitations/recitation-2.html#part-3-cps-data",
    "href": "recitations/recitation-2.html#part-3-cps-data",
    "title": "Recitation 2",
    "section": "Part 3: CPS data",
    "text": "Part 3: CPS data\n\nReading the data\nLet’s take a look at data from the CPS on wages, employment status, and labor market transitions. Here is code to read in the data:\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\n\n183277×22 DataFrame183252 rows omitted\n\n\n\nRow\nYEAR\nSERIAL\nMONTH\nHWTFINL\nCPSID\nASECFLAG\nPERNUM\nWTFINL\nCPSIDP\nAGE\nSEX\nRACE\nMARST\nEMPSTAT\nLABFORCE\nUHRSWORKT\nDURUNEMP\nEDUC\nHOURWAGE\nPAIDHOUR\nEARNWEEK\nUHRSWORKORG\n\n\n\nInt64\nInt64\nInt64\nFloat64\nInt64\nInt64?\nInt64\nFloat64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\nInt64\nFloat64\nInt64\n\n\n\n\n1\n2018\n2\n1\n1609.49\n20161200000200\nmissing\n1\n1420.75\n20161200000201\n72\n1\n100\n1\n10\n2\n55\n999\n81\n99.99\n0\n9999.99\n999\n\n\n2\n2018\n3\n1\n1797.04\n20180100000300\nmissing\n1\n2053.27\n20180100000301\n66\n1\n100\n1\n10\n2\n997\n999\n111\n99.99\n0\n9999.99\n999\n\n\n3\n2018\n3\n1\n1797.04\n20180100000300\nmissing\n2\n1797.04\n20180100000302\n61\n2\n100\n1\n10\n2\n997\n999\n111\n99.99\n0\n9999.99\n999\n\n\n4\n2018\n9\n1\n1735.76\n20171000000400\nmissing\n1\n1735.76\n20171000000401\n52\n2\n200\n4\n10\n2\n40\n999\n73\n20.84\n2\n903.0\n999\n\n\n5\n2018\n9\n1\n1735.76\n20171000000400\nmissing\n4\n3069.4\n20171000000404\n19\n2\n200\n6\n10\n2\n40\n999\n73\n10.0\n2\n400.0\n40\n\n\n6\n2018\n10\n1\n1582.77\n20171000000600\nmissing\n1\n1582.77\n20171000000601\n56\n2\n200\n4\n10\n2\n40\n999\n111\n25.0\n2\n1250.0\n999\n\n\n7\n2018\n10\n1\n1582.77\n20171000000600\nmissing\n2\n2409.42\n20171000000602\n22\n2\n200\n6\n10\n2\n30\n999\n81\n9.5\n2\n70.0\n999\n\n\n8\n2018\n11\n1\n1795.64\n20170100000900\nmissing\n1\n1795.64\n20170100000901\n23\n2\n100\n6\n10\n2\n40\n999\n124\n99.99\n0\n9999.99\n999\n\n\n9\n2018\n11\n1\n1795.64\n20170100000900\nmissing\n2\n1795.64\n20170100000902\n24\n2\n100\n6\n10\n2\n40\n999\n124\n99.99\n0\n9999.99\n999\n\n\n10\n2018\n12\n1\n1927.69\n20170100001000\nmissing\n1\n1927.69\n20170100001001\n59\n2\n200\n1\n10\n2\n55\n999\n111\n99.99\n0\n9999.99\n999\n\n\n11\n2018\n12\n1\n1927.69\n20170100001000\nmissing\n2\n2151.5\n20170100001002\n53\n1\n200\n1\n10\n2\n58\n999\n81\n99.99\n0\n9999.99\n999\n\n\n12\n2018\n14\n1\n2926.96\n20171200001200\nmissing\n1\n2926.96\n20171200001201\n24\n2\n200\n6\n10\n2\n40\n999\n73\n99.99\n0\n9999.99\n999\n\n\n13\n2018\n15\n1\n1861.24\n20161200000800\nmissing\n1\n1557.36\n20161200000801\n60\n1\n100\n1\n10\n2\n40\n999\n124\n99.99\n0\n9999.99\n999\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n183266\n2018\n72283\n3\n275.748\n20170107444500\n2\n3\n280.118\n20170107444503\n33\n1\n100\n6\n21\n2\n999\n61\n73\n99.99\n0\n9999.99\n999\n\n\n183267\n2018\n72286\n3\n332.029\n20170307448900\n2\n1\n332.029\n20170307448901\n44\n2\n100\n1\n10\n2\n30\n999\n111\n99.99\n0\n9999.99\n999\n\n\n183268\n2018\n72286\n3\n332.029\n20170307448900\n2\n2\n334.955\n20170307448902\n46\n1\n100\n1\n10\n2\n45\n999\n91\n99.99\n0\n9999.99\n999\n\n\n183269\n2018\n72288\n3\n341.227\n20171207232200\n2\n1\n341.227\n20171207232201\n42\n1\n100\n6\n10\n2\n40\n999\n111\n99.99\n1\n365.0\n999\n\n\n183270\n2018\n72288\n3\n341.227\n20171207232200\n2\n2\n281.741\n20171207232202\n41\n2\n100\n4\n10\n2\n40\n999\n73\n14.0\n2\n560.0\n40\n\n\n183271\n2018\n72289\n3\n256.804\n20170107445500\n2\n1\n256.804\n20170107445501\n42\n1\n100\n4\n10\n2\n45\n999\n73\n99.99\n0\n9999.99\n999\n\n\n183272\n2018\n72291\n3\n281.741\n20171207232600\n2\n1\n507.056\n20171207232601\n42\n1\n100\n1\n10\n2\n40\n999\n111\n99.99\n1\n519.23\n999\n\n\n183273\n2018\n72291\n3\n281.741\n20171207232600\n2\n2\n281.741\n20171207232602\n43\n2\n100\n1\n10\n2\n50\n999\n123\n99.99\n1\n1442.3\n999\n\n\n183274\n2018\n72291\n3\n281.741\n20171207232600\n2\n4\n377.923\n20171207232604\n18\n1\n100\n6\n10\n2\n15\n999\n60\n9.0\n2\n108.0\n12\n\n\n183275\n2018\n72292\n3\n288.99\n20171207232700\n2\n1\n288.99\n20171207232701\n32\n2\n100\n6\n10\n2\n40\n999\n81\n25.0\n2\n1000.0\n40\n\n\n183276\n2018\n72292\n3\n288.99\n20171207232700\n2\n2\n288.99\n20171207232702\n30\n2\n100\n1\n10\n2\n20\n999\n81\n99.99\n0\n9999.99\n999\n\n\n183277\n2018\n72292\n3\n288.99\n20171207232700\n2\n3\n336.057\n20171207232703\n31\n1\n100\n1\n10\n2\n997\n999\n91\n99.99\n1\n1346.0\n999\n\n\n\n\n\n\nAs you can see from the preview of the data, the data is taken from January-March 2018. Here is a quick snippet of code to see how many observations we have on average per person:\n\n@chain data begin\n    groupby(:CPSIDP)\n    @combine :T = length(:EMPSTAT)\n    @combine :average = mean(:T) :frac_panel = mean(:T.&gt;1)\nend\n\n1×2 DataFrame\n\n\n\nRow\naverage\nfrac_panel\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n1.83191\n0.582806\n\n\n\n\n\n\nSo we see that more than half of the individuals in this sample can be found in more than one month of the data.\nThe @chain macro comes from the package DataFramesMeta and is a convenient syntax for composing operations into one block. For example:\n\n@chain x begin\n    func1(y1)\n    func2(y2)\n    func3(y3)\nend\n\nis equivalent to\n\nfunc3(func2(func1(x,y1),y2),y3)\n\n\n\nCalculating some moments\nYou may find the codebook useful for understanding particular variables. We have already limited the data to individuals who are working (EMPSTAT=10), have a job but did not work last week (EMPSTAT==12), or are unemployed (EMPSTAT==21).\nSuppose we wanted to use the panel dimension to measure transition rates. Here is a simple way to do that by simply measuring transitions between January and Feburary.\n\ndata[!,:E] .= data.EMPSTAT.&lt;21 #&lt;- code the employment variable\n\ndata_jan = @chain data begin\n    @subset :MONTH.==1\n    @select :CPSIDP :AGE :SEX :EDUC :RACE :E\n    @rename :E_lag = :E\nend\n\ndata_merged = @chain data begin\n    @subset :MONTH.==2\n    @select :CPSIDP :E\n    innerjoin(data_jan,on=:CPSIDP)\nend\n\n41262×7 DataFrame41237 rows omitted\n\n\n\nRow\nCPSIDP\nE\nAGE\nSEX\nEDUC\nRACE\nE_lag\n\n\n\nInt64\nBool\nInt64\nInt64\nInt64\nInt64\nBool\n\n\n\n\n1\n20161200000201\ntrue\n72\n1\n81\n100\ntrue\n\n\n2\n20180100000301\ntrue\n66\n1\n111\n100\ntrue\n\n\n3\n20180100000302\ntrue\n61\n2\n111\n100\ntrue\n\n\n4\n20170100000901\ntrue\n23\n2\n124\n100\ntrue\n\n\n5\n20170100000902\ntrue\n24\n2\n124\n100\ntrue\n\n\n6\n20170100001001\ntrue\n59\n2\n111\n200\ntrue\n\n\n7\n20170100001002\ntrue\n53\n1\n81\n200\ntrue\n\n\n8\n20171200001201\ntrue\n24\n2\n73\n200\ntrue\n\n\n9\n20161200000801\ntrue\n60\n1\n124\n100\ntrue\n\n\n10\n20161200000802\ntrue\n57\n2\n123\n100\ntrue\n\n\n11\n20170100001401\nfalse\n50\n2\n73\n200\nfalse\n\n\n12\n20170100001403\ntrue\n18\n1\n81\n200\ntrue\n\n\n13\n20170100001405\ntrue\n29\n1\n50\n200\ntrue\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n41251\n20161107451001\ntrue\n59\n1\n92\n100\ntrue\n\n\n41252\n20161107451901\ntrue\n59\n1\n91\n100\ntrue\n\n\n41253\n20171107237801\ntrue\n45\n1\n91\n100\ntrue\n\n\n41254\n20171107237802\ntrue\n37\n2\n123\n100\ntrue\n\n\n41255\n20171207232201\ntrue\n41\n1\n111\n100\ntrue\n\n\n41256\n20171207232202\ntrue\n41\n2\n73\n100\ntrue\n\n\n41257\n20161107452301\ntrue\n38\n1\n73\n100\ntrue\n\n\n41258\n20161107452302\ntrue\n29\n2\n73\n100\ntrue\n\n\n41259\n20170107445501\ntrue\n41\n1\n73\n100\ntrue\n\n\n41260\n20171207232601\ntrue\n42\n1\n111\n100\ntrue\n\n\n41261\n20171207232602\ntrue\n43\n2\n123\n100\ntrue\n\n\n41262\n20171207232604\ntrue\n17\n1\n60\n100\ntrue\n\n\n\n\n\n\nSo now we can calculate the overall transition rate out of unemployment:\n\n@combine data_merged begin\n    :EU =  1-mean(:E[:E_lag.==1])\n    :UE = mean(:E[:E_lag.==0])\nend \n\n1×2 DataFrame\n\n\n\nRow\nEU\nUE\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n0.00997824\n0.377255\n\n\n\n\n\n\nSo here we’re estimating a very low separation rate and a pretty high hazard rate out of unemployment.\n\n\nObservable heterogeneity\nNext we’ll define a very simple education classification (Bachelor’s degree or not) and race classification (white vs non-white), and use groupby to calculate rates separately by demographics:\n\n@chain data_merged begin\n    @transform begin\n        :bachelors = :EDUC.&gt;=111\n        :nonwhite = :RACE.!=100 \n    end\n    groupby([:bachelors,:nonwhite,:SEX])\n    @combine begin\n       :EU =  1-mean(:E[:E_lag.==1])\n       :UE = mean(:E[:E_lag.==0])\n    end \nend\n\n8×5 DataFrame\n\n\n\nRow\nbachelors\nnonwhite\nSEX\nEU\nUE\n\n\n\nBool\nBool\nInt64\nFloat64\nFloat64\n\n\n\n\n1\nfalse\nfalse\n1\n0.0131\n0.40257\n\n\n2\nfalse\nfalse\n2\n0.0110061\n0.385417\n\n\n3\nfalse\ntrue\n1\n0.0159176\n0.338346\n\n\n4\nfalse\ntrue\n2\n0.0150977\n0.300885\n\n\n5\ntrue\nfalse\n1\n0.00447284\n0.315315\n\n\n6\ntrue\nfalse\n2\n0.00601388\n0.489362\n\n\n7\ntrue\ntrue\n1\n0.00547303\n0.342105\n\n\n8\ntrue\ntrue\n2\n0.00836237\n0.290323\n\n\n\n\n\n\nWhat do these differences in transition rates tell you about how we should extend the simple model with homogenous parameters?"
  },
  {
    "objectID": "recitations/recitation-2.html#a-disclaimer-for-ipums-cps-data",
    "href": "recitations/recitation-2.html#a-disclaimer-for-ipums-cps-data",
    "title": "Recitation 2",
    "section": "A Disclaimer for IPUMS CPS data",
    "text": "A Disclaimer for IPUMS CPS data\nThese data are a subsample of the IPUMS CPS data available from cps.ipums.org. Any use of these data should be cited as follows:\nSarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D030.V11.0\nThe CPS data file is intended only for exercises as part of ECON8208. Individuals are not to redistribute the data without permission. Contact ipums@umn.edu for redistribution requests. For all other uses of these data, please access data directly via cps.ipums.org."
  },
  {
    "objectID": "recitations/recitation-1.html",
    "href": "recitations/recitation-1.html",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "We are going to work through an example of “identification via functional form” that we will see in class. The model looks as follows:\n\\[ Y = X\\beta + \\alpha D + \\epsilon - \\varphi\\nu \\]\nand\n\\[ D = \\mathbf{1}\\{X\\beta - \\nu \\geq 0\\} \\]\nwhere \\(\\epsilon\\) and \\(\\nu\\) are independent with \\(\\epsilon\\sim\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\) and \\(\\nu\\sim\\mathcal{N}(0,1)\\). This is equivalent to writing:\n\\[ Y = X\\beta + \\alpha D + \\xi \\]\nwith\n\\[ \\left[\\begin{array}{c}\\xi \\\\ \\nu \\end{array}\\right] \\sim \\mathcal{N}\\left(0,\\left[\\begin{array}{cc}\\sigma^2_{\\epsilon} + \\varphi^2 & -\\varphi \\\\ -\\varphi & 1\\end{array}\\right]\\right).\\]\nLet’s start by writing some code to simulate data from this simple selection model:\n\nusing Random, Distributions\n\nfunction sim_data(X ; β,γ,α,φ,σ_ϵ)\n    N = size(X,1)\n    ν = rand(Normal(),N)\n    ϵ = σ_ϵ * rand(Normal(),N)\n    D = (X * γ .- ν) .&gt; 0\n    Y = X * β .+ α * D .+ ϵ .- φ*ν\n    return Y, D\nend\n\nsim_data (generic function with 1 method)\n\n\nLet’s quickly test this function by selecting some default parameters.\n\nN = 1000\nX = [ones(N) 2*rand(Normal(),N)]\nβ = [0., 1.]\nγ = [0.1, 0.5]\nφ = 1.\nα = 0.6\nσ_ϵ = 0.5\n\nY, D = sim_data(X ; β,γ,α,φ,σ_ϵ)\n\n([1.132860301951354, -3.6466921358145283, -2.7966645365786507, 1.074252425709782, 5.251260632435156, 5.333302836895356, 1.048140172519132, -0.012316247760416288, 1.0187652704332761, 3.398234585007554  …  3.1520102965904524, 6.306622263324812, 3.810961616754668, 0.6280107123658936, 0.23612972322392936, -0.735663499435032, 3.6357910280620107, 1.5955063606600457, 2.4628872110429034, 1.1654458069287195], Bool[1, 0, 0, 1, 1, 1, 1, 0, 1, 1  …  1, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n\n\nTo start, let’s think about estimating the parameters \\(\\gamma\\) using maximum likelihood. We will establish the properties of this estimator in class. Here let’s just focus on numerically how to attack the minimization problem. The log-likelihood of the data D given X is:\n\\[ \\mathcal{L}(\\gamma) = \\sum_{n}l(D_{n}; X_{n},\\gamma) = \\sum_{n=1}^{N}D_{n}\\log(\\Phi(X\\gamma)) + (1-D_{n})\\log(1-\\Phi(X\\gamma)) \\]\nLet’s write up this likelihood function.\n\nfunction log_likelihood(D,X,γ)\n    ll = 0.\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            ll += log(cdf(Normal(),xg))\n        else\n            ll += log(1-cdf(Normal(),xg))\n        end\n    end\n    return ll\nend\nlog_likelihood(D,X,[0.,0.])\n\n-693.1471805599322\n\n\nA necessary condition for the maximum likelihood estimator is:\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{n}s_{n}(\\hat{\\gamma}) = 0\\]\nwhere\n\\[s_{n}(\\gamma) = \\frac{\\partial l(D_{n} ; X_{n},\\gamma)}{\\partial \\gamma} \\]\nis often referred to as the “score” of the likelihood.\n\n\nOptimization is most efficient when we have access to the first and second order derivatives of the function. There is a general class of hill-climbing (or descent in the case of minimization) algorithms that find new guesses \\(\\gamma_{k+1}\\) given \\(\\gamma_{k}\\) as:\n\\[ \\gamma_{k+1} = \\gamma_{k} + \\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\gamma} \\]\nwhere \\(Q\\) is the function being maximized (or minimized). \\(A_{k}\\) defines a direction in which to search (providing weights on the derivatives) and \\(\\lambda_{k}\\) is a scalar variable known as a step-size which is often calculated optimally in each iteration \\(k\\). For Newton’s method, the matrix \\(A_{k}\\) is the inverse of the Hessian of the objective function \\(Q\\). Since the hessian can sometimes be expensive to calculate, other methods use approximations to the Hessian that are cheaper to compute.\nSince we have a simple model, we can calculate derivatives relatively easily. However julia and many other languages now provide packages for Automatic Differentiation, which essentially trace all of the operations inside the function and implement the chain rule. It is very quick! Below we’ll compare a hard-coded derivative to this automatic differentiation.\n\nusing ForwardDiff\n\nfunction deriv_ll(D,X,γ)\n    dll = zeros(2)\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            dl = pdf(Normal(),xg) / cdf(Normal(),xg)\n        else\n            dl = - pdf(Normal(),xg) / (1 - cdf(Normal(),xg))\n        end\n        dll[1] += X[n,1] * dl\n        dll[2] += X[n,2] * dl            \n    end\n    return dll\nend\n\nauto_deriv_ll(D,X,γ) = ForwardDiff.gradient(x-&gt;log_likelihood(D,X,x),γ)\n\nd1 = deriv_ll(D,X,γ)\nd2 = auto_deriv_ll(D,X,γ)\n[d1 d2]\n\n2×2 Matrix{Float64}:\n -31.4069  -31.4069\n -29.5423  -29.5423\n\n\nOk so we’re confident that these functions work as intended, but how do they compare in performance?\n\n@time deriv_ll(D,X,γ);\n@time auto_deriv_ll(D,X,γ);\n\n  0.000028 seconds (1 allocation: 80 bytes)\n  0.000043 seconds (6 allocations: 352 bytes)\n\n\nBoth are quite quick but you can see that we’re not losing much with automatic differentiation. In my experience, the gap between the two methods can narrow for more complicated functions.\n\n\n\nSo now let’s try implementing the maximum likelihood estimator using two different gradient-based algorithms: Newton’s Method (which uses the Hessian), and the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm (which updates search direction using chanegs in the first derivative). For maximum likelihood there is also the Berndt–Hall–Hall–Hausman (BHHH) algorithm which uses the following equality that we will derive in class:\n\\[ \\mathbb{E}[\\nabla{\\gamma'}s(\\gamma)] = -\\mathbb{E}[s(\\gamma)s(\\gamma)'] \\]\nThis formula states that the hessian of the log-likelihood is equal to the negative covariance of the score. This is a method that only works when maximizing the log-likelihood. Unfortunately Optim does not provide an implementation of BHHH but it is worth knowing about.\nWhile Newton’s method requires calculation of the Hessian (second derivatives), BFGS and related methods only require first derivatives. Typically, this makes each iteration quicker but will take more time to converge. Let’s test them.\n\nusing Optim\nmin_objective(x) = -log_likelihood(D,X,x) #&lt;- Optim assumes that we will minimize a function, hence the negative\nγ_guess = zeros(2)\nprintln(\" ---- Using Newton's Method ------ \")\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nprintln(\" ---- Using BFGS ------ \")\nres2 = optimize(min_objective,γ_guess,BFGS(),autodiff=:forward,Optim.Options(show_trace=true))\n[res1.minimizer res2.minimizer γ]\n\n ---- Using Newton's Method ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 4.696846008300781e-5\n     1     5.170501e+02     1.388497e+02\n * time: 0.0003650188446044922\n     2     5.074227e+02     2.790597e+00\n * time: 0.0006308555603027344\n     3     5.074194e+02     1.179917e-04\n * time: 0.0008649826049804688\n     4     5.074194e+02     9.096168e-12\n * time: 0.0010650157928466797\n ---- Using BFGS ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 7.295608520507812e-5\n     1     5.238897e+02     1.767467e+02\n * time: 0.00039386749267578125\n     2     5.236553e+02     1.761320e+02\n * time: 0.0006258487701416016\n     3     5.076641e+02     2.366312e+01\n * time: 0.0008478164672851562\n     4     5.074194e+02     7.988743e-02\n * time: 0.0010349750518798828\n     5     5.074194e+02     4.225729e-03\n * time: 0.0012378692626953125\n     6     5.074194e+02     7.219726e-08\n * time: 0.0014328956604003906\n     7     5.074194e+02     2.127742e-13\n * time: 0.0016629695892333984\n\n\n2×3 Matrix{Float64}:\n 0.0335542  0.0335542  0.1\n 0.472962   0.472962   0.5\n\n\n\n\n\nIn class we will see that\n\\[ \\mathbb{E}[Y|X,D] = X\\beta + \\alpha D - \\varphi\\left[(1-D)\\frac{\\phi(X\\gamma)}{1-\\Phi(X\\gamma)} - D\\frac{\\phi(X\\gamma)}{\\Phi(X\\gamma)}\\right] \\]\nwhich allows \\(\\beta\\) and \\(\\alpha\\) to be estimated by virtue of a functional form assumption on the distribution of unobservables. Numerically we can test this by adding the selection correction term to a regression of \\(Y\\) on \\(X\\) and \\(D\\):\n\nusing LinearAlgebra\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\n4×2 Matrix{Float64}:\n  0.313929   0.0\n  1.10537    1.0\n -0.14238    0.6\n -1.44073   -1.0\n\n\nAt first this seems way off, but if we increase the sample size by an order of magnitude we should (depending on sampling error) see that the estimator is consistent:\n\nN = 10000\nX = [ones(N) 2*rand(Normal(),N)]\nY, D = sim_data(X ; β,γ,α,φ,σ_ϵ)\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\nIter     Function value   Gradient norm \n     0     6.931472e+03     8.736635e+03\n * time: 7.987022399902344e-5\n     1     5.159514e+03     1.376363e+03\n * time: 0.0021448135375976562\n     2     5.054556e+03     3.380701e+01\n * time: 0.00448298454284668\n     3     5.054503e+03     2.126288e-03\n * time: 0.005650043487548828\n     4     5.054503e+03     2.817037e-10\n * time: 0.00681304931640625\n\n\n4×2 Matrix{Float64}:\n -0.0458329   0.0\n  0.987815    1.0\n  0.654354    0.6\n -0.960434   -1.0"
  },
  {
    "objectID": "recitations/recitation-1.html#numerical-optimization-and-automatic-differentiation",
    "href": "recitations/recitation-1.html#numerical-optimization-and-automatic-differentiation",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "Optimization is most efficient when we have access to the first and second order derivatives of the function. There is a general class of hill-climbing (or descent in the case of minimization) algorithms that find new guesses \\(\\gamma_{k+1}\\) given \\(\\gamma_{k}\\) as:\n\\[ \\gamma_{k+1} = \\gamma_{k} + \\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\gamma} \\]\nwhere \\(Q\\) is the function being maximized (or minimized). \\(A_{k}\\) defines a direction in which to search (providing weights on the derivatives) and \\(\\lambda_{k}\\) is a scalar variable known as a step-size which is often calculated optimally in each iteration \\(k\\). For Newton’s method, the matrix \\(A_{k}\\) is the inverse of the Hessian of the objective function \\(Q\\). Since the hessian can sometimes be expensive to calculate, other methods use approximations to the Hessian that are cheaper to compute.\nSince we have a simple model, we can calculate derivatives relatively easily. However julia and many other languages now provide packages for Automatic Differentiation, which essentially trace all of the operations inside the function and implement the chain rule. It is very quick! Below we’ll compare a hard-coded derivative to this automatic differentiation.\n\nusing ForwardDiff\n\nfunction deriv_ll(D,X,γ)\n    dll = zeros(2)\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            dl = pdf(Normal(),xg) / cdf(Normal(),xg)\n        else\n            dl = - pdf(Normal(),xg) / (1 - cdf(Normal(),xg))\n        end\n        dll[1] += X[n,1] * dl\n        dll[2] += X[n,2] * dl            \n    end\n    return dll\nend\n\nauto_deriv_ll(D,X,γ) = ForwardDiff.gradient(x-&gt;log_likelihood(D,X,x),γ)\n\nd1 = deriv_ll(D,X,γ)\nd2 = auto_deriv_ll(D,X,γ)\n[d1 d2]\n\n2×2 Matrix{Float64}:\n -31.4069  -31.4069\n -29.5423  -29.5423\n\n\nOk so we’re confident that these functions work as intended, but how do they compare in performance?\n\n@time deriv_ll(D,X,γ);\n@time auto_deriv_ll(D,X,γ);\n\n  0.000028 seconds (1 allocation: 80 bytes)\n  0.000043 seconds (6 allocations: 352 bytes)\n\n\nBoth are quite quick but you can see that we’re not losing much with automatic differentiation. In my experience, the gap between the two methods can narrow for more complicated functions."
  },
  {
    "objectID": "recitations/recitation-1.html#numerical-optimization-using-optim",
    "href": "recitations/recitation-1.html#numerical-optimization-using-optim",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "So now let’s try implementing the maximum likelihood estimator using two different gradient-based algorithms: Newton’s Method (which uses the Hessian), and the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm (which updates search direction using chanegs in the first derivative). For maximum likelihood there is also the Berndt–Hall–Hall–Hausman (BHHH) algorithm which uses the following equality that we will derive in class:\n\\[ \\mathbb{E}[\\nabla{\\gamma'}s(\\gamma)] = -\\mathbb{E}[s(\\gamma)s(\\gamma)'] \\]\nThis formula states that the hessian of the log-likelihood is equal to the negative covariance of the score. This is a method that only works when maximizing the log-likelihood. Unfortunately Optim does not provide an implementation of BHHH but it is worth knowing about.\nWhile Newton’s method requires calculation of the Hessian (second derivatives), BFGS and related methods only require first derivatives. Typically, this makes each iteration quicker but will take more time to converge. Let’s test them.\n\nusing Optim\nmin_objective(x) = -log_likelihood(D,X,x) #&lt;- Optim assumes that we will minimize a function, hence the negative\nγ_guess = zeros(2)\nprintln(\" ---- Using Newton's Method ------ \")\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nprintln(\" ---- Using BFGS ------ \")\nres2 = optimize(min_objective,γ_guess,BFGS(),autodiff=:forward,Optim.Options(show_trace=true))\n[res1.minimizer res2.minimizer γ]\n\n ---- Using Newton's Method ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 4.696846008300781e-5\n     1     5.170501e+02     1.388497e+02\n * time: 0.0003650188446044922\n     2     5.074227e+02     2.790597e+00\n * time: 0.0006308555603027344\n     3     5.074194e+02     1.179917e-04\n * time: 0.0008649826049804688\n     4     5.074194e+02     9.096168e-12\n * time: 0.0010650157928466797\n ---- Using BFGS ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 7.295608520507812e-5\n     1     5.238897e+02     1.767467e+02\n * time: 0.00039386749267578125\n     2     5.236553e+02     1.761320e+02\n * time: 0.0006258487701416016\n     3     5.076641e+02     2.366312e+01\n * time: 0.0008478164672851562\n     4     5.074194e+02     7.988743e-02\n * time: 0.0010349750518798828\n     5     5.074194e+02     4.225729e-03\n * time: 0.0012378692626953125\n     6     5.074194e+02     7.219726e-08\n * time: 0.0014328956604003906\n     7     5.074194e+02     2.127742e-13\n * time: 0.0016629695892333984\n\n\n2×3 Matrix{Float64}:\n 0.0335542  0.0335542  0.1\n 0.472962   0.472962   0.5"
  },
  {
    "objectID": "recitations/recitation-1.html#identification-via-functional-form",
    "href": "recitations/recitation-1.html#identification-via-functional-form",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "In class we will see that\n\\[ \\mathbb{E}[Y|X,D] = X\\beta + \\alpha D - \\varphi\\left[(1-D)\\frac{\\phi(X\\gamma)}{1-\\Phi(X\\gamma)} - D\\frac{\\phi(X\\gamma)}{\\Phi(X\\gamma)}\\right] \\]\nwhich allows \\(\\beta\\) and \\(\\alpha\\) to be estimated by virtue of a functional form assumption on the distribution of unobservables. Numerically we can test this by adding the selection correction term to a regression of \\(Y\\) on \\(X\\) and \\(D\\):\n\nusing LinearAlgebra\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\n4×2 Matrix{Float64}:\n  0.313929   0.0\n  1.10537    1.0\n -0.14238    0.6\n -1.44073   -1.0\n\n\nAt first this seems way off, but if we increase the sample size by an order of magnitude we should (depending on sampling error) see that the estimator is consistent:\n\nN = 10000\nX = [ones(N) 2*rand(Normal(),N)]\nY, D = sim_data(X ; β,γ,α,φ,σ_ϵ)\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\nIter     Function value   Gradient norm \n     0     6.931472e+03     8.736635e+03\n * time: 7.987022399902344e-5\n     1     5.159514e+03     1.376363e+03\n * time: 0.0021448135375976562\n     2     5.054556e+03     3.380701e+01\n * time: 0.00448298454284668\n     3     5.054503e+03     2.126288e-03\n * time: 0.005650043487548828\n     4     5.054503e+03     2.817037e-10\n * time: 0.00681304931640625\n\n\n4×2 Matrix{Float64}:\n -0.0458329   0.0\n  0.987815    1.0\n  0.654354    0.6\n -0.960434   -1.0"
  },
  {
    "objectID": "assignments/Assignment-3.html",
    "href": "assignments/Assignment-3.html",
    "title": "Assignment 3: Estimating a Search Model",
    "section": "",
    "text": "In this homework you are going to estimate the parameters of the search model for each demographic group individually. That is, you will not impose the parametric restrictions that mapped demographics \\(X\\) to deeper parameters using the \\(\\gamma\\) vectors from last week.\n\n\nFix \\(\\sigma_\\zeta\\) (the standard deviation of measurement error in log wages) to 0.05. Following your work from last week (and recitation this week)write a function that calculates the log-likelihood of a single month of data from the CPS given \\((h,\\delta,\\mu,\\sigma,w^*)\\) where \\(w^*\\) is the reservation wage and \\(h = \\lambda\\times(1-F_{W}(w^*;\\mu,\\sigma))\\).\n\n\n\nUse the log-likelihood function to get maximum likelihood estimates of \\((\\hat{h},\\hat{\\delta},\\hat{\\mu},\\hat{\\sigma},\\hat{w^*})\\) for white men with a college degree.\nWhat is the advantage of estimating \\(h\\) and \\(w^*\\) directly instead of \\(\\lambda\\) and \\(b\\)?\n\n\n\nBack out the implied maximum likelihood estimates of \\(\\hat{\\lambda}\\) and \\(\\hat{b}\\) as a function of the estimated parameters from part (1).\n\n\n\nProvide an estimate of the asymptotic variance of \\((\\hat{h},\\hat{\\delta},\\hat{\\mu},\\hat{\\sigma},\\hat{w^*})\\) using the standard MLE formula.\n\n\n\nRecall that the delta method implies that if \\(\\hat{\\theta}\\) is asymptotically normal with asymptotic variance \\(V\\) then the vector-valued function \\(F(\\hat{\\theta})\\) is also asymptotically normal with:\n\\[ \\sqrt{N}(F(\\hat{\\theta}) - F(\\theta)) \\rightarrow_{d} \\mathcal{N}(0,\\nabla_{\\theta'}FV\\nabla_{\\theta}F') \\]\nUse this fact to estimate the asymptotic variance of \\((\\hat{h},\\hat{\\delta},\\hat{\\mu},\\hat{\\sigma},\\hat{w^*},\\hat{\\lambda},\\hat{b})\\).\n\n\n\nNow report all of your estimates and standard errors for this group. Repeat this exercise for each group.\nIf we thought that the parametric relationships using \\(\\gamma\\) from Homework 2 described the true values of the parameters for each group, how might we use these group-specific estimates to derive estimates of each \\(\\gamma\\)?"
  },
  {
    "objectID": "assignments/Assignment-3.html#part-1",
    "href": "assignments/Assignment-3.html#part-1",
    "title": "Assignment 3: Estimating a Search Model",
    "section": "",
    "text": "Fix \\(\\sigma_\\zeta\\) (the standard deviation of measurement error in log wages) to 0.05. Following your work from last week (and recitation this week)write a function that calculates the log-likelihood of a single month of data from the CPS given \\((h,\\delta,\\mu,\\sigma,w^*)\\) where \\(w^*\\) is the reservation wage and \\(h = \\lambda\\times(1-F_{W}(w^*;\\mu,\\sigma))\\)."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-2",
    "href": "assignments/Assignment-3.html#part-2",
    "title": "Assignment 3: Estimating a Search Model",
    "section": "",
    "text": "Use the log-likelihood function to get maximum likelihood estimates of \\((\\hat{h},\\hat{\\delta},\\hat{\\mu},\\hat{\\sigma},\\hat{w^*})\\) for white men with a college degree.\nWhat is the advantage of estimating \\(h\\) and \\(w^*\\) directly instead of \\(\\lambda\\) and \\(b\\)?"
  },
  {
    "objectID": "assignments/Assignment-1.html",
    "href": "assignments/Assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Consider the following simple model of time allocation. Individual utility is given by:\n\\[ U(C,L) = (\\phi C^{\\rho} + (1-\\phi) L^{\\rho})^{1/\\rho} \\]\nwhere \\(L\\) is an aggregate leisure good composed of \\(K\\) different activities:\n\\[ L = \\prod_{k=1}^{K}l_{k}^{\\delta_{k}},\\qquad \\sum_{k}\\delta_{k} = 1 \\]\nIn addition to these leisure activities, the agent may supply labor to the market at a wage rate of \\(w\\). Letting \\(h_{k}\\) be hours, the time constraint is:\n\\[ h_{k} + \\sum_{k}l_{k} = 1 \\]\nThe model is static and the individual solves the following problem:\n\\[ \\max_{C,\\{l_{k}\\}_{k=1}^{K}} U(C,L) \\]\nsubject to the constraint:\n\\[ C + w \\left(\\sum_{k} l_{k}\\right) \\leq w \\]\n\n\n\nSuppose you are interested in using this model to study the effects of a wage subsidy on labor supply.\nNotice that the model can be written as\n\\[ \\max_{C,h} U(wh,L^*(1-h)) \\]\nwhere\n\\[ L^*(1-h) = \\max_{\\{l_{k}\\}_{k=1}^{K}} \\prod_{k=1}^{K}l_{k}^{\\delta_{k}} \\]\nsubject to \\(\\sum_{k}l_{k} = 1-h\\). Given this simplification, what does Marschak’s Maxim (and common sense) suggest about what parameters need to be estimated here?\n\n\n\nBased on your answer to the above, you simplify the model to the following specification:\n\\[ h^* = \\arg\\max (\\phi (wh)^{\\rho} + (1-\\phi) (1-h)^{\\rho})^{1/\\rho} \\]\nand you derive the following relationship:\n\\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi}{1-\\phi}\\right) + \\frac{1}{1-\\rho}\\log(w) \\]\nwhere \\(C=wh^*\\) is total labor income and \\(L=1-h^*\\) is non-market time.\nSuppose you have a cross-section of data \\((C_{n},L_{n},W_{n})\\) where \\(C_{n}\\) is labor market earnings, \\(L_{n}\\) is non-market time, and \\(W_{n}\\) is the wage-rate for person \\(n\\). This could be taken (for example) from the Outgoing Rotation Group of the CPS monthly survey.\nHow does this model compare to what you are likely to see in the data?\n\n\n\nSuppose now you augment the model to acommodate some randomness in how much individuals work by allowing for heterogeneity in preferences (\\(\\phi\\)):\n\\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi_{n}}{1-\\phi_{n}}\\right) + \\frac{1}{1-\\rho}\\log(w) \\]\nWhat assumption do you need for an OLS regression of \\(\\log(C_{n}/L_{n})\\) on \\(\\log(W_{n})\\) to consistently recover the elasticity of labor supply, \\(1/(1-\\rho)\\)? Do you consider this credible? Why/why not?"
  },
  {
    "objectID": "assignments/Assignment-1.html#setup",
    "href": "assignments/Assignment-1.html#setup",
    "title": "Assignment 1",
    "section": "",
    "text": "Consider the following simple model of time allocation. Individual utility is given by:\n\\[ U(C,L) = (\\phi C^{\\rho} + (1-\\phi) L^{\\rho})^{1/\\rho} \\]\nwhere \\(L\\) is an aggregate leisure good composed of \\(K\\) different activities:\n\\[ L = \\prod_{k=1}^{K}l_{k}^{\\delta_{k}},\\qquad \\sum_{k}\\delta_{k} = 1 \\]\nIn addition to these leisure activities, the agent may supply labor to the market at a wage rate of \\(w\\). Letting \\(h_{k}\\) be hours, the time constraint is:\n\\[ h_{k} + \\sum_{k}l_{k} = 1 \\]\nThe model is static and the individual solves the following problem:\n\\[ \\max_{C,\\{l_{k}\\}_{k=1}^{K}} U(C,L) \\]\nsubject to the constraint:\n\\[ C + w \\left(\\sum_{k} l_{k}\\right) \\leq w \\]"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-1",
    "href": "assignments/Assignment-1.html#question-1",
    "title": "Assignment 1",
    "section": "",
    "text": "Suppose you are interested in using this model to study the effects of a wage subsidy on labor supply.\nNotice that the model can be written as\n\\[ \\max_{C,h} U(wh,L^*(1-h)) \\]\nwhere\n\\[ L^*(1-h) = \\max_{\\{l_{k}\\}_{k=1}^{K}} \\prod_{k=1}^{K}l_{k}^{\\delta_{k}} \\]\nsubject to \\(\\sum_{k}l_{k} = 1-h\\). Given this simplification, what does Marschak’s Maxim (and common sense) suggest about what parameters need to be estimated here?"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-2",
    "href": "assignments/Assignment-1.html#question-2",
    "title": "Assignment 1",
    "section": "",
    "text": "Based on your answer to the above, you simplify the model to the following specification:\n\\[ h^* = \\arg\\max (\\phi (wh)^{\\rho} + (1-\\phi) (1-h)^{\\rho})^{1/\\rho} \\]\nand you derive the following relationship:\n\\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi}{1-\\phi}\\right) + \\frac{1}{1-\\rho}\\log(w) \\]\nwhere \\(C=wh^*\\) is total labor income and \\(L=1-h^*\\) is non-market time.\nSuppose you have a cross-section of data \\((C_{n},L_{n},W_{n})\\) where \\(C_{n}\\) is labor market earnings, \\(L_{n}\\) is non-market time, and \\(W_{n}\\) is the wage-rate for person \\(n\\). This could be taken (for example) from the Outgoing Rotation Group of the CPS monthly survey.\nHow does this model compare to what you are likely to see in the data?"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-3",
    "href": "assignments/Assignment-1.html#question-3",
    "title": "Assignment 1",
    "section": "",
    "text": "Suppose now you augment the model to acommodate some randomness in how much individuals work by allowing for heterogeneity in preferences (\\(\\phi\\)):\n\\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi_{n}}{1-\\phi_{n}}\\right) + \\frac{1}{1-\\rho}\\log(w) \\]\nWhat assumption do you need for an OLS regression of \\(\\log(C_{n}/L_{n})\\) on \\(\\log(W_{n})\\) to consistently recover the elasticity of labor supply, \\(1/(1-\\rho)\\)? Do you consider this credible? Why/why not?"
  },
  {
    "objectID": "assignments/Assignment-2.html",
    "href": "assignments/Assignment-2.html",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "Consider the following extension of the undirected search model. Let \\(X_{n}\\) be a vector of demographics for person \\(n\\):\n\\[ X_{n} = [1,\\ C_{n},\\ F_{n},\\ R_{n}] \\]\nwhere \\(C_{n}\\) is a dummy variable that indicates if an individual has a college degree, \\(F_{n}\\) is a dummy variable indicating that an individual is female, and \\(R_{n}\\) is a dummy that indicates if person \\(n\\) reports their race as not “white”. Define a new set of parameters that depend on these observables:\n\nThe flow value of unemployment is \\(b(X) = X\\beta_{b}\\)\nThe probability of job destruction is \\[ \\delta(X) = \\frac{\\exp(X\\gamma_{\\delta})}{1+\\exp(X\\gamma_{\\delta})} \\]\nThe probability of a job offer is \\[ \\lambda(X) = \\frac{\\exp(X\\gamma_{\\lambda})}{1+\\exp(X\\gamma_{\\lambda})} \\]\n\\(\\beta\\) takes a value of 0.995.\nWage offers are drawn from a log normal distribution with mean \\(\\mu(X) = X\\gamma_{\\mu}\\) and standard deviation \\(\\sigma(X) = \\exp(X\\gamma_{\\sigma})\\)\nLog wages are observed with measurement error: \\[ \\log(W^{o}_{n}) = \\log(W_{n}) + \\zeta_{n} \\] where \\(\\zeta_{n}\\sim\\mathcal{N}(0,\\sigma^2_{\\zeta})\\).\n\nSo the parameters of the model are:\n\\[ \\theta = (\\gamma_{b},\\gamma_{\\delta},\\gamma_{\\lambda},\\gamma_{\\mu},\\gamma_{\\sigma},\\sigma^2_{\\zeta}) \\]\nWe are going to estimate this model on CPS data. Here is code to import the data and impute wages for workers who are not paid by the hour. This code also limits to observations in January so that it is a single cross-section, although you could choose a different month if you wanted. I also convert weekly unemployment durations to monthly.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\ndata = @chain data begin\n    @transform :E = :EMPSTAT.&lt;21\n    @transform @byrow :wage = begin\n        if :PAIDHOUR==0\n            return missing\n        elseif :PAIDHOUR==2\n            if :HOURWAGE&lt;99.99 && :HOURWAGE&gt;0\n                return :HOURWAGE\n            else\n                return missing\n            end\n        elseif :PAIDHOUR==1\n            if :EARNWEEK&gt;0 && :UHRSWORKT.&lt;997\n                return :EARNWEEK / :UHRSWORKT\n            else\n                return missing\n            end\n        end\n    end\n    @subset :MONTH.==1\n    @select :AGE :SEX :RACE :EDUC :wage :E :DURUNEMP\n    @transform :DURUNEMP = round.(:DURUNEMP .* 12/52) #&lt;- we convert weekly unemployment durations to monthly since we have a monthly model\nend\n\n61364×7 DataFrame61339 rows omitted\n\n\n\nRow\nAGE\nSEX\nRACE\nEDUC\nwage\nE\nDURUNEMP\n\n\n\nInt64\nInt64\nInt64\nInt64\nFloat64?\nBool\nFloat64\n\n\n\n\n1\n72\n1\n100\n81\nmissing\ntrue\n231.0\n\n\n2\n66\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n3\n61\n2\n100\n111\nmissing\ntrue\n231.0\n\n\n4\n52\n2\n200\n73\n20.84\ntrue\n231.0\n\n\n5\n19\n2\n200\n73\n10.0\ntrue\n231.0\n\n\n6\n56\n2\n200\n111\n25.0\ntrue\n231.0\n\n\n7\n22\n2\n200\n81\n9.5\ntrue\n231.0\n\n\n8\n23\n2\n100\n124\nmissing\ntrue\n231.0\n\n\n9\n24\n2\n100\n124\nmissing\ntrue\n231.0\n\n\n10\n59\n2\n200\n111\nmissing\ntrue\n231.0\n\n\n11\n53\n1\n200\n81\nmissing\ntrue\n231.0\n\n\n12\n24\n2\n200\n73\nmissing\ntrue\n231.0\n\n\n13\n60\n1\n100\n124\nmissing\ntrue\n231.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n61353\n41\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n61354\n41\n2\n100\n73\nmissing\ntrue\n231.0\n\n\n61355\n38\n1\n100\n73\nmissing\ntrue\n231.0\n\n\n61356\n29\n2\n100\n73\nmissing\ntrue\n231.0\n\n\n61357\n71\n2\n100\n73\n12.0\ntrue\n231.0\n\n\n61358\n45\n1\n100\n92\n21.25\ntrue\n231.0\n\n\n61359\n41\n1\n100\n73\nmissing\ntrue\n231.0\n\n\n61360\n42\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n61361\n43\n2\n100\n123\nmissing\ntrue\n231.0\n\n\n61362\n17\n1\n100\n60\nmissing\ntrue\n231.0\n\n\n61363\n32\n2\n100\n81\nmissing\ntrue\n231.0\n\n\n61364\n30\n2\n100\n81\nmissing\ntrue\n231.0\n\n\n\n\n\n\n\n\n\nFollowing your notes from class, write a function that, given a set of parameters, solves the reservation wage for each unique combination of the variables in \\(X\\) (there are 8 total).\n\n\n\nWrite a function that takes a single observation from the cross-section and calculates the log-likelihood of that observation given the model solution, current parameters, and observables \\(X_{n}\\).\nShow the output from a function call to prove that it works, then use the @time macro to test how long it takes.\n\n\nRelative to your notes in class, you will need to integrate out the measurement error here for wages. Letting \\(\\phi(x;\\mu,\\sigma)\\) be the normal pdf with mean \\(\\mu\\) and standard error \\(\\sigma\\), the likelihood of observing a wage \\(W^{o}\\) will be:\n\\[ f(W^{o}|E,X) = \\int_{w^*}\\frac{\\phi(\\log(w);\\mu(X),\\sigma(X))}{1-\\Phi(\\log(w^*);\\mu(X),\\sigma(X))}\\phi(\\log(W^{o})-w ; \\sigma_{\\zeta})dw \\]\nYou will want to use a package like QuadGK to evaluate this integral numerically.\n\n\n\n\nWrite a function that iterates over every observation in the data and calculates the log-likelihood of the data given parameters.\nShow the output from a function call to prove that it works, then use the @time macro to test how long it takes.\n\n\nYou may find that these functions work faster if you pull the data you need out of DataFrame format and save it as arrays or vectors with known type. For example, I would recommend creating a flag for missing wage data and a default value for those missing wages, and iterating over those objects:\n\nwage_missing = ismissing.(data.wage)\nwage = coalesce.(data.wage,1.)\n# creat a named tuple with all variables to conveniently pass to the log-likelihood:\nd = (;logwage = log.(wage),wage_missing,E = data.E) #&lt;- you will need to add your demographics as well.\n\n(logwage = [0.0, 0.0, 0.0, 3.0368742168851663, 2.302585092994046, 3.2188758248682006, 2.2512917986064953, 0.0, 0.0, 0.0  …  0.0, 0.0, 2.4849066497880004, 3.056356895370426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 0, 0, 0, 0, 1, 1, 1  …  1, 1, 0, 0, 1, 1, 1, 1, 1, 1], E = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n\n\n\n\nThese data are a subsample of the IPUMS CPS data available from cps.ipums.org. Any use of these data should be cited as follows:\nSarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D030.V11.0\nThe CPS data file is intended only for exercises as part of ECON8208. Individuals are not to redistribute the data without permission. Contact ipums@umn.edu for redistribution requests. For all other uses of these data, please access data directly via cps.ipums.org."
  },
  {
    "objectID": "assignments/Assignment-2.html#setup",
    "href": "assignments/Assignment-2.html#setup",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "Consider the following extension of the undirected search model. Let \\(X_{n}\\) be a vector of demographics for person \\(n\\):\n\\[ X_{n} = [1,\\ C_{n},\\ F_{n},\\ R_{n}] \\]\nwhere \\(C_{n}\\) is a dummy variable that indicates if an individual has a college degree, \\(F_{n}\\) is a dummy variable indicating that an individual is female, and \\(R_{n}\\) is a dummy that indicates if person \\(n\\) reports their race as not “white”. Define a new set of parameters that depend on these observables:\n\nThe flow value of unemployment is \\(b(X) = X\\beta_{b}\\)\nThe probability of job destruction is \\[ \\delta(X) = \\frac{\\exp(X\\gamma_{\\delta})}{1+\\exp(X\\gamma_{\\delta})} \\]\nThe probability of a job offer is \\[ \\lambda(X) = \\frac{\\exp(X\\gamma_{\\lambda})}{1+\\exp(X\\gamma_{\\lambda})} \\]\n\\(\\beta\\) takes a value of 0.995.\nWage offers are drawn from a log normal distribution with mean \\(\\mu(X) = X\\gamma_{\\mu}\\) and standard deviation \\(\\sigma(X) = \\exp(X\\gamma_{\\sigma})\\)\nLog wages are observed with measurement error: \\[ \\log(W^{o}_{n}) = \\log(W_{n}) + \\zeta_{n} \\] where \\(\\zeta_{n}\\sim\\mathcal{N}(0,\\sigma^2_{\\zeta})\\).\n\nSo the parameters of the model are:\n\\[ \\theta = (\\gamma_{b},\\gamma_{\\delta},\\gamma_{\\lambda},\\gamma_{\\mu},\\gamma_{\\sigma},\\sigma^2_{\\zeta}) \\]\nWe are going to estimate this model on CPS data. Here is code to import the data and impute wages for workers who are not paid by the hour. This code also limits to observations in January so that it is a single cross-section, although you could choose a different month if you wanted. I also convert weekly unemployment durations to monthly.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\ndata = @chain data begin\n    @transform :E = :EMPSTAT.&lt;21\n    @transform @byrow :wage = begin\n        if :PAIDHOUR==0\n            return missing\n        elseif :PAIDHOUR==2\n            if :HOURWAGE&lt;99.99 && :HOURWAGE&gt;0\n                return :HOURWAGE\n            else\n                return missing\n            end\n        elseif :PAIDHOUR==1\n            if :EARNWEEK&gt;0 && :UHRSWORKT.&lt;997\n                return :EARNWEEK / :UHRSWORKT\n            else\n                return missing\n            end\n        end\n    end\n    @subset :MONTH.==1\n    @select :AGE :SEX :RACE :EDUC :wage :E :DURUNEMP\n    @transform :DURUNEMP = round.(:DURUNEMP .* 12/52) #&lt;- we convert weekly unemployment durations to monthly since we have a monthly model\nend\n\n61364×7 DataFrame61339 rows omitted\n\n\n\nRow\nAGE\nSEX\nRACE\nEDUC\nwage\nE\nDURUNEMP\n\n\n\nInt64\nInt64\nInt64\nInt64\nFloat64?\nBool\nFloat64\n\n\n\n\n1\n72\n1\n100\n81\nmissing\ntrue\n231.0\n\n\n2\n66\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n3\n61\n2\n100\n111\nmissing\ntrue\n231.0\n\n\n4\n52\n2\n200\n73\n20.84\ntrue\n231.0\n\n\n5\n19\n2\n200\n73\n10.0\ntrue\n231.0\n\n\n6\n56\n2\n200\n111\n25.0\ntrue\n231.0\n\n\n7\n22\n2\n200\n81\n9.5\ntrue\n231.0\n\n\n8\n23\n2\n100\n124\nmissing\ntrue\n231.0\n\n\n9\n24\n2\n100\n124\nmissing\ntrue\n231.0\n\n\n10\n59\n2\n200\n111\nmissing\ntrue\n231.0\n\n\n11\n53\n1\n200\n81\nmissing\ntrue\n231.0\n\n\n12\n24\n2\n200\n73\nmissing\ntrue\n231.0\n\n\n13\n60\n1\n100\n124\nmissing\ntrue\n231.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n61353\n41\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n61354\n41\n2\n100\n73\nmissing\ntrue\n231.0\n\n\n61355\n38\n1\n100\n73\nmissing\ntrue\n231.0\n\n\n61356\n29\n2\n100\n73\nmissing\ntrue\n231.0\n\n\n61357\n71\n2\n100\n73\n12.0\ntrue\n231.0\n\n\n61358\n45\n1\n100\n92\n21.25\ntrue\n231.0\n\n\n61359\n41\n1\n100\n73\nmissing\ntrue\n231.0\n\n\n61360\n42\n1\n100\n111\nmissing\ntrue\n231.0\n\n\n61361\n43\n2\n100\n123\nmissing\ntrue\n231.0\n\n\n61362\n17\n1\n100\n60\nmissing\ntrue\n231.0\n\n\n61363\n32\n2\n100\n81\nmissing\ntrue\n231.0\n\n\n61364\n30\n2\n100\n81\nmissing\ntrue\n231.0"
  },
  {
    "objectID": "assignments/Assignment-2.html#part-1",
    "href": "assignments/Assignment-2.html#part-1",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "Following your notes from class, write a function that, given a set of parameters, solves the reservation wage for each unique combination of the variables in \\(X\\) (there are 8 total)."
  },
  {
    "objectID": "assignments/Assignment-2.html#part-2",
    "href": "assignments/Assignment-2.html#part-2",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "Write a function that takes a single observation from the cross-section and calculates the log-likelihood of that observation given the model solution, current parameters, and observables \\(X_{n}\\).\nShow the output from a function call to prove that it works, then use the @time macro to test how long it takes.\n\n\nRelative to your notes in class, you will need to integrate out the measurement error here for wages. Letting \\(\\phi(x;\\mu,\\sigma)\\) be the normal pdf with mean \\(\\mu\\) and standard error \\(\\sigma\\), the likelihood of observing a wage \\(W^{o}\\) will be:\n\\[ f(W^{o}|E,X) = \\int_{w^*}\\frac{\\phi(\\log(w);\\mu(X),\\sigma(X))}{1-\\Phi(\\log(w^*);\\mu(X),\\sigma(X))}\\phi(\\log(W^{o})-w ; \\sigma_{\\zeta})dw \\]\nYou will want to use a package like QuadGK to evaluate this integral numerically."
  },
  {
    "objectID": "assignments/Assignment-2.html#part-3",
    "href": "assignments/Assignment-2.html#part-3",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "Write a function that iterates over every observation in the data and calculates the log-likelihood of the data given parameters.\nShow the output from a function call to prove that it works, then use the @time macro to test how long it takes.\n\n\nYou may find that these functions work faster if you pull the data you need out of DataFrame format and save it as arrays or vectors with known type. For example, I would recommend creating a flag for missing wage data and a default value for those missing wages, and iterating over those objects:\n\nwage_missing = ismissing.(data.wage)\nwage = coalesce.(data.wage,1.)\n# creat a named tuple with all variables to conveniently pass to the log-likelihood:\nd = (;logwage = log.(wage),wage_missing,E = data.E) #&lt;- you will need to add your demographics as well.\n\n(logwage = [0.0, 0.0, 0.0, 3.0368742168851663, 2.302585092994046, 3.2188758248682006, 2.2512917986064953, 0.0, 0.0, 0.0  …  0.0, 0.0, 2.4849066497880004, 3.056356895370426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 0, 0, 0, 0, 1, 1, 1  …  1, 1, 0, 0, 1, 1, 1, 1, 1, 1], E = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
  },
  {
    "objectID": "assignments/Assignment-2.html#a-disclaimer-for-ipums-cps-data",
    "href": "assignments/Assignment-2.html#a-disclaimer-for-ipums-cps-data",
    "title": "Assignment 2: Setup for Estimating a Search Model",
    "section": "",
    "text": "These data are a subsample of the IPUMS CPS data available from cps.ipums.org. Any use of these data should be cited as follows:\nSarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D030.V11.0\nThe CPS data file is intended only for exercises as part of ECON8208. Individuals are not to redistribute the data without permission. Contact ipums@umn.edu for redistribution requests. For all other uses of these data, please access data directly via cps.ipums.org."
  },
  {
    "objectID": "recitations/recitation-3.html",
    "href": "recitations/recitation-3.html",
    "title": "Recitation 3",
    "section": "",
    "text": "In this recitation we’ll do some maximum likelihood exercises using the simple probit model. First let’s write code to simulate data from this model and calculate the likelihood of the data given a set of parameters.\nRecall that the probit model can be written as:\n\\[ Y_{n} = \\mathbf{1}\\{X_{n}\\beta - \\epsilon_{n} \\geq 0 \\},\\ \\epsilon\\sim \\mathcal{N}(0,1) \\]\nSo it will be simple to simulate data from this model. We’ll assume that \\(X_{n} = [1\\ X_{n,1}\\ X_{n,2}]\\) where each \\(X_{n,j}\\) is distributed as a standard normal.\nFor no good reason other than that we want to explore the issues later on, suppose that the parameters \\(\\beta_{1},\\beta_{2},\\beta_{3}\\) are a function of deeper structural parameters \\(\\gamma\\):\n\\[ \\beta_{1} = \\exp(\\gamma_{1}+\\gamma_{2}), \\beta_{2} = \\gamma_{2}, \\beta_{3} = \\gamma_{1} \\]\nBelow is code for simulating the data and calculating the log-likelihood given \\(\\beta\\).\n\nusing Distributions, ForwardDiff, Optim, Random, LinearAlgebra, Plots\n\n\nfunction simulate_probit(β,N)\n    F = Normal()\n    X = [ones(N) rand(F,N,2)]\n    eps = rand(F,N)\n    Y = (X * β .- eps) .&gt; 0\n    return (Y,X)\nend\n\nfunction log_likelihood(n,Y,X,β,Fϵ)\n    @views xb = dot(X[n,:],β)\n    if Y[n]==1\n        return log(cdf(Fϵ,xb))\n    else\n        return log(1-cdf(Fϵ,xb))\n    end\nend\n\nfunction log_likelihood(Y,X,β)\n    ll = 0.\n    N = length(Y)\n    Fϵ = Normal()\n    for n in eachindex(Y)\n        ll += log_likelihood(n,Y,X,β,Fϵ)\n    end\n    return ll / N\nend\n\nlog_likelihood (generic function with 2 methods)\n\n\n\n\nFor simple models you are unlikely to run into this issue, but for more complicated models you need to be careful when pre-allocating arrays in functions that you plan to differentiate “automatically”. Here is an example. Suppose we write the following function to get the reduced form coefficients \\(\\beta\\) as a function of \\(\\gamma\\):\n\nfunction get_β_stupid(γ)\n    β = zeros(3)\n    β[1] = exp(γ[1]+γ[2])\n    β[2] = γ[2]\n    β[3] = γ[1]\n    return β\nend\n\nget_β_stupid (generic function with 1 method)\n\n\nLet’s see what happens when we try to take the derivative of the log-likelihood with respect to \\(\\gamma\\) using this function:\n\nγ = [0.,0.2]\nN = 200\nβ = get_β_stupid(γ)\nY,X = simulate_probit(β,N)\n\nL(γ) = log_likelihood(Y,X,get_β_stupid(γ))\nL(γ)\n\n-0.33713288149822374\n\n\n\ndL = ForwardDiff.gradient(L,γ)\n\n\nMethodError: no method matching Float64(::ForwardDiff.Dual{ForwardDiff.Tag{typeof(L), Float64}, Float64, 2})\nClosest candidates are:\n  (::Type{T})(::Real, ::RoundingMode) where T&lt;:AbstractFloat at rounding.jl:200\n  (::Type{T})(::T) where T&lt;:Number at boot.jl:772\n  (::Type{T})(::AbstractChar) where T&lt;:Union{AbstractChar, Number} at char.jl:50\n  ...\n\n\n\nOk, we get an error! Why? When we pre-allocated the array \\(\\beta\\) in the function get_β_stupid, using the zeros function, this asserts that the vector can only hold Float64 variables. When ForwardDiff attempts to take the derivative, it instead passes arguments that have a special Dual type that cannot be converted to a float. Here are two workarounds for this problem:\n\nfunction get_β_sensible(γ)\n    β = zeros(eltype(γ),3) #&lt;- we let the array take the same type as the argument γ\n    β[1] = exp(γ[1]+γ[2])\n    β[2] = γ[2]\n    β[3] = γ[1]\n    return β\nend\n\nfunction get_β_also_sensible(γ)\n    return [exp(γ[1]+γ[2]) γ[2] γ[1]]\nend\n\nL(γ) = log_likelihood(Y,X,get_β_sensible(γ))\nL2(γ) = log_likelihood(Y,X,get_β_also_sensible(γ))\n[L(γ) L2(γ)]\n\n1×2 Matrix{Float64}:\n -0.337133  -0.337133\n\n\n\n[ForwardDiff.gradient(L,γ) ForwardDiff.gradient(L2,γ)]\n\n2×2 Matrix{Float64}:\n 0.0175352  0.0175352\n 0.024424   0.024424\n\n\nSo now we have something we can pass to the optimizer and there will be no drama when taking derivatives.\n\nN = 2_000\nY,X = simulate_probit(β,N)\nr = optimize(x-&gt;-log_likelihood(Y,X,get_β_sensible(x)),γ,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nγ_est = r.minimizer\nγ_est\n\nIter     Function value   Gradient norm \n     0     3.436787e-01     1.513857e-02\n * time: 5.0067901611328125e-5\n     1     3.435150e-01     1.481243e-04\n * time: 0.0008308887481689453\n     2     3.435149e-01     5.684522e-09\n * time: 0.0011279582977294922\n\n\n2-element Vector{Float64}:\n -0.004991197627244919\n  0.22372487257130555\n\n\n\n\n\nIn class we saw that for maximum likelihood:\n\\[ \\sqrt{N}(\\hat{\\theta}-\\theta_{0}) \\rightarrow_{d} \\mathcal{N}(0,-\\mathbb{E}H(w,\\theta_{0})^{-1}) \\]\nor alternatively:\n\\[ \\sqrt{N}(\\hat{\\theta}-\\theta_{0}) \\rightarrow_{d} \\mathcal{N}(0,\\mathbb{E}[s(w,\\theta_{0})s(w,\\theta_{0})^{T}]^{-1}) \\]\nWe can calculate standard errors by estimating either term. In the first case we just take the hessian of the average log-likelihood at the estimate. In the second we calculate the sample covariance of the score at the estimate. The law of large numbers guarantees that either approach is consistent.\n\n# using the Hessian\nH = ForwardDiff.hessian(x-&gt;log_likelihood(Y,X,get_β_sensible(x)),γ_est)\navar_est = -inv(H) \n\n2×2 Matrix{Float64}:\n  1.64561  -1.02332\n -1.02332   2.00287\n\n\n\n# using the score\n\nS = hcat((ForwardDiff.gradient(x-&gt;log_likelihood(n,Y,X,get_β_sensible(x),Normal()),γ_est) for n in 1:N)...)\navar_est_2 = inv(cov(S'))\n\n2×2 Matrix{Float64}:\n  1.64537   -0.964128\n -0.964128   1.89245\n\n\nNotice that the estimates are close to each other but not identical. They are only exactly equal in the population limit. To get standard errors, we note that \\(\\hat{\\theta}\\) is going to be approximately normally distributed around \\(\\theta\\) with variance equal to the asymptotic variance divided by \\(N\\). So for example the standard errors for \\(\\gamma\\) are:\n\nse = sqrt.(diag(avar_est / N))\n\n2-element Vector{Float64}:\n 0.02868456117694807\n 0.03164541875055554\n\n\n\n\n\nLet’s validate the estimated variances using a monte-carlo simulation:\n\nfunction monte_carlo(num_trials,γ,N)\n    γ_sim = zeros(2,num_trials)\n    β = get_β_sensible(γ)\n    for b in axes(γ_sim,2)\n        Y,X = simulate_probit(β,N)\n        r = optimize(x-&gt;-log_likelihood(Y,X,get_β_sensible(x)),γ,Newton(),autodiff=:forward)\n        γ_sim[:,b] = r.minimizer\n    end\n    return γ_sim\nend\nγ_sim = monte_carlo(200,γ,N)\nvar_sim = cov(γ_sim')\nse_sim = sqrt.(diag(var_sim))\n\n2-element Vector{Float64}:\n 0.02941598344758601\n 0.03372272126529965\n\n\n\n\n\nSome questions for you to ponder.\n\n\nSuppose you estimate the \\(\\beta\\) parameters directly. It would be simple enough to do here. How you could you then back out estimates of \\(\\gamma\\) that would give you the same asymptotic variance as maximum likelihood? We will see how to do this when we study Optimal Minimum Distance\n\n\n\nSuppose we keep the same data-generating process, but that we misspecify the model as: \\[ \\beta = [\\gamma_{1}+\\gamma_{2},\\ \\gamma_{1},\\ \\gamma_{2}] \\] The monte-carlo simulation below shows that the estimates of this misspecified model are still approximately normal. Which assumption is now violated and what formula for the asymptotic variance should we use instead for this pseudo-likelihood?\n\nget_β_misspecified(γ) = [γ[1]+γ[2],γ[2],γ[1]]\n\nfunction monte_carlo_misspecified(num_trials,γ,N)\n    γ_sim = zeros(2,num_trials)\n    β = get_β_sensible(γ)\n    for b in axes(γ_sim,2)\n        Y,X = simulate_probit(β,N)\n        r = optimize(x-&gt;-log_likelihood(Y,X,get_β_misspecified(x)),γ,Newton(),autodiff=:forward)\n        γ_sim[:,b] = r.minimizer\n    end\n    return γ_sim\nend\nγ_sim = monte_carlo_misspecified(200,γ,N)\nhistogram(γ_sim[1,:])"
  },
  {
    "objectID": "recitations/recitation-3.html#a-warning-when-using-automatic-differentiation",
    "href": "recitations/recitation-3.html#a-warning-when-using-automatic-differentiation",
    "title": "Recitation 3",
    "section": "",
    "text": "For simple models you are unlikely to run into this issue, but for more complicated models you need to be careful when pre-allocating arrays in functions that you plan to differentiate “automatically”. Here is an example. Suppose we write the following function to get the reduced form coefficients \\(\\beta\\) as a function of \\(\\gamma\\):\n\nfunction get_β_stupid(γ)\n    β = zeros(3)\n    β[1] = exp(γ[1]+γ[2])\n    β[2] = γ[2]\n    β[3] = γ[1]\n    return β\nend\n\nget_β_stupid (generic function with 1 method)\n\n\nLet’s see what happens when we try to take the derivative of the log-likelihood with respect to \\(\\gamma\\) using this function:\n\nγ = [0.,0.2]\nN = 200\nβ = get_β_stupid(γ)\nY,X = simulate_probit(β,N)\n\nL(γ) = log_likelihood(Y,X,get_β_stupid(γ))\nL(γ)\n\n-0.33713288149822374\n\n\n\ndL = ForwardDiff.gradient(L,γ)\n\n\nMethodError: no method matching Float64(::ForwardDiff.Dual{ForwardDiff.Tag{typeof(L), Float64}, Float64, 2})\nClosest candidates are:\n  (::Type{T})(::Real, ::RoundingMode) where T&lt;:AbstractFloat at rounding.jl:200\n  (::Type{T})(::T) where T&lt;:Number at boot.jl:772\n  (::Type{T})(::AbstractChar) where T&lt;:Union{AbstractChar, Number} at char.jl:50\n  ...\n\n\n\nOk, we get an error! Why? When we pre-allocated the array \\(\\beta\\) in the function get_β_stupid, using the zeros function, this asserts that the vector can only hold Float64 variables. When ForwardDiff attempts to take the derivative, it instead passes arguments that have a special Dual type that cannot be converted to a float. Here are two workarounds for this problem:\n\nfunction get_β_sensible(γ)\n    β = zeros(eltype(γ),3) #&lt;- we let the array take the same type as the argument γ\n    β[1] = exp(γ[1]+γ[2])\n    β[2] = γ[2]\n    β[3] = γ[1]\n    return β\nend\n\nfunction get_β_also_sensible(γ)\n    return [exp(γ[1]+γ[2]) γ[2] γ[1]]\nend\n\nL(γ) = log_likelihood(Y,X,get_β_sensible(γ))\nL2(γ) = log_likelihood(Y,X,get_β_also_sensible(γ))\n[L(γ) L2(γ)]\n\n1×2 Matrix{Float64}:\n -0.337133  -0.337133\n\n\n\n[ForwardDiff.gradient(L,γ) ForwardDiff.gradient(L2,γ)]\n\n2×2 Matrix{Float64}:\n 0.0175352  0.0175352\n 0.024424   0.024424\n\n\nSo now we have something we can pass to the optimizer and there will be no drama when taking derivatives.\n\nN = 2_000\nY,X = simulate_probit(β,N)\nr = optimize(x-&gt;-log_likelihood(Y,X,get_β_sensible(x)),γ,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nγ_est = r.minimizer\nγ_est\n\nIter     Function value   Gradient norm \n     0     3.436787e-01     1.513857e-02\n * time: 5.0067901611328125e-5\n     1     3.435150e-01     1.481243e-04\n * time: 0.0008308887481689453\n     2     3.435149e-01     5.684522e-09\n * time: 0.0011279582977294922\n\n\n2-element Vector{Float64}:\n -0.004991197627244919\n  0.22372487257130555"
  },
  {
    "objectID": "recitations/recitation-3.html#estimating-standard-errors",
    "href": "recitations/recitation-3.html#estimating-standard-errors",
    "title": "Recitation 3",
    "section": "",
    "text": "In class we saw that for maximum likelihood:\n\\[ \\sqrt{N}(\\hat{\\theta}-\\theta_{0}) \\rightarrow_{d} \\mathcal{N}(0,-\\mathbb{E}H(w,\\theta_{0})^{-1}) \\]\nor alternatively:\n\\[ \\sqrt{N}(\\hat{\\theta}-\\theta_{0}) \\rightarrow_{d} \\mathcal{N}(0,\\mathbb{E}[s(w,\\theta_{0})s(w,\\theta_{0})^{T}]^{-1}) \\]\nWe can calculate standard errors by estimating either term. In the first case we just take the hessian of the average log-likelihood at the estimate. In the second we calculate the sample covariance of the score at the estimate. The law of large numbers guarantees that either approach is consistent.\n\n# using the Hessian\nH = ForwardDiff.hessian(x-&gt;log_likelihood(Y,X,get_β_sensible(x)),γ_est)\navar_est = -inv(H) \n\n2×2 Matrix{Float64}:\n  1.64561  -1.02332\n -1.02332   2.00287\n\n\n\n# using the score\n\nS = hcat((ForwardDiff.gradient(x-&gt;log_likelihood(n,Y,X,get_β_sensible(x),Normal()),γ_est) for n in 1:N)...)\navar_est_2 = inv(cov(S'))\n\n2×2 Matrix{Float64}:\n  1.64537   -0.964128\n -0.964128   1.89245\n\n\nNotice that the estimates are close to each other but not identical. They are only exactly equal in the population limit. To get standard errors, we note that \\(\\hat{\\theta}\\) is going to be approximately normally distributed around \\(\\theta\\) with variance equal to the asymptotic variance divided by \\(N\\). So for example the standard errors for \\(\\gamma\\) are:\n\nse = sqrt.(diag(avar_est / N))\n\n2-element Vector{Float64}:\n 0.02868456117694807\n 0.03164541875055554"
  },
  {
    "objectID": "recitations/recitation-3.html#monte-carlo-validation",
    "href": "recitations/recitation-3.html#monte-carlo-validation",
    "title": "Recitation 3",
    "section": "",
    "text": "Let’s validate the estimated variances using a monte-carlo simulation:\n\nfunction monte_carlo(num_trials,γ,N)\n    γ_sim = zeros(2,num_trials)\n    β = get_β_sensible(γ)\n    for b in axes(γ_sim,2)\n        Y,X = simulate_probit(β,N)\n        r = optimize(x-&gt;-log_likelihood(Y,X,get_β_sensible(x)),γ,Newton(),autodiff=:forward)\n        γ_sim[:,b] = r.minimizer\n    end\n    return γ_sim\nend\nγ_sim = monte_carlo(200,γ,N)\nvar_sim = cov(γ_sim')\nse_sim = sqrt.(diag(var_sim))\n\n2-element Vector{Float64}:\n 0.02941598344758601\n 0.03372272126529965"
  },
  {
    "objectID": "recitations/recitation-3.html#additional-exercises",
    "href": "recitations/recitation-3.html#additional-exercises",
    "title": "Recitation 3",
    "section": "",
    "text": "Some questions for you to ponder.\n\n\nSuppose you estimate the \\(\\beta\\) parameters directly. It would be simple enough to do here. How you could you then back out estimates of \\(\\gamma\\) that would give you the same asymptotic variance as maximum likelihood? We will see how to do this when we study Optimal Minimum Distance\n\n\n\nSuppose we keep the same data-generating process, but that we misspecify the model as: \\[ \\beta = [\\gamma_{1}+\\gamma_{2},\\ \\gamma_{1},\\ \\gamma_{2}] \\] The monte-carlo simulation below shows that the estimates of this misspecified model are still approximately normal. Which assumption is now violated and what formula for the asymptotic variance should we use instead for this pseudo-likelihood?\n\nget_β_misspecified(γ) = [γ[1]+γ[2],γ[2],γ[1]]\n\nfunction monte_carlo_misspecified(num_trials,γ,N)\n    γ_sim = zeros(2,num_trials)\n    β = get_β_sensible(γ)\n    for b in axes(γ_sim,2)\n        Y,X = simulate_probit(β,N)\n        r = optimize(x-&gt;-log_likelihood(Y,X,get_β_misspecified(x)),γ,Newton(),autodiff=:forward)\n        γ_sim[:,b] = r.minimizer\n    end\n    return γ_sim\nend\nγ_sim = monte_carlo_misspecified(200,γ,N)\nhistogram(γ_sim[1,:])"
  },
  {
    "objectID": "models/search-model.html",
    "href": "models/search-model.html",
    "title": "Simple Search Model",
    "section": "",
    "text": "In class we discuss the solution and identification of this simple model of undirected search. Time is discrete and indexed by \\(t\\) over an infinite horizon. Workers move between employment and unemployment, have linear utility and cannot save. Let us review the parameters of the model:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n\\(\\lambda\\)\nThe probability an unemployed worker receives a job offer\n\n\n\\(\\delta\\)\nThe probability an employed worker loses their job\n\n\n\\(F_{W}\\)\nThe distribution of wage offers\n\n\n\\(1-\\beta\\)\nThe exponential rate of discounting\n\n\n\\(b\\)\nPer-period utility when unemployed\n\n\n\n\n\nIn class we showed that the optimal decision rule of the worker is characterized by a reservation wage. We derived the reservation wage equation:\n\\[ w^* = b + \\beta\\lambda\\int_{w^*}\\frac{1-F_{W}(w)}{1 - \\beta(1-\\delta)}dw \\]\nand we characterized the steady state rate of unemployment as:\n\\[ P[E = 0] = \\frac{h}{h+\\delta} \\]\nwhere \\(h = \\lambda(1-F_{W}(w^*))\\) is the rate at which workers exit unemployment.\nSimilarly, we showed that the steady state fraction of unemployment durations \\(t\\) is\n\\[ P[t_{U}=t] = h(1-h)^{t} \\]\nLet’s write some code to solve the reservation wage equation, starting with code to evalute the equation below:\n\nusing Distributions, QuadGK\n\n# this function evaluates the reservation wage equation\ndS(x ; F,β,δ) = (1-cdf(F,x)) / (1-β*(1-δ)) #&lt;- this function defines S'(x)\nres_wage(wres ; b,λ,δ,β,F::Distribution) = wres - b - β * λ * quadgk(x-&gt;dS(x;F,β,δ),wres,Inf)[1]\npars = (;b = -5.,λ = 0.45,δ = 0.03,β = 0.99,F = LogNormal(1,1))\nres_wage(1. ; pars...)\n\n-33.70656559385876\n\n\nThe most straightforward way to solve for the reservation wage would be to use a root-finding method here. Since we have used Optim already, let’s just go ahead and use that package:\n\nusing Optim\nfunction solve_res_wage(pars)\n    (;F) = pars\n    w_lb = quantile(F,0.001) #&lt;- get a lower and upper bound for the solution\n    w_ub = quantile(F,0.999)\n    r = optimize(x-&gt;res_wage(x;pars...)^2,w_lb,w_ub)\n    return r.minimizer\nend\nrwage = solve_res_wage(pars)\n\n7.2471320591661295"
  },
  {
    "objectID": "models/search-model.html#model-solution",
    "href": "models/search-model.html#model-solution",
    "title": "Simple Search Model",
    "section": "",
    "text": "In class we showed that the optimal decision rule of the worker is characterized by a reservation wage. We derived the reservation wage equation:\n\\[ w^* = b + \\beta\\lambda\\int_{w^*}\\frac{1-F_{W}(w)}{1 - \\beta(1-\\delta)}dw \\]\nand we characterized the steady state rate of unemployment as:\n\\[ P[E = 0] = \\frac{h}{h+\\delta} \\]\nwhere \\(h = \\lambda(1-F_{W}(w^*))\\) is the rate at which workers exit unemployment.\nSimilarly, we showed that the steady state fraction of unemployment durations \\(t\\) is\n\\[ P[t_{U}=t] = h(1-h)^{t} \\]\nLet’s write some code to solve the reservation wage equation, starting with code to evalute the equation below:\n\nusing Distributions, QuadGK\n\n# this function evaluates the reservation wage equation\ndS(x ; F,β,δ) = (1-cdf(F,x)) / (1-β*(1-δ)) #&lt;- this function defines S'(x)\nres_wage(wres ; b,λ,δ,β,F::Distribution) = wres - b - β * λ * quadgk(x-&gt;dS(x;F,β,δ),wres,Inf)[1]\npars = (;b = -5.,λ = 0.45,δ = 0.03,β = 0.99,F = LogNormal(1,1))\nres_wage(1. ; pars...)\n\n-33.70656559385876\n\n\nThe most straightforward way to solve for the reservation wage would be to use a root-finding method here. Since we have used Optim already, let’s just go ahead and use that package:\n\nusing Optim\nfunction solve_res_wage(pars)\n    (;F) = pars\n    w_lb = quantile(F,0.001) #&lt;- get a lower and upper bound for the solution\n    w_ub = quantile(F,0.999)\n    r = optimize(x-&gt;res_wage(x;pars...)^2,w_lb,w_ub)\n    return r.minimizer\nend\nrwage = solve_res_wage(pars)\n\n7.2471320591661295"
  },
  {
    "objectID": "models/savings.html",
    "href": "models/savings.html",
    "title": "Life-Cycle Savings Model",
    "section": "",
    "text": "Time is discrete and indexed by \\(t\\). Individuals live for a finite number of periods, \\(T\\). They derive utility from consumption according to a CRRA utility function:\n\\[ u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma} \\]\nand from “bequests”, which are modeled here as cash on hand net of consumption in the final period:\n\\[ \\nu(a) = \\psi \\frac{a^{1-\\sigma}}{1-\\sigma} \\].\nConsumption can be transferred between periods via a portfolio of one-period bonds (“savings’, \\(a\\)) that can be purchased at the price \\(1 / (1+r)\\), and there is no borrowing. Inviduals receive income \\(y\\) every period that is governed by a deterministic (\\(\\mu_{t}\\)) and stochastic component:\n\\[ \\log(y_{t}) = \\mu_{t} + \\varepsilon_{it} \\]\nwhere \\(\\varepsilon_{it}\\) is a stationary AR 1 process:\n\\[ \\varepsilon_{it} = \\rho \\varepsilon_{it-1} + \\eta_{it} \\]\nwhere \\(\\eta_{it} \\sim \\mathcal{N}(0,\\sigma^2_{\\eta})\\). The unconditional variance of \\(\\varepsilon_{it}\\) is therefore \\(\\sigma^2_{\\eta} / (1-\\rho^2)\\).\n\n\n\nDefine\n\\[ V_{T}(a,\\varepsilon) = \\max_{c}\\left\\{u(c) + \\nu(y - c)\\right\\} \\]\nAnd now define the remaining value functions recursively:\n\\[ V_{t}(a,\\varepsilon) = \\max_{c,a'}\\left\\{u(c) + \\beta\\mathbb{E}_{\\varepsilon'|\\varepsilon}V(a',\\varepsilon')\\right\\} \\]\nsubject to:\n\\[ c + \\frac{1}{1+r}a' \\leq y + a \\]\nand\n\\[ a' \\geq 0\\]\nWe’re going to write code to solve the model naively using this recursive formulation. You may already be aware that there are more efficient solution methods that exploit the first order conditions of the problem. Not the focus of our class! Please don’t use the example below as a demonstration of best practice when it comes to solving savings models.\nWe’ll start picking some default parameters.\n\npars = (;\n    T = 45, β = 0.95, σ = 2,ρ = 0.9,ση = 0.1, μ = fill(2.,45), ψ = 5., r = 0.05\n)\n\n(T = 45, β = 0.95, σ = 2, ρ = 0.9, ση = 0.1, μ = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0  …  2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], ψ = 5.0, r = 0.05)\n\n\nNext we’ll write a function that uses Tauchen’s method to approximate the income process as a discrete markov process.\n\nusing Distributions,Random\nusing LinearAlgebra\nΦ(x) = cdf(Normal(),x)\n\nfunction tauchen(ρ,ση,Kϵ)\n    sd = ση/sqrt(1-ρ^2)\n    grid = range(-3sd,stop=3sd,length=Kϵ)\n    Π = zeros(Kϵ,Kϵ)\n    Δ = grid[2]-grid[1]\n    for j=1:Kϵ\n        Π[1,j] = Φ((grid[1] + Δ/2 - ρ*grid[j])/ση)\n        Π[end,j] = 1 - Φ((grid[end] - Δ/2 - ρ*grid[j])/ση)\n        for k=2:(Kϵ-1)\n            Π[k,j] = Φ((grid[k] + Δ/2 - ρ*grid[j])/ση) - Φ((grid[k] - Δ/2 - ρ*grid[j])/ση)\n        end\n    end\n    return Π,grid\nend\n\ntauchen (generic function with 1 method)\n\n\nNow, let’s think about how to solve this model. We have two state variables to track. We have discretized \\(\\varepsilon\\), now let’s discretize assets and define a max operator.\n\nKa = 100\nKϵ = 5\nagrid = LinRange(0,pars.μ[1] * pars.T,Ka) #&lt;- is this a reasonable upper bound? We'll find out!\nΠ,ϵgrid = tauchen(pars.ρ,pars.ση,Kϵ)\npars = (;pars...,Ka,agrid,Π,ϵgrid,Kϵ)\n\nu(c,σ) = c^(1-σ) / (1-σ)\n\nfunction solve_max(V,t,iϵ,ia,pars)\n    (;agrid,ϵgrid,Π,σ,Ka,r,β) = pars\n    cash = exp(pars.μ[t] + ϵgrid[iϵ]) + agrid[ia]\n    amax = 0\n    vmax = -Inf\n    loop = true\n    a = 1\n    while loop && a&lt;Ka\n        c = cash - agrid[a] / (1+r)\n        if c&gt;0\n            #@views v = u(c,σ) + β * dot(Π[:,iϵ],V[:,a,t+1])\n            v = u(c,σ)\n            for iϵ′ in axes(V,1)\n                v += β * Π[iϵ′,iϵ] * V[iϵ′,a,t+1]\n            end\n            if v&gt;vmax\n                vmax = v\n                amax = a\n            end\n        else\n            loop = false\n        end\n        a += 1 #&lt;- move one up the grid space\n    end\n    return amax,vmax\nend\n\nsolve_max (generic function with 1 method)\n\n\nNext, a function that uses this max operator to get the value function for all states in a period, \\(t\\), and records the optimal savings policy.\n\nfunction iterate!(V,A,t,pars)\n    for ia in axes(V,2), iϵ in axes(V,1)\n        A[iϵ,ia,t],V[iϵ,ia,t] = solve_max(V,t,iϵ,ia,pars)\n    end\nend\nfunction terminal_values!(V,pars)\n    (;σ,ψ,agrid) = pars\n    for ia in axes(V,2), iϵ in axes(V,1)\n        V[iϵ,ia] = ψ * u(agrid[ia],σ)\n    end\nend\n\nterminal_values! (generic function with 2 methods)\n\n\n\nfunction backward_induction!(V,A,pars)\n    (;ψ,σ,T,agrid) = pars\n    # set the values at T+1 (bequest motives)\n    @views terminal_values!(V[:,:,T+1],pars)\n    for t in reverse(1:T)\n        iterate!(V,A,t,pars)\n    end\nend\n\nbackward_induction! (generic function with 1 method)\n\n\nLet’s check the model solution and time it also.\n\nV = zeros(pars.Kϵ,pars.Ka,pars.T+1)\nA = zeros(Int64,pars.Kϵ,pars.Ka,pars.T)\nbackward_induction!(V,A,pars)\n@time backward_induction!(V,A,pars)\n\n  0.010476 seconds\n\n\nSeems ok. We can plot the policy functions as a sanity check. The plot below shows savings policy at the median wage shock over time at different levels of assets.\n\nusing Plots\n\nplot(1:pars.T,agrid[A[3,1:10:Ka,:]'],legend=false)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can see that the discreteness creates some jumpiness in the policy functions. As I said, other solution methods that use interpolation can be more efficient and will create smoother pictures, but since that is not the focus of this class we will use this simple solution method."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignments/Assignment-3.html#part-3",
    "href": "assignments/Assignment-3.html#part-3",
    "title": "Assignment 3: Estimating a Search Model",
    "section": "",
    "text": "Back out the implied maximum likelihood estimates of \\(\\hat{\\lambda}\\) and \\(\\hat{b}\\) as a function of the estimated parameters from part (1)."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-4",
    "href": "assignments/Assignment-3.html#part-4",
    "title": "Assignment 3: Estimating a Search Model",
    "section": "",
    "text": "Provide an estimate of the asymptotic variance of \\((\\hat{h},\\hat{\\delta},\\hat{\\mu},\\hat{\\sigma},\\hat{w^*})\\) using the standard MLE formula."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-5",
    "href": "assignments/Assignment-3.html#part-5",
    "title": "Assignment 3: Estimating a Search Model",
    "section": "",
    "text": "Recall that the delta method implies that if \\(\\hat{\\theta}\\) is asymptotically normal with asymptotic variance \\(V\\) then the vector-valued function \\(F(\\hat{\\theta})\\) is also asymptotically normal with:\n\\[ \\sqrt{N}(F(\\hat{\\theta}) - F(\\theta)) \\rightarrow_{d} \\mathcal{N}(0,\\nabla_{\\theta'}FV\\nabla_{\\theta}F') \\]\nUse this fact to estimate the asymptotic variance of \\((\\hat{h},\\hat{\\delta},\\hat{\\mu},\\hat{\\sigma},\\hat{w^*},\\hat{\\lambda},\\hat{b})\\)."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-6",
    "href": "assignments/Assignment-3.html#part-6",
    "title": "Assignment 3: Estimating a Search Model",
    "section": "",
    "text": "Now report all of your estimates and standard errors for this group. Repeat this exercise for each group.\nIf we thought that the parametric relationships using \\(\\gamma\\) from Homework 2 described the true values of the parameters for each group, how might we use these group-specific estimates to derive estimates of each \\(\\gamma\\)?"
  },
  {
    "objectID": "recitations/recitation-4.html",
    "href": "recitations/recitation-4.html",
    "title": "Some Hints and Tricks for Assignment 3",
    "section": "",
    "text": "Depending on how much you played around last week, you may have found that methods like QuadGK don’t play nicely with automatic differentiation. This is because QuadGK adjusts the number of nodes adaptively.\nOne solution is to use a fixed number of nodes and weights with FastGaussQuadrature. Here is a simple example.\n\nusing FastGaussQuadrature, QuadGK, ForwardDiff, Distributions, Optim, Roots\n\n# a function to integrate using quadrature (default: 10 nodes)\nfunction integrateGL(f,a,b ; num_nodes = 10)\n    nodes, weights = gausslegendre( num_nodes )\n    ∫f = 0.\n    for k in eachindex(nodes)\n        x = (a+b)/2 + (b - a)/2 * nodes[k]\n        ∫f += weights[k] * f(x)\n    end\n    return (b - a)/2 * ∫f\nend\n\ndS(x ; F,β,δ) = (1-cdf(F,x)) / (1-β*(1-δ))\nres_wage_1(wres , b,λ,δ,β,F) = wres - b - β * λ * integrateGL(x-&gt;dS(x;F,β,δ),wres,quantile(F,0.9999))\nres_wage_2(wres , b,λ,δ,β,F) = wres - b - β * λ * quadgk(x-&gt;dS(x;F,β,δ),wres,Inf)[1]\n\nForwardDiff.derivative(x-&gt;res_wage_1(x,0.,0.5,0.03,0.99,LogNormal(0,1)),1.) \n#ForwardDiff.derivative(x-&gt;res_wage_2(x,0.,0.5,0.03,0.99,LogNormal(0,1)),1.) &lt;- you can try running this and you will see an error\n\n7.224647846541125\n\n\n\n\n\nBased on this, we’re going to re-write the model solution using this new integration routine. We will also use Roots to solve for the reservation wage in a way that will also play nicely with ForwardDiff.\n\nres_wage(wres , b,λ,δ,β,F::Distribution) = wres - b - β * λ * integrateGL(x-&gt;dS(x;F,β,δ),wres,quantile(F,0.9999))\npars = (;b = -5.,λ = 0.45,δ = 0.03,β = 0.99,F = LogNormal(1,1))\n\n\nfunction solve_res_wage(b,λ,δ,β,F)\n    return find_zero(x-&gt;res_wage(x,b,λ,δ,β,F),eltype(b)(4.)) #&lt;- initial guess of $4/hour\nend\nsolve_res_wage(0.,0.4,0.03,0.995,LogNormal())\n# testing that we can take a derivative here:\nForwardDiff.derivative(x-&gt;solve_res_wage(x,0.4,0.04,0.995,LogNormal()),0.)\n\n\n0.45978046027514413\n\n\n\n\n\nNone of this is really different from before, but we added a function that pulls a named tuple out for a specific demographic group. You might find that useful for your assignment.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\ndata = @chain data begin\n    @transform :E = :EMPSTAT.&lt;21\n    @transform @byrow :wage = begin\n        if :PAIDHOUR==0\n            return missing\n        elseif :PAIDHOUR==2\n            if :HOURWAGE&lt;99.99 && :HOURWAGE&gt;0\n                return :HOURWAGE\n            else\n                return missing\n            end\n        elseif :PAIDHOUR==1\n            if :EARNWEEK&gt;0 && :UHRSWORKT&lt;997 && :UHRSWORKT&gt;0\n                return :EARNWEEK / :UHRSWORKT\n            else\n                return missing\n            end\n        end\n    end\n    @subset :MONTH.==1\n    @select :AGE :SEX :RACE :EDUC :wage :E :DURUNEMP\n    @transform begin\n        :bachelors = :EDUC.&gt;=111\n        :nonwhite = :RACE.!=100 \n        :female = :SEX.==2\n        :DURUNEMP = round.(:DURUNEMP .* 12/52)\n    end\nend\n\n# the whole dataset in a named tuple\nwage_missing = ismissing.(data.wage)\nwage = coalesce.(data.wage,1.)\nN = length(data.AGE)\nX = [ones(N) data.bachelors data.female data.nonwhite]\n# create a named tuple with all variables to conveniently pass to the log-likelihood:\nd = (;logwage = log.(wage),wage_missing,E = data.E,tU = data.DURUNEMP, X) #&lt;- you will need to add your demographics as well.\n\nfunction get_data(data,C,F,R)\n    data = @subset data :bachelors.==C :female.==F :nonwhite.==R\n    wage_missing = ismissing.(data.wage)\n    wage = coalesce.(data.wage,1.)\n    N = length(data.AGE)\n    # create a named tuple with all variables to conveniently pass to the log-likelihood:\n    return d = (;logwage = log.(wage),wage_missing,E = data.E,tU = data.DURUNEMP) \nend\n\ndx = get_data(data,1,0,0) #&lt;- data for white men with a college degree\n\n(logwage = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.516124491189194, 3.872798692268385, 0.0  …  2.7762279256323206, 0.0, 0.0, 0.0, 2.976307324928243, 0.0, 0.0, 3.410759848526933, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 1, 1, 1, 1, 0, 0, 1  …  0, 1, 1, 1, 0, 1, 1, 0, 1, 1], E = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1], tU = [231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0  …  231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0])\n\n\n\n\n\nIn your assignment you will estimate the model for one set of demographic characteristics by directly estimating \\((h,\\delta,\\mu,\\sigma,w^*)\\) and backing out \\(b\\) and \\(\\lambda\\). Here is a log-likelihood function that you could use (feel free to write your own).\n\nϕ(x,μ,σ) = pdf(Normal(μ,σ),x)\nΦ(x,μ,σ) = cdf(Normal(μ,σ),x)\n# a function for the log-likelihood of observed wages (integrating out measurement error)\nfunction logwage_likelihood(logwage,F,σζ,wres)\n    f(x) = pdf(F,x) / (1-cdf(F,wres)) * ϕ(logwage,log(x),σζ)\n    ub = quantile(F,0.9999)\n    return integrateGL(f,wres,ub)\nend\n# a function to get the log-likelihood of a single observation\nfunction log_likelihood(n,data,pars)\n    (;h,δ,wres,F,σζ) = pars\n    ll = 0.\n    if data.E[n]\n        ll += log(h) - log(h + δ)\n        if !data.wage_missing[n]\n            ll += logwage_likelihood(data.logwage[n],F,σζ,wres)\n        end\n    else\n        ll += log(δ) - log(h + δ)\n        ll += log(h) + data.tU[n] * log(1-h)\n    end\n    return ll\nend\n\n# a function to map the vector x into parameters\nlogit(x) = exp(x) / (1+exp(x))\nlogit_inv(x) = log(x/(1-x))\nfunction update(pars,x)\n    h = logit(x[1])\n    δ = logit(x[2])\n    μ = x[3]\n    σ = exp(x[4])\n    wres = exp(x[5])\n    F = LogNormal(μ,σ)\n    σζ = 0.05\n    return (;pars...,h,δ,μ,σ,wres,F,σζ)\nend\n# a function to iterate over all observations\nfunction log_likelihood_obj(x,pars,data)\n    pars = update(pars,x)\n    ll = 0.\n    for n in eachindex(data.E)\n        ll += log_likelihood(n,data,pars)\n    end\n    return ll / length(data.E)\nend\n\n\nlog_likelihood_obj (generic function with 1 method)\n\n\nWith this log-likelihood we can pass straight to Optim.\n\nx0 = [logit_inv(0.5),logit_inv(0.03),2.,log(1.),log(5.)]\npars = (;σζ = 0.05, β = 0.995)\nlog_likelihood_obj(x0,pars,dx) #&lt;- test.\nres = optimize(x-&gt;-log_likelihood_obj(x,pars,dx),x0,Newton(),Optim.Options(show_trace=true))\n\nIter     Function value   Gradient norm \n     0     1.566021e-01     9.112564e-02\n * time: 8.916854858398438e-5\n     1     1.124900e-01     8.425665e-02\n * time: 0.18092608451843262\n     2     9.438133e-02     7.424673e-02\n * time: 0.3399372100830078\n     3     7.262438e-02     9.064264e-02\n * time: 0.49828410148620605\n     4     6.527359e-02     4.043801e-01\n * time: 0.6780951023101807\n     5     5.390735e-02     7.264660e-02\n * time: 0.8790051937103271\n     6     4.456252e-02     5.104587e-02\n * time: 1.2180511951446533\n     7     4.419638e-02     4.246399e-02\n * time: 1.492077112197876\n     8     4.331776e-02     9.864366e-02\n * time: 1.6455280780792236\n     9     4.279575e-02     8.358208e-02\n * time: 1.8211140632629395\n    10     4.266464e-02     3.147197e-02\n * time: 1.9958012104034424\n    11     4.263086e-02     1.838941e-02\n * time: 2.1707379817962646\n    12     4.262640e-02     3.127715e-03\n * time: 2.3470070362091064\n    13     4.262633e-02     3.959605e-07\n * time: 2.499476194381714\n    14     4.262633e-02     2.184184e-08\n * time: 2.674283981323242\n    15     4.262633e-02     7.462975e-10\n * time: 2.744335174560547\n\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     4.262633e-02\n\n * Found with\n    Algorithm:     Newton's Method\n\n * Convergence measures\n    |x - x'|               = 6.68e-08 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.20e-08 ≰ 0.0e+00\n    |f(x) - f(x')|         = 8.81e-15 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 2.07e-13 ≰ 0.0e+00\n    |g(x)|                 = 7.46e-10 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   3  (vs limit Inf)\n    Iterations:    15\n    f(x) calls:    54\n    ∇f(x) calls:   54\n    ∇²f(x) calls:  15\n\n\n\npars = update(pars,res.minimizer)\n\n(σζ = 0.05, h = 0.17641764242681596, δ = 0.003828372201989135, μ = 2.416002041470427, σ = 1.147127370295339, wres = 18.6280911963576, F = LogNormal{Float64}(μ=2.416002041470427, σ=1.147127370295339))\n\n\nWe could back out estimates of \\(b\\) and \\(\\lambda\\) as follows:\n\nλ = pars.h / (1 - cdf(pars.F,pars.wres))\nb = pars.wres - pars.β * λ * integrateGL(x-&gt;dS(x;pars.F,pars.β,pars.δ),pars.wres,quantile(pars.F,0.9999))\n\n-601.9015559474371\n\n\nWhat about standard errors? Some parameters (such as \\(\\delta\\)) are transformations of the estimated vector. Here we need the delta method, which tells us that if \\(\\mathbb{V}(\\hat{\\theta}) = v\\) and \\(\\beta = f(\\theta)\\) then \\(\\mathbb{V}(\\hat{\\beta}) = f'(\\theta)^2v\\):\n\nH = ForwardDiff.hessian(x-&gt;log_likelihood_obj(x,pars,dx),res.minimizer)\nN = length(dx.E)\navar = inv(-H) #&lt;- asymptotic variance estimate\nstd_err_δ = pars.δ * sqrt(avar[2,2] / N)\n\n0.00037289231946497606\n\n\nOne final note. You could automate this whole calculation by writing a function that returns all your transformed parameters of interest as a function of the estimated vector, then calling ForwardDiff:\n\nusing LinearAlgebra\nfunction final_ests(x_est,pars)\n    pars = update(pars,x_est)\n    (;h,δ,μ,σ,wres,F,β) = pars\n    λ = h / (1 - cdf(F,wres))\n    b = wres - β * λ * integrateGL(x-&gt;dS(x;F,β,δ),wres,quantile(F,0.9999))\n    return [h , δ, μ, σ, wres, λ, b]\nend\ndF = ForwardDiff.jacobian(x-&gt;final_ests(x,pars),res.minimizer)\nvar_est = dF * avar * dF' / N #&lt;- this is the variance of the estimates implied by the delta method\n# now a table with estimates and standard errors:\np_str = [\"h\",\"δ\",\"μ\",\"σ\",\"wres\",\"λ\",\"b\"]\nDataFrame(par = p_str,est = final_ests(res.minimizer,pars),se = sqrt.(diag(var_est)))\n\n7×3 DataFrame\n\n\n\nRow\npar\nest\nse\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nh\n0.176418\n0.0114358\n\n\n2\nδ\n0.00382837\n0.000371465\n\n\n3\nμ\n2.416\n0.167726\n\n\n4\nσ\n1.14713\n0.044975\n\n\n5\nwres\n18.6281\n0.178748\n\n\n6\nλ\n0.536668\n0.0841872\n\n\n7\nb\n-601.902\n31.4198\n\n\n\n\n\n\nBeautiful!"
  },
  {
    "objectID": "recitations/recitation-4.html#adapting-code-for-automatic-differentiation",
    "href": "recitations/recitation-4.html#adapting-code-for-automatic-differentiation",
    "title": "Some Hints and Tricks for Assignment 3",
    "section": "",
    "text": "Depending on how much you played around last week, you may have found that methods like QuadGK don’t play nicely with automatic differentiation. This is because QuadGK adjusts the number of nodes adaptively.\nOne solution is to use a fixed number of nodes and weights with FastGaussQuadrature. Here is a simple example.\n\nusing FastGaussQuadrature, QuadGK, ForwardDiff, Distributions, Optim, Roots\n\n# a function to integrate using quadrature (default: 10 nodes)\nfunction integrateGL(f,a,b ; num_nodes = 10)\n    nodes, weights = gausslegendre( num_nodes )\n    ∫f = 0.\n    for k in eachindex(nodes)\n        x = (a+b)/2 + (b - a)/2 * nodes[k]\n        ∫f += weights[k] * f(x)\n    end\n    return (b - a)/2 * ∫f\nend\n\ndS(x ; F,β,δ) = (1-cdf(F,x)) / (1-β*(1-δ))\nres_wage_1(wres , b,λ,δ,β,F) = wres - b - β * λ * integrateGL(x-&gt;dS(x;F,β,δ),wres,quantile(F,0.9999))\nres_wage_2(wres , b,λ,δ,β,F) = wres - b - β * λ * quadgk(x-&gt;dS(x;F,β,δ),wres,Inf)[1]\n\nForwardDiff.derivative(x-&gt;res_wage_1(x,0.,0.5,0.03,0.99,LogNormal(0,1)),1.) \n#ForwardDiff.derivative(x-&gt;res_wage_2(x,0.,0.5,0.03,0.99,LogNormal(0,1)),1.) &lt;- you can try running this and you will see an error\n\n7.224647846541125"
  },
  {
    "objectID": "recitations/recitation-4.html#re-writing-the-model-solution",
    "href": "recitations/recitation-4.html#re-writing-the-model-solution",
    "title": "Some Hints and Tricks for Assignment 3",
    "section": "",
    "text": "Based on this, we’re going to re-write the model solution using this new integration routine. We will also use Roots to solve for the reservation wage in a way that will also play nicely with ForwardDiff.\n\nres_wage(wres , b,λ,δ,β,F::Distribution) = wres - b - β * λ * integrateGL(x-&gt;dS(x;F,β,δ),wres,quantile(F,0.9999))\npars = (;b = -5.,λ = 0.45,δ = 0.03,β = 0.99,F = LogNormal(1,1))\n\n\nfunction solve_res_wage(b,λ,δ,β,F)\n    return find_zero(x-&gt;res_wage(x,b,λ,δ,β,F),eltype(b)(4.)) #&lt;- initial guess of $4/hour\nend\nsolve_res_wage(0.,0.4,0.03,0.995,LogNormal())\n# testing that we can take a derivative here:\nForwardDiff.derivative(x-&gt;solve_res_wage(x,0.4,0.04,0.995,LogNormal()),0.)\n\n\n0.45978046027514413"
  },
  {
    "objectID": "recitations/recitation-4.html#cleaning-the-data",
    "href": "recitations/recitation-4.html#cleaning-the-data",
    "title": "Some Hints and Tricks for Assignment 3",
    "section": "",
    "text": "None of this is really different from before, but we added a function that pulls a named tuple out for a specific demographic group. You might find that useful for your assignment.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\ndata = @chain data begin\n    @transform :E = :EMPSTAT.&lt;21\n    @transform @byrow :wage = begin\n        if :PAIDHOUR==0\n            return missing\n        elseif :PAIDHOUR==2\n            if :HOURWAGE&lt;99.99 && :HOURWAGE&gt;0\n                return :HOURWAGE\n            else\n                return missing\n            end\n        elseif :PAIDHOUR==1\n            if :EARNWEEK&gt;0 && :UHRSWORKT&lt;997 && :UHRSWORKT&gt;0\n                return :EARNWEEK / :UHRSWORKT\n            else\n                return missing\n            end\n        end\n    end\n    @subset :MONTH.==1\n    @select :AGE :SEX :RACE :EDUC :wage :E :DURUNEMP\n    @transform begin\n        :bachelors = :EDUC.&gt;=111\n        :nonwhite = :RACE.!=100 \n        :female = :SEX.==2\n        :DURUNEMP = round.(:DURUNEMP .* 12/52)\n    end\nend\n\n# the whole dataset in a named tuple\nwage_missing = ismissing.(data.wage)\nwage = coalesce.(data.wage,1.)\nN = length(data.AGE)\nX = [ones(N) data.bachelors data.female data.nonwhite]\n# create a named tuple with all variables to conveniently pass to the log-likelihood:\nd = (;logwage = log.(wage),wage_missing,E = data.E,tU = data.DURUNEMP, X) #&lt;- you will need to add your demographics as well.\n\nfunction get_data(data,C,F,R)\n    data = @subset data :bachelors.==C :female.==F :nonwhite.==R\n    wage_missing = ismissing.(data.wage)\n    wage = coalesce.(data.wage,1.)\n    N = length(data.AGE)\n    # create a named tuple with all variables to conveniently pass to the log-likelihood:\n    return d = (;logwage = log.(wage),wage_missing,E = data.E,tU = data.DURUNEMP) \nend\n\ndx = get_data(data,1,0,0) #&lt;- data for white men with a college degree\n\n(logwage = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.516124491189194, 3.872798692268385, 0.0  …  2.7762279256323206, 0.0, 0.0, 0.0, 2.976307324928243, 0.0, 0.0, 3.410759848526933, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 1, 1, 1, 1, 0, 0, 1  …  0, 1, 1, 1, 0, 1, 1, 0, 1, 1], E = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1], tU = [231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0  …  231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0])"
  },
  {
    "objectID": "recitations/recitation-4.html#coding-the-log-likelihood-with-one-set-of-parameters",
    "href": "recitations/recitation-4.html#coding-the-log-likelihood-with-one-set-of-parameters",
    "title": "Some Hints and Tricks for Assignment 3",
    "section": "",
    "text": "In your assignment you will estimate the model for one set of demographic characteristics by directly estimating \\((h,\\delta,\\mu,\\sigma,w^*)\\) and backing out \\(b\\) and \\(\\lambda\\). Here is a log-likelihood function that you could use (feel free to write your own).\n\nϕ(x,μ,σ) = pdf(Normal(μ,σ),x)\nΦ(x,μ,σ) = cdf(Normal(μ,σ),x)\n# a function for the log-likelihood of observed wages (integrating out measurement error)\nfunction logwage_likelihood(logwage,F,σζ,wres)\n    f(x) = pdf(F,x) / (1-cdf(F,wres)) * ϕ(logwage,log(x),σζ)\n    ub = quantile(F,0.9999)\n    return integrateGL(f,wres,ub)\nend\n# a function to get the log-likelihood of a single observation\nfunction log_likelihood(n,data,pars)\n    (;h,δ,wres,F,σζ) = pars\n    ll = 0.\n    if data.E[n]\n        ll += log(h) - log(h + δ)\n        if !data.wage_missing[n]\n            ll += logwage_likelihood(data.logwage[n],F,σζ,wres)\n        end\n    else\n        ll += log(δ) - log(h + δ)\n        ll += log(h) + data.tU[n] * log(1-h)\n    end\n    return ll\nend\n\n# a function to map the vector x into parameters\nlogit(x) = exp(x) / (1+exp(x))\nlogit_inv(x) = log(x/(1-x))\nfunction update(pars,x)\n    h = logit(x[1])\n    δ = logit(x[2])\n    μ = x[3]\n    σ = exp(x[4])\n    wres = exp(x[5])\n    F = LogNormal(μ,σ)\n    σζ = 0.05\n    return (;pars...,h,δ,μ,σ,wres,F,σζ)\nend\n# a function to iterate over all observations\nfunction log_likelihood_obj(x,pars,data)\n    pars = update(pars,x)\n    ll = 0.\n    for n in eachindex(data.E)\n        ll += log_likelihood(n,data,pars)\n    end\n    return ll / length(data.E)\nend\n\n\nlog_likelihood_obj (generic function with 1 method)\n\n\nWith this log-likelihood we can pass straight to Optim.\n\nx0 = [logit_inv(0.5),logit_inv(0.03),2.,log(1.),log(5.)]\npars = (;σζ = 0.05, β = 0.995)\nlog_likelihood_obj(x0,pars,dx) #&lt;- test.\nres = optimize(x-&gt;-log_likelihood_obj(x,pars,dx),x0,Newton(),Optim.Options(show_trace=true))\n\nIter     Function value   Gradient norm \n     0     1.566021e-01     9.112564e-02\n * time: 8.916854858398438e-5\n     1     1.124900e-01     8.425665e-02\n * time: 0.18092608451843262\n     2     9.438133e-02     7.424673e-02\n * time: 0.3399372100830078\n     3     7.262438e-02     9.064264e-02\n * time: 0.49828410148620605\n     4     6.527359e-02     4.043801e-01\n * time: 0.6780951023101807\n     5     5.390735e-02     7.264660e-02\n * time: 0.8790051937103271\n     6     4.456252e-02     5.104587e-02\n * time: 1.2180511951446533\n     7     4.419638e-02     4.246399e-02\n * time: 1.492077112197876\n     8     4.331776e-02     9.864366e-02\n * time: 1.6455280780792236\n     9     4.279575e-02     8.358208e-02\n * time: 1.8211140632629395\n    10     4.266464e-02     3.147197e-02\n * time: 1.9958012104034424\n    11     4.263086e-02     1.838941e-02\n * time: 2.1707379817962646\n    12     4.262640e-02     3.127715e-03\n * time: 2.3470070362091064\n    13     4.262633e-02     3.959605e-07\n * time: 2.499476194381714\n    14     4.262633e-02     2.184184e-08\n * time: 2.674283981323242\n    15     4.262633e-02     7.462975e-10\n * time: 2.744335174560547\n\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     4.262633e-02\n\n * Found with\n    Algorithm:     Newton's Method\n\n * Convergence measures\n    |x - x'|               = 6.68e-08 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.20e-08 ≰ 0.0e+00\n    |f(x) - f(x')|         = 8.81e-15 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 2.07e-13 ≰ 0.0e+00\n    |g(x)|                 = 7.46e-10 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   3  (vs limit Inf)\n    Iterations:    15\n    f(x) calls:    54\n    ∇f(x) calls:   54\n    ∇²f(x) calls:  15\n\n\n\npars = update(pars,res.minimizer)\n\n(σζ = 0.05, h = 0.17641764242681596, δ = 0.003828372201989135, μ = 2.416002041470427, σ = 1.147127370295339, wres = 18.6280911963576, F = LogNormal{Float64}(μ=2.416002041470427, σ=1.147127370295339))\n\n\nWe could back out estimates of \\(b\\) and \\(\\lambda\\) as follows:\n\nλ = pars.h / (1 - cdf(pars.F,pars.wres))\nb = pars.wres - pars.β * λ * integrateGL(x-&gt;dS(x;pars.F,pars.β,pars.δ),pars.wres,quantile(pars.F,0.9999))\n\n-601.9015559474371\n\n\nWhat about standard errors? Some parameters (such as \\(\\delta\\)) are transformations of the estimated vector. Here we need the delta method, which tells us that if \\(\\mathbb{V}(\\hat{\\theta}) = v\\) and \\(\\beta = f(\\theta)\\) then \\(\\mathbb{V}(\\hat{\\beta}) = f'(\\theta)^2v\\):\n\nH = ForwardDiff.hessian(x-&gt;log_likelihood_obj(x,pars,dx),res.minimizer)\nN = length(dx.E)\navar = inv(-H) #&lt;- asymptotic variance estimate\nstd_err_δ = pars.δ * sqrt(avar[2,2] / N)\n\n0.00037289231946497606\n\n\nOne final note. You could automate this whole calculation by writing a function that returns all your transformed parameters of interest as a function of the estimated vector, then calling ForwardDiff:\n\nusing LinearAlgebra\nfunction final_ests(x_est,pars)\n    pars = update(pars,x_est)\n    (;h,δ,μ,σ,wres,F,β) = pars\n    λ = h / (1 - cdf(F,wres))\n    b = wres - β * λ * integrateGL(x-&gt;dS(x;F,β,δ),wres,quantile(F,0.9999))\n    return [h , δ, μ, σ, wres, λ, b]\nend\ndF = ForwardDiff.jacobian(x-&gt;final_ests(x,pars),res.minimizer)\nvar_est = dF * avar * dF' / N #&lt;- this is the variance of the estimates implied by the delta method\n# now a table with estimates and standard errors:\np_str = [\"h\",\"δ\",\"μ\",\"σ\",\"wres\",\"λ\",\"b\"]\nDataFrame(par = p_str,est = final_ests(res.minimizer,pars),se = sqrt.(diag(var_est)))\n\n7×3 DataFrame\n\n\n\nRow\npar\nest\nse\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nh\n0.176418\n0.0114358\n\n\n2\nδ\n0.00382837\n0.000371465\n\n\n3\nμ\n2.416\n0.167726\n\n\n4\nσ\n1.14713\n0.044975\n\n\n5\nwres\n18.6281\n0.178748\n\n\n6\nλ\n0.536668\n0.0841872\n\n\n7\nb\n-601.902\n31.4198\n\n\n\n\n\n\nBeautiful!"
  },
  {
    "objectID": "assignments/Assignment-4.html",
    "href": "assignments/Assignment-4.html",
    "title": "Assignment 4: Income and Savings from the PSID",
    "section": "",
    "text": "In this assignment we’ll start working with data from the PSID. If you would like more details on how these data are constructed, you should refer to Arellano, Blundell, and Bonhomme (2018).\nTo begin, let’s load the data and pull out the variables we are interested in using. These are person identifiers (person), year, total income (y), savings (tot_assets1) and age. You should bear in mind that it is by no means trivial to measure total income and total assets in these data. The variables we are looking at are the product of a lot of data cleaning and careful choices by the authors.\nusing CSV, DataFrames, DataFramesMeta, Statistics\ndata = @chain begin \n    CSV.read(\"../data/abb_aea_data.csv\",DataFrame,missingstring = \"NA\")\n    @select :person :y :tot_assets1 :asset :age :year\nend\n\n19317×6 DataFrame19292 rows omitted\n\n\n\nRow\nperson\ny\ntot_assets1\nasset\nage\nyear\n\n\n\nInt64\nInt64\nInt64\nFloat64\nInt64\nInt64\n\n\n\n\n1\n12061\n173100\n605000\n15500.0\n65\n98\n\n\n2\n17118\n54000\n60000\n0.0\n49\n98\n\n\n3\n12630\n61283\n224000\n39283.0\n59\n98\n\n\n4\n12647\n42300\n28240\n0.0\n38\n98\n\n\n5\n5239\n82275\n7500\n0.0\n56\n98\n\n\n6\n2671\n69501\n48000\n3600.0\n35\n98\n\n\n7\n13027\n68000\n148000\n20000.0\n49\n98\n\n\n8\n6791\n93758\n80000\n160.0\n41\n98\n\n\n9\n6475\n26581\n23300\n0.0\n35\n98\n\n\n10\n18332\n33785\n0\n0.0\n42\n98\n\n\n11\n3856\n55300\n311000\n5300.0\n33\n98\n\n\n12\n19326\n40200\n105250\n0.0\n40\n98\n\n\n13\n21818\n42500\n13000\n0.0\n36\n98\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n19306\n6617\n115887\n241000\n21346.0\n62\n108\n\n\n19307\n626\n128600\n98000\n0.0\n46\n108\n\n\n19308\n4795\n105000\n-68000\n0.0\n34\n108\n\n\n19309\n3223\n120000\n132000\n0.0\n47\n108\n\n\n19310\n8098\n26527\n4700\n0.0\n37\n108\n\n\n19311\n8954\n144026\n220000\n25.0\n46\n108\n\n\n19312\n12990\n122665\n220000\n0.0\n53\n108\n\n\n19313\n8782\n55000\n69000\n0.0\n31\n108\n\n\n19314\n13059\n42728\n-10000\n0.0\n26\n108\n\n\n19315\n13535\n57000\n0\n0.0\n26\n108\n\n\n19316\n3806\n87000\n74200\n0.0\n26\n108\n\n\n19317\n11085\n74000\n-50000\n0.0\n31\n108\nYou are going to estimate the parameters of the income process in the simple savings model by matching the implied variances and covariances from the model to those that are calculated from the data.\nRecall that the income process is:\n\\[ \\log(y_{it}) = \\mu_{t} + \\varepsilon_{it} \\]\nwhere \\(\\varepsilon_{it}\\) is an AR1 process with autocorrelation \\(\\rho\\) and variance \\(\\sigma^2_{\\eta} / (1-\\rho^2)\\). Thus, there are only two parameters dictating the income process: (\\(\\rho,\\sigma_\\eta\\))."
  },
  {
    "objectID": "assignments/Assignment-4.html#setup",
    "href": "assignments/Assignment-4.html#setup",
    "title": "Assignment 4: Income and Savings from the PSID",
    "section": "Setup",
    "text": "Setup\nTo map to the model, assume that agents begin (\\(t=1\\)) when aged 25 and live for 40 years (so the “terminal” period is at age 64). Thus, we should filter the data to look at only these ages.\n\n@subset!(data,:age.&gt;=25,:age.&lt;=64)\n\n19139×6 DataFrame19114 rows omitted\n\n\n\nRow\nperson\ny\ntot_assets1\nasset\nage\nyear\n\n\n\nInt64\nInt64\nInt64\nFloat64\nInt64\nInt64\n\n\n\n\n1\n17118\n54000\n60000\n0.0\n49\n98\n\n\n2\n12630\n61283\n224000\n39283.0\n59\n98\n\n\n3\n12647\n42300\n28240\n0.0\n38\n98\n\n\n4\n5239\n82275\n7500\n0.0\n56\n98\n\n\n5\n2671\n69501\n48000\n3600.0\n35\n98\n\n\n6\n13027\n68000\n148000\n20000.0\n49\n98\n\n\n7\n6791\n93758\n80000\n160.0\n41\n98\n\n\n8\n6475\n26581\n23300\n0.0\n35\n98\n\n\n9\n18332\n33785\n0\n0.0\n42\n98\n\n\n10\n3856\n55300\n311000\n5300.0\n33\n98\n\n\n11\n19326\n40200\n105250\n0.0\n40\n98\n\n\n12\n21818\n42500\n13000\n0.0\n36\n98\n\n\n13\n7300\n121508\n178000\n10008.0\n59\n98\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n19128\n6617\n115887\n241000\n21346.0\n62\n108\n\n\n19129\n626\n128600\n98000\n0.0\n46\n108\n\n\n19130\n4795\n105000\n-68000\n0.0\n34\n108\n\n\n19131\n3223\n120000\n132000\n0.0\n47\n108\n\n\n19132\n8098\n26527\n4700\n0.0\n37\n108\n\n\n19133\n8954\n144026\n220000\n25.0\n46\n108\n\n\n19134\n12990\n122665\n220000\n0.0\n53\n108\n\n\n19135\n8782\n55000\n69000\n0.0\n31\n108\n\n\n19136\n13059\n42728\n-10000\n0.0\n26\n108\n\n\n19137\n13535\n57000\n0\n0.0\n26\n108\n\n\n19138\n3806\n87000\n74200\n0.0\n26\n108\n\n\n19139\n11085\n74000\n-50000\n0.0\n31\n108"
  },
  {
    "objectID": "assignments/Assignment-4.html#part-1",
    "href": "assignments/Assignment-4.html#part-1",
    "title": "Assignment 4: Income and Savings from the PSID",
    "section": "Part 1",
    "text": "Part 1\nEstimate the parameters \\(\\mu\\) using the sample mean of log income at each age. Create residuals \\(\\hat{\\varepsilon}_{it}\\) for each individual in each period using these estimates."
  },
  {
    "objectID": "assignments/Assignment-4.html#part-2",
    "href": "assignments/Assignment-4.html#part-2",
    "title": "Assignment 4: Income and Savings from the PSID",
    "section": "Part 2",
    "text": "Part 2\nThe PSID data are taken biennially (every two years). Thus, write a function that takes a guess of \\((\\rho,\\sigma_\\eta)\\) and calculates:\n\nThe unconditional variance of the residual.\nThe covariance of the residual with its two year lag.\nThe covariance of the residual with its four year lag."
  },
  {
    "objectID": "assignments/Assignment-4.html#part-3",
    "href": "assignments/Assignment-4.html#part-3",
    "title": "Assignment 4: Income and Savings from the PSID",
    "section": "Part 3",
    "text": "Part 3\nCalculate the sample equivalent of these moments from the data, and write a function that calculates the sum of squared differences between the data and those predicted by a particular choice of \\((\\rho,\\sigma_\\eta)\\).\nIf it helps, here is code to create the lags for income (you could adapt this code to create lags for the residuals you calculated in part 1).\n\nd1 = @chain data begin\n    @select :year :person :y\n    @transform :year = :year .+ 2\n    @rename :ylag1 = :y\nend\n\nd2 = @chain data begin\n    @select :year :person :y\n    @transform :year = :year .+ 4\n    @rename :ylag2 = :y\nend\n\ndata = @chain data begin\n    innerjoin(d1 , on=[:person,:year])\n    innerjoin(d2 , on=[:person,:year])\nend\n\n9785×8 DataFrame9760 rows omitted\n\n\n\nRow\nperson\ny\ntot_assets1\nasset\nage\nyear\nylag1\nylag2\n\n\n\nInt64\nInt64\nInt64\nFloat64\nInt64\nInt64\nInt64\nInt64\n\n\n\n\n1\n17118\n43799\n-2000\n0.0\n53\n102\n51700\n54000\n\n\n2\n12630\n68554\n1519000\n29454.0\n63\n102\n104104\n61283\n\n\n3\n12647\n35000\n78000\n0.0\n42\n102\n30500\n42300\n\n\n4\n5239\n49948\n29900\n0.0\n60\n102\n54332\n82275\n\n\n5\n2671\n77000\n84000\n0.0\n39\n102\n75000\n69501\n\n\n6\n13027\n91000\n248000\n25000.0\n53\n102\n50678\n68000\n\n\n7\n6791\n122296\n118650\n154.0\n45\n102\n100503\n93758\n\n\n8\n18332\n54000\n56000\n0.0\n46\n102\n40200\n33785\n\n\n9\n3856\n95800\n357000\n9600.0\n37\n102\n76400\n55300\n\n\n10\n21818\n72334\n25540\n0.0\n40\n102\n58700\n42500\n\n\n11\n7300\n64319\n710000\n6130.0\n63\n102\n111140\n121508\n\n\n12\n20796\n88000\n75000\n0.0\n48\n102\n112600\n105000\n\n\n13\n8455\n50880\n110500\n130.0\n53\n102\n46000\n54000\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n9774\n3360\n198300\n775000\n3300.0\n40\n108\n161000\n161000\n\n\n9775\n1204\n44710\n5200\n0.0\n30\n108\n46360\n35000\n\n\n9776\n6483\n55019\n-3000\n0.0\n38\n108\n28000\n22720\n\n\n9777\n2182\n53908\n9000\n0.0\n32\n108\n51247\n57625\n\n\n9778\n3971\n48406\n108000\n15905.0\n43\n108\n109000\n55400\n\n\n9779\n12094\n70500\n83000\n0.0\n41\n108\n55830\n10375\n\n\n9780\n12975\n104500\n266800\n2500.0\n40\n108\n77240\n98200\n\n\n9781\n9940\n133074\n282000\n274.0\n45\n108\n102900\n100300\n\n\n9782\n8048\n185500\n88000\n50000.0\n45\n108\n183000\n75200\n\n\n9783\n2921\n84845\n390800\n773.0\n59\n108\n76507\n77235\n\n\n9784\n13562\n39200\n18000\n0.0\n43\n108\n35000\n47500\n\n\n9785\n3193\n66000\n37400\n0.0\n44\n108\n51000\n61210\n\n\n\n\n\n\nAn example of calculating covariances:\n\n@chain data begin\n    @combine begin \n        :c1 = cov(log.(:y),log.(:ylag1)) \n        :c2 = cov(log.(:y),log.(:ylag2))\n    end\nend\n\n1×2 DataFrame\n\n\n\nRow\nc1\nc2\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n0.402346\n0.363079"
  },
  {
    "objectID": "assignments/Assignment-4.html#part-4",
    "href": "assignments/Assignment-4.html#part-4",
    "title": "Assignment 4: Income and Savings from the PSID",
    "section": "Part 4",
    "text": "Part 4\nNow estimate the income process parameters by minimizing this weighted sum of squares (i.e. implement a minimum distance estimator with identity weighting matrix)."
  },
  {
    "objectID": "assignments/Assignment-4.html#part-5",
    "href": "assignments/Assignment-4.html#part-5",
    "title": "Assignment 4: Income and Savings from the PSID",
    "section": "Part 5",
    "text": "Part 5\nNote that in this model:\n\\[ \\rho = \\frac{\\mathbb{C}(\\varepsilon_{it},\\varepsilon_{it-1})}{\\mathbb{V}(\\varepsilon_{it})} \\].\nSuppose that the true model is:\n\\[ \\log(y_{it}) = \\mu_{t} + \\varepsilon_{it} + \\zeta_{it} \\]\nwhere \\(\\zeta_{it}\\) is an additional shock to income that is completely iid (i.e. no persistence). Suppose we estimate the persistence parameter \\(\\rho\\) using the relationship above (which is now misspecified).\n\\[ \\hat{\\rho} = \\frac{\\widehat{\\mathbb{C}(\\hat{\\varepsilon}_{it},\\hat{\\varepsilon}_{it-1})}}{\\widehat{\\mathbb{V}(\\hat{\\varepsilon}_{it})}} \\]\nDoes the population limit of our estimator over- or under-estimate \\(\\rho\\), the persistence in \\(\\varepsilon\\)?"
  },
  {
    "objectID": "recitations/recitation-5.html",
    "href": "recitations/recitation-5.html",
    "title": "Recitation 5: Simulating from the life-cycle model",
    "section": "",
    "text": "Review the details of the simple life-cycle savings model. Today we’re only going to be thinking about the income process, but for the sake of completeness I include below all of the code to parameterize and solve that model.\n\nusing Distributions,Random\nusing LinearAlgebra\nΦ(x) = cdf(Normal(),x)\n\nfunction tauchen(ρ,ση,Kϵ)\n    sd = ση/sqrt(1-ρ^2)\n    grid = range(-3sd,stop=3sd,length=Kϵ)\n    Π = zeros(Kϵ,Kϵ)\n    Δ = grid[2]-grid[1]\n    for j=1:Kϵ\n        Π[1,j] = Φ((grid[1] + Δ/2 - ρ*grid[j])/ση)\n        Π[end,j] = 1 - Φ((grid[end] - Δ/2 - ρ*grid[j])/ση)\n        for k=2:(Kϵ-1)\n            Π[k,j] = Φ((grid[k] + Δ/2 - ρ*grid[j])/ση) - Φ((grid[k] - Δ/2 - ρ*grid[j])/ση)\n        end\n    end\n    return Π,grid\nend\n\nu(c,σ) = c^(1-σ) / (1-σ)\nfunction solve_max(V,t,iϵ,ia,pars)\n    (;agrid,ϵgrid,Π,σ,Ka,r,β) = pars\n    cash = exp(pars.μ[t] + ϵgrid[iϵ]) + agrid[ia]\n    amax = 0\n    vmax = -Inf\n    loop = true\n    a = 1\n    while loop && a&lt;Ka\n        c = cash - agrid[a] / (1+r)\n        if c&gt;0\n            #@views v = u(c,σ) + β * dot(Π[:,iϵ],V[:,a,t+1])\n            v = u(c,σ)\n            for iϵ′ in axes(V,1)\n                v += β * Π[iϵ′,iϵ] * V[iϵ′,a,t+1]\n            end\n            if v&gt;vmax\n                vmax = v\n                amax = a\n            end\n        else\n            loop = false\n        end\n        a += 1 #&lt;- move one up the grid space\n    end\n    return amax,vmax\nend\n\nfunction iterate!(V,A,t,pars)\n    for ia in axes(V,2), iϵ in axes(V,1)\n        A[iϵ,ia,t],V[iϵ,ia,t] = solve_max(V,t,iϵ,ia,pars)\n    end\nend\nfunction terminal_values!(V,pars)\n    (;σ,ψ,agrid) = pars\n    for ia in axes(V,2), iϵ in axes(V,1)\n        V[iϵ,ia] = ψ * u(agrid[ia],σ)\n    end\nend\n\nfunction backward_induction!(V,A,pars)\n    (;ψ,σ,T,agrid) = pars\n    # set the values at T+1 (bequest motives)\n    @views terminal_values!(V[:,:,T+1],pars)\n    for t in reverse(1:T)\n        iterate!(V,A,t,pars)\n    end\nend\n\npars = (;\n    T = 45, β = 0.95, σ = 2,ρ = 0.9,ση = 0.1, μ = fill(2.,45), ψ = 5., r = 0.05\n)\nKa = 100\nKϵ = 5\nagrid = LinRange(0,pars.μ[1] * pars.T,Ka) #\nΠ,ϵgrid = tauchen(pars.ρ,pars.ση,Kϵ)\npars = (;pars...,Ka,agrid,Π,ϵgrid,Kϵ)\nV = zeros(pars.Kϵ,pars.Ka,pars.T+1)\nA = zeros(Int64,pars.Kϵ,pars.Ka,pars.T)\nbackward_induction!(V,A,pars)\n@time backward_induction!(V,A,pars)\n\n  0.011214 seconds"
  },
  {
    "objectID": "recitations/recitation-5.html#part-1",
    "href": "recitations/recitation-5.html#part-1",
    "title": "Recitation 5: Simulating from the life-cycle model",
    "section": "",
    "text": "Review the details of the simple life-cycle savings model. Today we’re only going to be thinking about the income process, but for the sake of completeness I include below all of the code to parameterize and solve that model.\n\nusing Distributions,Random\nusing LinearAlgebra\nΦ(x) = cdf(Normal(),x)\n\nfunction tauchen(ρ,ση,Kϵ)\n    sd = ση/sqrt(1-ρ^2)\n    grid = range(-3sd,stop=3sd,length=Kϵ)\n    Π = zeros(Kϵ,Kϵ)\n    Δ = grid[2]-grid[1]\n    for j=1:Kϵ\n        Π[1,j] = Φ((grid[1] + Δ/2 - ρ*grid[j])/ση)\n        Π[end,j] = 1 - Φ((grid[end] - Δ/2 - ρ*grid[j])/ση)\n        for k=2:(Kϵ-1)\n            Π[k,j] = Φ((grid[k] + Δ/2 - ρ*grid[j])/ση) - Φ((grid[k] - Δ/2 - ρ*grid[j])/ση)\n        end\n    end\n    return Π,grid\nend\n\nu(c,σ) = c^(1-σ) / (1-σ)\nfunction solve_max(V,t,iϵ,ia,pars)\n    (;agrid,ϵgrid,Π,σ,Ka,r,β) = pars\n    cash = exp(pars.μ[t] + ϵgrid[iϵ]) + agrid[ia]\n    amax = 0\n    vmax = -Inf\n    loop = true\n    a = 1\n    while loop && a&lt;Ka\n        c = cash - agrid[a] / (1+r)\n        if c&gt;0\n            #@views v = u(c,σ) + β * dot(Π[:,iϵ],V[:,a,t+1])\n            v = u(c,σ)\n            for iϵ′ in axes(V,1)\n                v += β * Π[iϵ′,iϵ] * V[iϵ′,a,t+1]\n            end\n            if v&gt;vmax\n                vmax = v\n                amax = a\n            end\n        else\n            loop = false\n        end\n        a += 1 #&lt;- move one up the grid space\n    end\n    return amax,vmax\nend\n\nfunction iterate!(V,A,t,pars)\n    for ia in axes(V,2), iϵ in axes(V,1)\n        A[iϵ,ia,t],V[iϵ,ia,t] = solve_max(V,t,iϵ,ia,pars)\n    end\nend\nfunction terminal_values!(V,pars)\n    (;σ,ψ,agrid) = pars\n    for ia in axes(V,2), iϵ in axes(V,1)\n        V[iϵ,ia] = ψ * u(agrid[ia],σ)\n    end\nend\n\nfunction backward_induction!(V,A,pars)\n    (;ψ,σ,T,agrid) = pars\n    # set the values at T+1 (bequest motives)\n    @views terminal_values!(V[:,:,T+1],pars)\n    for t in reverse(1:T)\n        iterate!(V,A,t,pars)\n    end\nend\n\npars = (;\n    T = 45, β = 0.95, σ = 2,ρ = 0.9,ση = 0.1, μ = fill(2.,45), ψ = 5., r = 0.05\n)\nKa = 100\nKϵ = 5\nagrid = LinRange(0,pars.μ[1] * pars.T,Ka) #\nΠ,ϵgrid = tauchen(pars.ρ,pars.ση,Kϵ)\npars = (;pars...,Ka,agrid,Π,ϵgrid,Kϵ)\nV = zeros(pars.Kϵ,pars.Ka,pars.T+1)\nA = zeros(Int64,pars.Kϵ,pars.Ka,pars.T)\nbackward_induction!(V,A,pars)\n@time backward_induction!(V,A,pars)\n\n  0.011214 seconds"
  },
  {
    "objectID": "recitations/recitation-5.html#part-2-the-stationary-distribution-of-varepsilon",
    "href": "recitations/recitation-5.html#part-2-the-stationary-distribution-of-varepsilon",
    "title": "Recitation 5: Simulating from the life-cycle model",
    "section": "Part 2: The Stationary Distribution of \\(\\varepsilon\\)",
    "text": "Part 2: The Stationary Distribution of \\(\\varepsilon\\)\nSuppose that \\(\\pi^*\\) is the stationary distribution of \\(\\varepsilon\\). It must obey:\n\\[ \\pi^* = \\Pi \\pi^*  \\]\nwhere the \\(k\\)th column of \\(\\Pi\\) gives the transition probabilities given that the current value is \\(\\varepsilon_{k}\\). This is equivalent to \\(\\pi^*\\) being the eigenvector of \\(\\mathbf{I} - \\Pi\\) that corresponds to an eigenvalue of 0.\n\nfunction stat_dist(Π)\n    Kϵ = size(Π,1)\n    vals, vec = eigen(I(Kϵ) .- Π)\n    return vec[:,1] ./ sum(vec[:,1])\nend\n\nπ_stat = stat_dist(Π)\npars = (;pars...,π_stat)\n\n(T = 45, β = 0.95, σ = 2, ρ = 0.9, ση = 0.1, μ = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0  …  2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], ψ = 5.0, r = 0.05, Ka = 100, agrid = LinRange{Float64}(0.0, 90.0, 100), Π = [0.8490507777857362 0.019473727871012682 … 7.346962855655732e-17 3.459030953951908e-30; 0.15094537665867613 0.8961919626850798 … 7.260018586910001e-7 1.237828285826989e-15; … ; 1.2212453270876722e-15 7.260018586308092e-7 … 0.8961919626850798 0.15094537665867613; 0.0 1.1102230246251565e-16 … 0.019473727871012647 0.8490507777857362], ϵgrid = -0.6882472016116855:0.34412360080584276:0.6882472016116855, Kϵ = 5, π_stat = [0.03046350803405268, 0.23613279404893608, 0.46680739583402275, 0.23613279404893592, 0.03046350803405253])\n\n\nWe can make sure we’re doing this correctly by checking:\n\nπ_stat .- Π * π_stat\n\n5-element Vector{Float64}:\n  0.0\n  2.7755575615628914e-17\n  0.0\n  0.0\n -3.469446951953614e-18"
  },
  {
    "objectID": "recitations/recitation-5.html#part-3-simulating-the-income-process",
    "href": "recitations/recitation-5.html#part-3-simulating-the-income-process",
    "title": "Recitation 5: Simulating from the life-cycle model",
    "section": "Part 3: Simulating the Income Process",
    "text": "Part 3: Simulating the Income Process\nIn principle, one can simulate draws from any distribution by simulating draws from the uniform distribution using rand() and using the inverse cdf. To see this, note that for any random variable:\n\\[ F_{X}(X) \\sim \\text{unif}[0,1] \\]\nand hence if \\(U\\sim\\text{unif}[0,1]\\):\n\\[ F_{X}^{-1}(U) \\sim X \\]\nHere we will make use of the Distributions package and the Categorical distribution for simulation.\n\n# this function simulates on path\nfunction simulate_shocks!(ε,π_stat,Π)\n    ε_current = rand(π_stat)\n    for t in eachindex(ε)\n        ε[t] = ε_current\n        ε_current = rand(Π[ε_current])\n    end\nend\n\nfunction simulate_shocks!(ε,pars)\n    (;Π,π_stat) = pars\n    π_stat_dist = Categorical(π_stat)\n    Π_dist = [Categorical(Π[:,k]) for k in axes(Π,2)]\n    for n in axes(ε,2)\n        @views simulate_shocks!(ε[:,n],π_stat_dist,Π_dist)\n    end\nend\n\nnsim = 500\nε_sim = zeros(Int64,pars.T,nsim)\nsimulate_shocks!(ε_sim,pars)"
  },
  {
    "objectID": "recitations/recitation-5.html#part-4-holding-simulation-error-fixed",
    "href": "recitations/recitation-5.html#part-4-holding-simulation-error-fixed",
    "title": "Recitation 5: Simulating from the life-cycle model",
    "section": "Part 4: Holding Simulation Error Fixed",
    "text": "Part 4: Holding Simulation Error Fixed\nNow suppose we plan to use the simulated draws of \\(\\varepsilon\\) to calculate some moments to match to the data. For example, we could try to match the lagged covariance to lagged covariance in the data. In practice we won’t have to simulation to do this, but for the sake of argument, let’s pretend we do. We could write the function to calculate moments as:\n\nfunction moments_ε(ε,pars)\n    simulate_shocks!(ε,pars)\n    ε_value = pars.ϵgrid[ε]\n    return [var(ε_value[1,:]) ; cov(ε_value[2,:],ε_value[1,:]) ; cov(ε_value[3,:],ε_value[1,:])]\nend\n\nmoments_ε (generic function with 1 method)\n\n\nBut notice what happens when we call this function multiple times:\n\n@show moments_ε(ε_sim,pars);\n@show moments_ε(ε_sim,pars);\n@show moments_ε(ε_sim,pars);\n\nmoments_ε(ε_sim, pars) = [0.080886562598882, 0.07397874696761952, 0.06613875118658376]\nmoments_ε(ε_sim, pars) = [0.08498739584432013, 0.07629638223816074, 0.07038339837569878]\nmoments_ε(ε_sim, pars) = [0.08589157261892205, 0.08071616918046617, 0.0733280244699928]\n\n\nThe moments keep jumping around! This is going to be a problem if we want to match the simulated moments to ones we have calculated in the data. The simulation error is moving the goalposts around in ways that we can’t predict. The solution is to fix simulation error by fixing the seed of our pseudorandom number generator. To see how, let’s rewrite the function:\n\nfunction moments_ε(ε,pars ; seed = 202404)\n    Random.seed!(seed)\n    simulate_shocks!(ε,pars)\n    ε_value = pars.ϵgrid[ε]\n    return [var(ε_value[1,:]) ; cov(ε_value[2,:],ε_value[1,:]) ; cov(ε_value[3,:],ε_value[1,:])]\nend\n\n@show moments_ε(ε_sim,pars);\n@show moments_ε(ε_sim,pars);\n@show moments_ε(ε_sim,pars);\n\n@show moments_ε(ε_sim,pars ; seed = 12345);\n@show moments_ε(ε_sim,pars ; seed = 12345);\n@show moments_ε(ε_sim,pars ; seed = 12345);\n\nmoments_ε(ε_sim, pars) = [0.07921632739162526, 0.0694721020989348, 0.06356291530429277]\nmoments_ε(ε_sim, pars) = [0.07921632739162526, 0.0694721020989348, 0.06356291530429277]\nmoments_ε(ε_sim, pars) = [0.07921632739162526, 0.0694721020989348, 0.06356291530429277]\nmoments_ε(ε_sim, pars; seed = 12345) = [0.08676489821748767, 0.08202520831135969, 0.07516960236262014]\nmoments_ε(ε_sim, pars; seed = 12345) = [0.08676489821748767, 0.08202520831135969, 0.07516960236262014]\nmoments_ε(ε_sim, pars; seed = 12345) = [0.08676489821748767, 0.08202520831135969, 0.07516960236262014]\n\n\nThis is something that is very important to keep in mind when you are using the simulated method of moments to estimate models."
  },
  {
    "objectID": "models/savings.html#environment",
    "href": "models/savings.html#environment",
    "title": "Life-Cycle Savings Model",
    "section": "",
    "text": "Time is discrete and indexed by \\(t\\). Individuals live for a finite number of periods, \\(T\\). They derive utility from consumption according to a CRRA utility function:\n\\[ u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma} \\]\nand from “bequests”, which are modeled here as cash on hand net of consumption in the final period:\n\\[ \\nu(a) = \\psi \\frac{a^{1-\\sigma}}{1-\\sigma} \\].\nConsumption can be transferred between periods via a portfolio of one-period bonds (“savings’, \\(a\\)) that can be purchased at the price \\(1 / (1+r)\\), and there is no borrowing. Inviduals receive income \\(y\\) every period that is governed by a deterministic (\\(\\mu_{t}\\)) and stochastic component:\n\\[ \\log(y_{t}) = \\mu_{t} + \\varepsilon_{it} \\]\nwhere \\(\\varepsilon_{it}\\) is a stationary AR 1 process:\n\\[ \\varepsilon_{it} = \\rho \\varepsilon_{it-1} + \\eta_{it} \\]\nwhere \\(\\eta_{it} \\sim \\mathcal{N}(0,\\sigma^2_{\\eta})\\). The unconditional variance of \\(\\varepsilon_{it}\\) is therefore \\(\\sigma^2_{\\eta} / (1-\\rho^2)\\)."
  },
  {
    "objectID": "models/savings.html#model-solution",
    "href": "models/savings.html#model-solution",
    "title": "Life-Cycle Savings Model",
    "section": "",
    "text": "Define\n\\[ V_{T}(a,\\varepsilon) = \\max_{c}\\left\\{u(c) + \\nu(y - c)\\right\\} \\]\nAnd now define the remaining value functions recursively:\n\\[ V_{t}(a,\\varepsilon) = \\max_{c,a'}\\left\\{u(c) + \\beta\\mathbb{E}_{\\varepsilon'|\\varepsilon}V(a',\\varepsilon')\\right\\} \\]\nsubject to:\n\\[ c + \\frac{1}{1+r}a' \\leq y + a \\]\nand\n\\[ a' \\geq 0\\]\nWe’re going to write code to solve the model naively using this recursive formulation. You may already be aware that there are more efficient solution methods that exploit the first order conditions of the problem. Not the focus of our class! Please don’t use the example below as a demonstration of best practice when it comes to solving savings models.\nWe’ll start picking some default parameters.\n\npars = (;\n    T = 45, β = 0.95, σ = 2,ρ = 0.9,ση = 0.1, μ = fill(2.,45), ψ = 5., r = 0.05\n)\n\n(T = 45, β = 0.95, σ = 2, ρ = 0.9, ση = 0.1, μ = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0  …  2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], ψ = 5.0, r = 0.05)\n\n\nNext we’ll write a function that uses Tauchen’s method to approximate the income process as a discrete markov process.\n\nusing Distributions,Random\nusing LinearAlgebra\nΦ(x) = cdf(Normal(),x)\n\nfunction tauchen(ρ,ση,Kϵ)\n    sd = ση/sqrt(1-ρ^2)\n    grid = range(-3sd,stop=3sd,length=Kϵ)\n    Π = zeros(Kϵ,Kϵ)\n    Δ = grid[2]-grid[1]\n    for j=1:Kϵ\n        Π[1,j] = Φ((grid[1] + Δ/2 - ρ*grid[j])/ση)\n        Π[end,j] = 1 - Φ((grid[end] - Δ/2 - ρ*grid[j])/ση)\n        for k=2:(Kϵ-1)\n            Π[k,j] = Φ((grid[k] + Δ/2 - ρ*grid[j])/ση) - Φ((grid[k] - Δ/2 - ρ*grid[j])/ση)\n        end\n    end\n    return Π,grid\nend\n\ntauchen (generic function with 1 method)\n\n\nNow, let’s think about how to solve this model. We have two state variables to track. We have discretized \\(\\varepsilon\\), now let’s discretize assets and define a max operator.\n\nKa = 100\nKϵ = 5\nagrid = LinRange(0,pars.μ[1] * pars.T,Ka) #&lt;- is this a reasonable upper bound? We'll find out!\nΠ,ϵgrid = tauchen(pars.ρ,pars.ση,Kϵ)\npars = (;pars...,Ka,agrid,Π,ϵgrid,Kϵ)\n\nu(c,σ) = c^(1-σ) / (1-σ)\n\nfunction solve_max(V,t,iϵ,ia,pars)\n    (;agrid,ϵgrid,Π,σ,Ka,r,β) = pars\n    cash = exp(pars.μ[t] + ϵgrid[iϵ]) + agrid[ia]\n    amax = 0\n    vmax = -Inf\n    loop = true\n    a = 1\n    while loop && a&lt;Ka\n        c = cash - agrid[a] / (1+r)\n        if c&gt;0\n            #@views v = u(c,σ) + β * dot(Π[:,iϵ],V[:,a,t+1])\n            v = u(c,σ)\n            for iϵ′ in axes(V,1)\n                v += β * Π[iϵ′,iϵ] * V[iϵ′,a,t+1]\n            end\n            if v&gt;vmax\n                vmax = v\n                amax = a\n            end\n        else\n            loop = false\n        end\n        a += 1 #&lt;- move one up the grid space\n    end\n    return amax,vmax\nend\n\nsolve_max (generic function with 1 method)\n\n\nNext, a function that uses this max operator to get the value function for all states in a period, \\(t\\), and records the optimal savings policy.\n\nfunction iterate!(V,A,t,pars)\n    for ia in axes(V,2), iϵ in axes(V,1)\n        A[iϵ,ia,t],V[iϵ,ia,t] = solve_max(V,t,iϵ,ia,pars)\n    end\nend\nfunction terminal_values!(V,pars)\n    (;σ,ψ,agrid) = pars\n    for ia in axes(V,2), iϵ in axes(V,1)\n        V[iϵ,ia] = ψ * u(agrid[ia],σ)\n    end\nend\n\nterminal_values! (generic function with 2 methods)\n\n\n\nfunction backward_induction!(V,A,pars)\n    (;ψ,σ,T,agrid) = pars\n    # set the values at T+1 (bequest motives)\n    @views terminal_values!(V[:,:,T+1],pars)\n    for t in reverse(1:T)\n        iterate!(V,A,t,pars)\n    end\nend\n\nbackward_induction! (generic function with 1 method)\n\n\nLet’s check the model solution and time it also.\n\nV = zeros(pars.Kϵ,pars.Ka,pars.T+1)\nA = zeros(Int64,pars.Kϵ,pars.Ka,pars.T)\nbackward_induction!(V,A,pars)\n@time backward_induction!(V,A,pars)\n\n  0.010476 seconds\n\n\nSeems ok. We can plot the policy functions as a sanity check. The plot below shows savings policy at the median wage shock over time at different levels of assets.\n\nusing Plots\n\nplot(1:pars.T,agrid[A[3,1:10:Ka,:]'],legend=false)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can see that the discreteness creates some jumpiness in the policy functions. As I said, other solution methods that use interpolation can be more efficient and will create smoother pictures, but since that is not the focus of this class we will use this simple solution method."
  },
  {
    "objectID": "assignments/Assignment-5.html",
    "href": "assignments/Assignment-5.html",
    "title": "Assignment 5: Estimating the Savings Model",
    "section": "",
    "text": "In this assignment you are going to estimate the life-cycle savings model using indirect inference.\n\nPart 1\nRe-use your code from last week’s assignment to:\n\nLoad the PSID dataset from Arellano, Blundell, and Bonhomme (2018)\nProduce a first stage estimate of the income process parameters \\((\\mu,\\rho,\\sigma_{\\eta})\\).\n\nRecall that we already made a modelling assumption on the ages at which the life-cycle “begins” and “ends”.\n\n\nPart 2\nDescribe which statistics from the data you are going to try to match using the model. Write a function that takes the dataset and calculates these statistics.\n\n\nPart 3\nWrite a function that:\n\nSolves for optimal savings policies given the estimates \\((\\hat{\\mu},\\hat{\\rho},\\hat{\\sigma}_\\eta)\\) and given a guess of the parameters \\(\\beta\\) and \\(\\sigma\\). You can re-use the code from the model description or write your own.\nSimulates a panel dataset of income and savings outcomes \\((y_{n,r,t},a_{n,r,t})_{n=1,r=1,t=1}^{N,R,T}\\) \\(N\\times R\\) panels of length \\(T\\) of income and savings outcomes where \\(R\\) is the number of simulations per observation. Note that you will have to make an assumption on initial conditions for assets. You can assume whatever you like but do be clear.\nCalculates the same statistics from the simulated data as you have calculated from the real data.\n\nNote: it is typically best practice to separate work into individual functions. Divide it up in whatever way makes sense to you.\n\n\nPart 4\nWrite a function that calculates an indirect inference estimate by minimizing the difference between the model’s implied statistics and those in the data. Two hints:\n\nYou should use a minimization routine that does not require a derivative (such as Nelder-Mead), since the objective is likely not differentiable.\nYou will need to fix the seed in your simulation so the objective does not “jump around” unnecessarily.\n\nFor now, feel free to use the identity matrix as a weigting scheme in the indirect inference objective function. If you would prefer to use a different weighting scheme, also feel free.\n\n\n\n\n\nReferences\n\nArellano, Manuel, Richard Blundell, and Stephane Bonhomme. 2018. “Nonlinear Persistence and Partial Insurance: Income and Consumption Dynamics in the PSID.” AEA Papers and Proceedings 108 (May): 281–86. https://doi.org/10.1257/pandp.20181049."
  },
  {
    "objectID": "recitations/recitation-6.html",
    "href": "recitations/recitation-6.html",
    "title": "Recitation 6: Introduction to the Bootstrap",
    "section": "",
    "text": "We are going to try getting bootstrapped standard errors for our estimates of the income process parameters."
  },
  {
    "objectID": "recitations/recitation-6.html#setup-loading-the-data",
    "href": "recitations/recitation-6.html#setup-loading-the-data",
    "title": "Recitation 6: Introduction to the Bootstrap",
    "section": "Setup: Loading the Data",
    "text": "Setup: Loading the Data\nNothing new here relative to previous weeks. We load the data and do some filtering.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\ndata = @chain begin \n    CSV.read(\"../data/abb_aea_data.csv\",DataFrame,missingstring = \"NA\")\n    @select :person :y :tot_assets1 :asset :age :year\n    @subset :age.&gt;=25 :age.&lt;=64\nend\n\n19139×6 DataFrame19114 rows omitted\n\n\n\nRow\nperson\ny\ntot_assets1\nasset\nage\nyear\n\n\n\nInt64\nInt64\nInt64\nFloat64\nInt64\nInt64\n\n\n\n\n1\n17118\n54000\n60000\n0.0\n49\n98\n\n\n2\n12630\n61283\n224000\n39283.0\n59\n98\n\n\n3\n12647\n42300\n28240\n0.0\n38\n98\n\n\n4\n5239\n82275\n7500\n0.0\n56\n98\n\n\n5\n2671\n69501\n48000\n3600.0\n35\n98\n\n\n6\n13027\n68000\n148000\n20000.0\n49\n98\n\n\n7\n6791\n93758\n80000\n160.0\n41\n98\n\n\n8\n6475\n26581\n23300\n0.0\n35\n98\n\n\n9\n18332\n33785\n0\n0.0\n42\n98\n\n\n10\n3856\n55300\n311000\n5300.0\n33\n98\n\n\n11\n19326\n40200\n105250\n0.0\n40\n98\n\n\n12\n21818\n42500\n13000\n0.0\n36\n98\n\n\n13\n7300\n121508\n178000\n10008.0\n59\n98\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n19128\n6617\n115887\n241000\n21346.0\n62\n108\n\n\n19129\n626\n128600\n98000\n0.0\n46\n108\n\n\n19130\n4795\n105000\n-68000\n0.0\n34\n108\n\n\n19131\n3223\n120000\n132000\n0.0\n47\n108\n\n\n19132\n8098\n26527\n4700\n0.0\n37\n108\n\n\n19133\n8954\n144026\n220000\n25.0\n46\n108\n\n\n19134\n12990\n122665\n220000\n0.0\n53\n108\n\n\n19135\n8782\n55000\n69000\n0.0\n31\n108\n\n\n19136\n13059\n42728\n-10000\n0.0\n26\n108\n\n\n19137\n13535\n57000\n0\n0.0\n26\n108\n\n\n19138\n3806\n87000\n74200\n0.0\n26\n108\n\n\n19139\n11085\n74000\n-50000\n0.0\n31\n108\n\n\n\n\n\n\nHere is a function to calculate \\(\\hat{\\mu}_{t}\\):\n\nfunction estimate_mu(data)\n    μ = @chain data begin\n        groupby(:age)\n        @combine :μ = mean(log.(:y))\n        @orderby :age\n        _.μ\n    end\n    return μ\nend\n\nμ_est = estimate_mu(data)\n\n40-element Vector{Float64}:\n 10.760860880321463\n 10.841247443701185\n 10.902095326248284\n 10.91448929395576\n 10.998556154206424\n 10.953465022792393\n 11.100954062096019\n 11.091165981452196\n 11.084065028498708\n 11.063159174262802\n 11.164647085813545\n 11.09762896621344\n 11.0958958847166\n  ⋮\n 11.305097087283851\n 11.30167368879086\n 11.289951332574214\n 11.218984368534626\n 11.22669701047502\n 11.157589262616021\n 11.212633411833302\n 11.139162326045438\n 11.048452223663316\n 11.020306496546475\n 10.955869785866497\n 10.995127001069111\n\n\nA function to calculate residuals and get lagged residuals:\n\nfunction get_resids(data)\n\n    data = @chain data begin\n        groupby(:age)\n        @transform :resid = log.(:y) .- mean(log.(:y))\n    end\n\n    d1 = @chain data begin\n        @select :year :person :resid\n        @transform :year = :year .+ 2\n        @rename :rlag1 = :resid\n    end\n\n    d2 = @chain data begin\n        @select :year :person :resid\n        @transform :year = :year .+ 4\n        @rename :rlag2 = :resid\n    end\n\n    data = @chain data begin\n        innerjoin(d1 , on=[:person,:year])\n        innerjoin(d2 , on=[:person,:year])\n    end\n    return data\nend\n\nget_resids (generic function with 1 method)\n\n\nThen finally a function to calculate the moments that we are going to target in the minimum distance step.\n\nfunction get_moments(data)\n    data = get_resids(data)\n    return [var(data.resid), cov(data.resid,data.rlag1), cov(data.resid,data.rlag2)]\nend\n\nm = get_moments(data)\n\n3-element Vector{Float64}:\n 0.6042229503656219\n 0.3937491407622511\n 0.3537264122186026"
  },
  {
    "objectID": "recitations/recitation-6.html#creating-a-boostrapped-sample",
    "href": "recitations/recitation-6.html#creating-a-boostrapped-sample",
    "title": "Recitation 6: Introduction to the Bootstrap",
    "section": "Creating a boostrapped sample",
    "text": "Creating a boostrapped sample\nRecall from class that we have to be careful to sample from the bootstrap at the right unit level. Since we know that observations at the level of person are correlated, we have to sample on this variable with replacement. Here’s a function to create a bootstrapped panel dataset by sampling individuals with replacement.\n\nfunction draw_bootstrap_sample(data)\n    id = unique(data.person) #&lt;- create a list of all person identifiers in the data\n    N = length(id)\n    id_boot = id[rand(1:N,N)] #&lt;- sample individuals randomly N times with replacement\n    d = DataFrame(person = id_boot, id_boot = 1:N) #&lt;- add a unique identified for each draw (:id_boot)\n    boot_data = innerjoin(d,data,on=:person) \n    @transform!(boot_data,:id_old = :person, :person = :id_boot) #&lt;- since we are creating lags using the person identified, we need this to be unique for each draw.\n    return boot_data\nend\n\ndb = draw_bootstrap_sample(data)\n\n19078×8 DataFrame19053 rows omitted\n\n\n\nRow\nperson\nid_boot\ny\ntot_assets1\nasset\nage\nyear\nid_old\n\n\n\nInt64\nInt64\nInt64\nInt64\nFloat64\nInt64\nInt64\nInt64\n\n\n\n\n1\n4489\n4489\n42300\n28240\n0.0\n38\n98\n12647\n\n\n2\n4158\n4158\n82275\n7500\n0.0\n56\n98\n5239\n\n\n3\n3180\n3180\n69501\n48000\n3600.0\n35\n98\n2671\n\n\n4\n937\n937\n68000\n148000\n20000.0\n49\n98\n13027\n\n\n5\n4278\n4278\n93758\n80000\n160.0\n41\n98\n6791\n\n\n6\n4855\n4855\n93758\n80000\n160.0\n41\n98\n6791\n\n\n7\n590\n590\n26581\n23300\n0.0\n35\n98\n6475\n\n\n8\n3136\n3136\n55300\n311000\n5300.0\n33\n98\n3856\n\n\n9\n4937\n4937\n55300\n311000\n5300.0\n33\n98\n3856\n\n\n10\n2966\n2966\n42500\n13000\n0.0\n36\n98\n21818\n\n\n11\n1300\n1300\n121508\n178000\n10008.0\n59\n98\n7300\n\n\n12\n3010\n3010\n121508\n178000\n10008.0\n59\n98\n7300\n\n\n13\n3144\n3144\n121508\n178000\n10008.0\n59\n98\n7300\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n19067\n2635\n2635\n128600\n98000\n0.0\n46\n108\n626\n\n\n19068\n1170\n1170\n105000\n-68000\n0.0\n34\n108\n4795\n\n\n19069\n3701\n3701\n105000\n-68000\n0.0\n34\n108\n4795\n\n\n19070\n5023\n5023\n105000\n-68000\n0.0\n34\n108\n4795\n\n\n19071\n1703\n1703\n26527\n4700\n0.0\n37\n108\n8098\n\n\n19072\n2410\n2410\n26527\n4700\n0.0\n37\n108\n8098\n\n\n19073\n4654\n4654\n144026\n220000\n25.0\n46\n108\n8954\n\n\n19074\n1207\n1207\n55000\n69000\n0.0\n31\n108\n8782\n\n\n19075\n4153\n4153\n55000\n69000\n0.0\n31\n108\n8782\n\n\n19076\n2238\n2238\n42728\n-10000\n0.0\n26\n108\n13059\n\n\n19077\n3722\n3722\n42728\n-10000\n0.0\n26\n108\n13059\n\n\n19078\n4025\n4025\n42728\n-10000\n0.0\n26\n108\n13059"
  },
  {
    "objectID": "recitations/recitation-6.html#bootstrapping-for-a-weighting-matrix",
    "href": "recitations/recitation-6.html#bootstrapping-for-a-weighting-matrix",
    "title": "Recitation 6: Introduction to the Bootstrap",
    "section": "Bootstrapping for a Weighting Matrix",
    "text": "Bootstrapping for a Weighting Matrix\nSuppose we would like to weight out minimum distance criterion using the inverse of the variance for each statistic. A simple way to compute this is to use the bootstrap. Here is code to do that:\n\nusing Random\nfunction boot_moment_variance(data,B ; seed = 1010)\n    M = zeros(3,B)\n    Random.seed!(seed)\n    for b in axes(M,2)\n        db = draw_bootstrap_sample(data)\n        M[:,b] = get_moments(db)\n    end\n    V = cov(M')\nend\n\nV_mom = boot_moment_variance(data,100)\n\n3×3 Matrix{Float64}:\n 0.00162775   0.000432301  0.000423284\n 0.000432301  0.000378095  0.000195091\n 0.000423284  0.000195091  0.00035493\n\n\nWe could use the inverse of this matrix or just the inverse of the diagonal component."
  },
  {
    "objectID": "recitations/recitation-6.html#bootstrapping-the-parameters",
    "href": "recitations/recitation-6.html#bootstrapping-the-parameters",
    "title": "Recitation 6: Introduction to the Bootstrap",
    "section": "Bootstrapping the Parameters",
    "text": "Bootstrapping the Parameters\nTo bootstrap the parameters we first need to write some simple code to estimate the income process.\n\nusing LinearAlgebra, Optim\nfunction model_moms(ρ,σ)\n    v = σ^2 / (1 - ρ^2)\n    return [v, ρ^2 * v, ρ^4 * v]\nend\n\nfunction min_distance_obj(x,m0,W)\n    m1 = model_moms(x[1],x[2])\n    dm = m1 .- m0\n    return dm' * W * dm\nend\n\nfunction estimate_income_process(data, W)\n    μ = estimate_mu(data)\n    m0 = get_moments(data)\n    res = optimize(x-&gt;min_distance_obj(x,m0,W),[0.9,0.1],Newton(),autodiff=:forward)\n    return (; μ, ρ = res.minimizer[1],ση = res.minimizer[2])\nend\n\nr = estimate_income_process(data,I(3))\n\n(μ = [10.760860880321463, 10.841247443701185, 10.902095326248284, 10.91448929395576, 10.998556154206424, 10.953465022792393, 11.100954062096019, 11.091165981452196, 11.084065028498708, 11.063159174262802  …  11.289951332574214, 11.218984368534626, 11.22669701047502, 11.157589262616021, 11.212633411833302, 11.139162326045438, 11.048452223663316, 11.020306496546475, 10.955869785866497, 10.995127001069111], ρ = 0.8619354766570221, ση = 0.3888428287852483)\n\n\nNow we can apply the exact same routine as before. In this case, let’s return the entire bootstrapped sample of parameters instead of just the variance.\n\nfunction boot_parameters(data,B,W ; seed = 2020)\n    mu_b = zeros(40,B)\n    ρ_b = zeros(B)\n    σ_b = zeros(B)\n    Random.seed!(seed)\n    for b in eachindex(ρ_b)\n        db = draw_bootstrap_sample(data)\n        r = estimate_income_process(db,W)\n        mu_b[:,b] = r.μ\n        ρ_b[b] = r.ρ\n        σ_b[b] = r.ση\n    end\n    return mu_b, ρ_b, σ_b\nend\n\nW_opt = inv(V_mom)\n\nmu_b, ρ_b, σ_b = boot_parameters(data,100,W_opt)\n\n([10.73580760699535 10.816247401707118 … 10.826997992362788 10.823451250771296; 10.80821234096262 10.838035193746032 … 10.838646491668136 10.860638767368634; … ; 10.932575248306065 10.976627931101024 … 10.993294394103312 10.905486358310869; 10.987084615895972 10.949352984020404 … 10.90096845528756 11.097782268019063], [0.8758970696492175, 0.8749031842046412, 0.876266168284345, 0.8842489702570284, 0.8895021308118279, 0.8813061168077119, 0.8674746704022943, 0.8631370148641132, 0.869562261996464, 0.8691779540595619  …  0.873224740029997, 0.8858033021530047, 0.8832392563927576, 0.8705757274838543, 0.8793051161142941, 0.8883241567846492, 0.9232806638603563, 0.871593632447117, 0.8752252490344966, 0.9017572628768009], [0.3631400069730627, 0.34742241728061213, 0.352626326206807, 0.34117177871761845, 0.33614178219528, 0.343324062252806, 0.3863865751127758, 0.3778308056994547, 0.373132407007769, 0.37179238682244176  …  0.37077420410018186, 0.3435244271756953, 0.339663045471818, 0.36043023196833307, 0.3503088163031108, 0.3481738289608236, 0.2622013601425302, 0.35282338167619254, 0.35564318535399464, 0.3018655922610499])\n\n\nThis would give us standard errors:\n\nse_mu = std(mu_b,dims=2)\n@show se_ρ = std(ρ_b)\n@show se_σ = std(σ_b)\n\nse_ρ = std(ρ_b) = 0.015419484415975956\nse_σ = std(σ_b) = 0.029651429799185287\n\n\n0.029651429799185287"
  },
  {
    "objectID": "recitations/recitation-6.html#exercise",
    "href": "recitations/recitation-6.html#exercise",
    "title": "Recitation 6: Introduction to the Bootstrap",
    "section": "Exercise",
    "text": "Exercise\nTry comparing the variance of these estimators using the estimated optimal weighting matrix to the identity matrix. Does it make an appreciable difference to the estimated distributino of the parameter estimates?"
  }
]