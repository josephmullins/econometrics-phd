[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to this course! Here you’ll find details on the syllabus."
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Syllabus",
    "section": "Topics",
    "text": "Topics\n\nIdentification, Credible Inference, and Marschak’s Maxim\nWe formally define identification and discuss (via examples) what people really mean when they talk about identification and credible inference. We use the Generalized Roy Model to compare identification via functional form to nonparametric identification.\nWe introduce Marschak’s Maxim as a guide for doing empirical model-based research.\n\nReading\nThe two survey articles by Keane (2010) (link) and Angrist and Pischke (2010) (link) - although aging - provide two important perspectives on the issues of credible inference in economics. Low and Meghir (2017) provide a nice review of the advantages of the structural approach.\nThe original paper by Marschak (1953) may be of interest. Heckman and Vytlacil (2007) provide a nice discussion of Marschak’s Maxim in the context of policy evaluation. They introduce (Heckman and Vytlacil 2005; Carneiro, Heckman, and Vytlacil 2011) the Marginal Treatment Effect as a tool for thinking about quasi-experimental estimators and policy evaluation.\n\n\n\nExtremum Estimators\nWe introduce the concept of an extremum estimator and discuss conditions under which this estimator has good asymptotic properties, with specific applications to maximum likelihood, minimum distance, and generalized method of moments estimators. We discuss optimal weighting of the relative efficiency properties of these estimators.\n\nReading\nThis section relies heavily on the Newey and McFadden (1994) chapter of the Handbook of Econometrics. Although not necessary, Hayashi (2011) provides a very thorough treatment of all of these estimators.\n\n\n\nSimulation Methods\nWe introduce simulation methods for the estimation of structural models, including the Simulated Method of Moments, Indirect Inference, and the Bootstrap method for inference.\n\nReading\nYou may find the Horowitz (2001) handbook chapter useful. Cameron and Trivedi (2005) provide a useful discussion of simulation-based estimators in their textbook.\n\n\n\nPanel Data Methods\nWe talk about individual heterogeneity and discuss the use of panel data for detecting individual heterogeneity in data.\n\n\nDiscrete Choice and Dynamic Discrete Choice\nWe review some of the formalities of discrete choice models and consider estimation of these models in the presence of dynamics."
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\n5 problem sets, each worth 20%."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\nI will provide a link on Canvas to sign up for my weekly office hours."
  },
  {
    "objectID": "recitations/recitation-1.html",
    "href": "recitations/recitation-1.html",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "We are going to work through an example of “identification via functional form” that we will see in class. The model looks as follows:\n\\[ Y = X\\beta + \\alpha D + \\epsilon - \\varphi\\nu \\]\nand\n\\[ D = \\mathbf{1}\\{X\\beta - \\nu \\geq 0\\} \\]\nwhere \\(\\epsilon\\) and \\(\\nu\\) are independent with \\(\\epsilon\\sim\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\) and \\(\\nu\\sim\\mathcal{N}(0,1)\\). This is equivalent to writing:\n\\[ Y = X\\beta + \\alpha D + \\xi \\]\nwith\n\\[ \\left[\\begin{array}{c}\\xi \\\\ \\nu \\end{array}\\right] \\sim \\mathcal{N}\\left(0,\\left[\\begin{array}{cc}\\sigma^2_{\\epsilon} + \\varphi^2 & -\\varphi \\\\ -\\varphi & 1\\end{array}\\right]\\right).\\]\nLet’s start by writing some code to simulate data from this simple selection model:\n\nusing Random, Distributions\n\nfunction sim_data(X ; β,γ,α,φ,σ_ϵ)\n    N = size(X,1)\n    ν = rand(Normal(),N)\n    ϵ = σ_ϵ * rand(Normal(),N)\n    D = (X * γ .- ν) .&gt; 0\n    Y = X * β .+ α * D .+ ϵ .- φ*ν\n    return Y, D\nend\n\nsim_data (generic function with 1 method)\n\n\nLet’s quickly test this function by selecting some default parameters.\n\nN = 1000\nX = [ones(N) 2*rand(Normal(),N)]\nβ = [0., 1.]\nγ = [0.1, 0.5]\nφ = 1.\nα = 0.6\nσ_ϵ = 0.5\n\nY, D = sim_data(X ; β,γ,α,φ,σ_ϵ)\n\n([1.132860301951354, -3.6466921358145283, -2.7966645365786507, 1.074252425709782, 5.251260632435156, 5.333302836895356, 1.048140172519132, -0.012316247760416288, 1.0187652704332761, 3.398234585007554  …  3.1520102965904524, 6.306622263324812, 3.810961616754668, 0.6280107123658936, 0.23612972322392936, -0.735663499435032, 3.6357910280620107, 1.5955063606600457, 2.4628872110429034, 1.1654458069287195], Bool[1, 0, 0, 1, 1, 1, 1, 0, 1, 1  …  1, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n\n\nTo start, let’s think about estimating the parameters \\(\\gamma\\) using maximum likelihood. We will establish the properties of this estimator in class. Here let’s just focus on numerically how to attack the minimization problem. The log-likelihood of the data D given X is:\n\\[ \\mathcal{L}(\\gamma) = \\sum_{n}l(D_{n}; X_{n},\\gamma) = \\sum_{n=1}^{N}D_{n}\\log(\\Phi(X\\gamma)) + (1-D_{n})\\log(1-\\Phi(X\\gamma)) \\]\nLet’s write up this likelihood function.\n\nfunction log_likelihood(D,X,γ)\n    ll = 0.\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            ll += log(cdf(Normal(),xg))\n        else\n            ll += log(1-cdf(Normal(),xg))\n        end\n    end\n    return ll\nend\nlog_likelihood(D,X,[0.,0.])\n\n-693.1471805599322\n\n\nA necessary condition for the maximum likelihood estimator is:\n\\[ \\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_{n}s_{n}(\\hat{\\gamma}) = 0\\]\nwhere\n\\[s_{n}(\\gamma) = \\frac{\\partial l(D_{n} ; X_{n},\\gamma)}{\\partial \\gamma} \\]\nis often referred to as the “score” of the likelihood.\n\n\nOptimization is most efficient when we have access to the first and second order derivatives of the function. There is a general class of hill-climbing (or descent in the case of minimization) algorithms that find new guesses \\(\\gamma_{k+1}\\) given \\(\\gamma_{k}\\) as:\n\\[ \\gamma_{k+1} = \\gamma_{k} + \\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\gamma} \\]\nwhere \\(Q\\) is the function being maximized (or minimized). \\(A_{k}\\) defines a direction in which to search (providing weights on the derivatives) and \\(\\lambda_{k}\\) is a scalar variable known as a step-size which is often calculated optimally in each iteration \\(k\\). For Newton’s method, the matrix \\(A_{k}\\) is the inverse of the Hessian of the objective function \\(Q\\). Since the hessian can sometimes be expensive to calculate, other methods use approximations to the Hessian that are cheaper to compute.\nSince we have a simple model, we can calculate derivatives relatively easily. However julia and many other languages now provide packages for Automatic Differentiation, which essentially trace all of the operations inside the function and implement the chain rule. It is very quick! Below we’ll compare a hard-coded derivative to this automatic differentiation.\n\nusing ForwardDiff\n\nfunction deriv_ll(D,X,γ)\n    dll = zeros(2)\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            dl = pdf(Normal(),xg) / cdf(Normal(),xg)\n        else\n            dl = - pdf(Normal(),xg) / (1 - cdf(Normal(),xg))\n        end\n        dll[1] += X[n,1] * dl\n        dll[2] += X[n,2] * dl            \n    end\n    return dll\nend\n\nauto_deriv_ll(D,X,γ) = ForwardDiff.gradient(x-&gt;log_likelihood(D,X,x),γ)\n\nd1 = deriv_ll(D,X,γ)\nd2 = auto_deriv_ll(D,X,γ)\n[d1 d2]\n\n2×2 Matrix{Float64}:\n -31.4069  -31.4069\n -29.5423  -29.5423\n\n\nOk so we’re confident that these functions work as intended, but how do they compare in performance?\n\n@time deriv_ll(D,X,γ);\n@time auto_deriv_ll(D,X,γ);\n\n  0.000028 seconds (1 allocation: 80 bytes)\n  0.000043 seconds (6 allocations: 352 bytes)\n\n\nBoth are quite quick but you can see that we’re not losing much with automatic differentiation. In my experience, the gap between the two methods can narrow for more complicated functions.\n\n\n\nSo now let’s try implementing the maximum likelihood estimator using two different gradient-based algorithms: Newton’s Method (which uses the Hessian), and the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm (which updates search direction using chanegs in the first derivative). For maximum likelihood there is also the Berndt–Hall–Hall–Hausman (BHHH) algorithm which uses the following equality that we will derive in class:\n\\[ \\mathbb{E}[\\nabla{\\gamma'}s(\\gamma)] = -\\mathbb{E}[s(\\gamma)s(\\gamma)'] \\]\nThis formula states that the hessian of the log-likelihood is equal to the negative covariance of the score. This is a method that only works when maximizing the log-likelihood. Unfortunately Optim does not provide an implementation of BHHH but it is worth knowing about.\nWhile Newton’s method requires calculation of the Hessian (second derivatives), BFGS and related methods only require first derivatives. Typically, this makes each iteration quicker but will take more time to converge. Let’s test them.\n\nusing Optim\nmin_objective(x) = -log_likelihood(D,X,x) #&lt;- Optim assumes that we will minimize a function, hence the negative\nγ_guess = zeros(2)\nprintln(\" ---- Using Newton's Method ------ \")\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nprintln(\" ---- Using BFGS ------ \")\nres2 = optimize(min_objective,γ_guess,BFGS(),autodiff=:forward,Optim.Options(show_trace=true))\n[res1.minimizer res2.minimizer γ]\n\n ---- Using Newton's Method ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 4.696846008300781e-5\n     1     5.170501e+02     1.388497e+02\n * time: 0.0003650188446044922\n     2     5.074227e+02     2.790597e+00\n * time: 0.0006308555603027344\n     3     5.074194e+02     1.179917e-04\n * time: 0.0008649826049804688\n     4     5.074194e+02     9.096168e-12\n * time: 0.0010650157928466797\n ---- Using BFGS ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 7.295608520507812e-5\n     1     5.238897e+02     1.767467e+02\n * time: 0.00039386749267578125\n     2     5.236553e+02     1.761320e+02\n * time: 0.0006258487701416016\n     3     5.076641e+02     2.366312e+01\n * time: 0.0008478164672851562\n     4     5.074194e+02     7.988743e-02\n * time: 0.0010349750518798828\n     5     5.074194e+02     4.225729e-03\n * time: 0.0012378692626953125\n     6     5.074194e+02     7.219726e-08\n * time: 0.0014328956604003906\n     7     5.074194e+02     2.127742e-13\n * time: 0.0016629695892333984\n\n\n2×3 Matrix{Float64}:\n 0.0335542  0.0335542  0.1\n 0.472962   0.472962   0.5\n\n\n\n\n\nIn class we will see that\n\\[ \\mathbb{E}[Y|X,D] = X\\beta + \\alpha D - \\varphi\\left[(1-D)\\frac{\\phi(X\\gamma)}{1-\\Phi(X\\gamma)} - D\\frac{\\phi(X\\gamma)}{\\Phi(X\\gamma)}\\right] \\]\nwhich allows \\(\\beta\\) and \\(\\alpha\\) to be estimated by virtue of a functional form assumption on the distribution of unobservables. Numerically we can test this by adding the selection correction term to a regression of \\(Y\\) on \\(X\\) and \\(D\\):\n\nusing LinearAlgebra\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\n4×2 Matrix{Float64}:\n  0.313929   0.0\n  1.10537    1.0\n -0.14238    0.6\n -1.44073   -1.0\n\n\nAt first this seems way off, but if we increase the sample size by an order of magnitude we should (depending on sampling error) see that the estimator is consistent:\n\nN = 10000\nX = [ones(N) 2*rand(Normal(),N)]\nY, D = sim_data(X ; β,γ,α,φ,σ_ϵ)\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\nIter     Function value   Gradient norm \n     0     6.931472e+03     8.736635e+03\n * time: 7.987022399902344e-5\n     1     5.159514e+03     1.376363e+03\n * time: 0.0021448135375976562\n     2     5.054556e+03     3.380701e+01\n * time: 0.00448298454284668\n     3     5.054503e+03     2.126288e-03\n * time: 0.005650043487548828\n     4     5.054503e+03     2.817037e-10\n * time: 0.00681304931640625\n\n\n4×2 Matrix{Float64}:\n -0.0458329   0.0\n  0.987815    1.0\n  0.654354    0.6\n -0.960434   -1.0"
  },
  {
    "objectID": "recitations/recitation-1.html#numerical-optimization-and-automatic-differentiation",
    "href": "recitations/recitation-1.html#numerical-optimization-and-automatic-differentiation",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "Optimization is most efficient when we have access to the first and second order derivatives of the function. There is a general class of hill-climbing (or descent in the case of minimization) algorithms that find new guesses \\(\\gamma_{k+1}\\) given \\(\\gamma_{k}\\) as:\n\\[ \\gamma_{k+1} = \\gamma_{k} + \\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\gamma} \\]\nwhere \\(Q\\) is the function being maximized (or minimized). \\(A_{k}\\) defines a direction in which to search (providing weights on the derivatives) and \\(\\lambda_{k}\\) is a scalar variable known as a step-size which is often calculated optimally in each iteration \\(k\\). For Newton’s method, the matrix \\(A_{k}\\) is the inverse of the Hessian of the objective function \\(Q\\). Since the hessian can sometimes be expensive to calculate, other methods use approximations to the Hessian that are cheaper to compute.\nSince we have a simple model, we can calculate derivatives relatively easily. However julia and many other languages now provide packages for Automatic Differentiation, which essentially trace all of the operations inside the function and implement the chain rule. It is very quick! Below we’ll compare a hard-coded derivative to this automatic differentiation.\n\nusing ForwardDiff\n\nfunction deriv_ll(D,X,γ)\n    dll = zeros(2)\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            dl = pdf(Normal(),xg) / cdf(Normal(),xg)\n        else\n            dl = - pdf(Normal(),xg) / (1 - cdf(Normal(),xg))\n        end\n        dll[1] += X[n,1] * dl\n        dll[2] += X[n,2] * dl            \n    end\n    return dll\nend\n\nauto_deriv_ll(D,X,γ) = ForwardDiff.gradient(x-&gt;log_likelihood(D,X,x),γ)\n\nd1 = deriv_ll(D,X,γ)\nd2 = auto_deriv_ll(D,X,γ)\n[d1 d2]\n\n2×2 Matrix{Float64}:\n -31.4069  -31.4069\n -29.5423  -29.5423\n\n\nOk so we’re confident that these functions work as intended, but how do they compare in performance?\n\n@time deriv_ll(D,X,γ);\n@time auto_deriv_ll(D,X,γ);\n\n  0.000028 seconds (1 allocation: 80 bytes)\n  0.000043 seconds (6 allocations: 352 bytes)\n\n\nBoth are quite quick but you can see that we’re not losing much with automatic differentiation. In my experience, the gap between the two methods can narrow for more complicated functions."
  },
  {
    "objectID": "recitations/recitation-1.html#numerical-optimization-using-optim",
    "href": "recitations/recitation-1.html#numerical-optimization-using-optim",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "So now let’s try implementing the maximum likelihood estimator using two different gradient-based algorithms: Newton’s Method (which uses the Hessian), and the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm (which updates search direction using chanegs in the first derivative). For maximum likelihood there is also the Berndt–Hall–Hall–Hausman (BHHH) algorithm which uses the following equality that we will derive in class:\n\\[ \\mathbb{E}[\\nabla{\\gamma'}s(\\gamma)] = -\\mathbb{E}[s(\\gamma)s(\\gamma)'] \\]\nThis formula states that the hessian of the log-likelihood is equal to the negative covariance of the score. This is a method that only works when maximizing the log-likelihood. Unfortunately Optim does not provide an implementation of BHHH but it is worth knowing about.\nWhile Newton’s method requires calculation of the Hessian (second derivatives), BFGS and related methods only require first derivatives. Typically, this makes each iteration quicker but will take more time to converge. Let’s test them.\n\nusing Optim\nmin_objective(x) = -log_likelihood(D,X,x) #&lt;- Optim assumes that we will minimize a function, hence the negative\nγ_guess = zeros(2)\nprintln(\" ---- Using Newton's Method ------ \")\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nprintln(\" ---- Using BFGS ------ \")\nres2 = optimize(min_objective,γ_guess,BFGS(),autodiff=:forward,Optim.Options(show_trace=true))\n[res1.minimizer res2.minimizer γ]\n\n ---- Using Newton's Method ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 4.696846008300781e-5\n     1     5.170501e+02     1.388497e+02\n * time: 0.0003650188446044922\n     2     5.074227e+02     2.790597e+00\n * time: 0.0006308555603027344\n     3     5.074194e+02     1.179917e-04\n * time: 0.0008649826049804688\n     4     5.074194e+02     9.096168e-12\n * time: 0.0010650157928466797\n ---- Using BFGS ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.072234e+02\n * time: 7.295608520507812e-5\n     1     5.238897e+02     1.767467e+02\n * time: 0.00039386749267578125\n     2     5.236553e+02     1.761320e+02\n * time: 0.0006258487701416016\n     3     5.076641e+02     2.366312e+01\n * time: 0.0008478164672851562\n     4     5.074194e+02     7.988743e-02\n * time: 0.0010349750518798828\n     5     5.074194e+02     4.225729e-03\n * time: 0.0012378692626953125\n     6     5.074194e+02     7.219726e-08\n * time: 0.0014328956604003906\n     7     5.074194e+02     2.127742e-13\n * time: 0.0016629695892333984\n\n\n2×3 Matrix{Float64}:\n 0.0335542  0.0335542  0.1\n 0.472962   0.472962   0.5"
  },
  {
    "objectID": "recitations/recitation-1.html#identification-via-functional-form",
    "href": "recitations/recitation-1.html#identification-via-functional-form",
    "title": "Introduction to Julia, Automatic Differentiation and Optimization",
    "section": "",
    "text": "In class we will see that\n\\[ \\mathbb{E}[Y|X,D] = X\\beta + \\alpha D - \\varphi\\left[(1-D)\\frac{\\phi(X\\gamma)}{1-\\Phi(X\\gamma)} - D\\frac{\\phi(X\\gamma)}{\\Phi(X\\gamma)}\\right] \\]\nwhich allows \\(\\beta\\) and \\(\\alpha\\) to be estimated by virtue of a functional form assumption on the distribution of unobservables. Numerically we can test this by adding the selection correction term to a regression of \\(Y\\) on \\(X\\) and \\(D\\):\n\nusing LinearAlgebra\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\n4×2 Matrix{Float64}:\n  0.313929   0.0\n  1.10537    1.0\n -0.14238    0.6\n -1.44073   -1.0\n\n\nAt first this seems way off, but if we increase the sample size by an order of magnitude we should (depending on sampling error) see that the estimator is consistent:\n\nN = 10000\nX = [ones(N) 2*rand(Normal(),N)]\nY, D = sim_data(X ; β,γ,α,φ,σ_ϵ)\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nγ_est = res1.minimizer\nxg = X * γ_est\nselection_correction = (1 .- D) .* pdf.(Normal(),xg) ./ (1 .- cdf.(Normal(),xg)) .- D .* pdf.(Normal(),xg) ./ cdf.(Normal(),xg)\nX2 = [X D selection_correction]\nb_est = inv(X2' * X2) * X2' * Y\n[b_est [β; α; -φ]]\n\nIter     Function value   Gradient norm \n     0     6.931472e+03     8.736635e+03\n * time: 7.987022399902344e-5\n     1     5.159514e+03     1.376363e+03\n * time: 0.0021448135375976562\n     2     5.054556e+03     3.380701e+01\n * time: 0.00448298454284668\n     3     5.054503e+03     2.126288e-03\n * time: 0.005650043487548828\n     4     5.054503e+03     2.817037e-10\n * time: 0.00681304931640625\n\n\n4×2 Matrix{Float64}:\n -0.0458329   0.0\n  0.987815    1.0\n  0.654354    0.6\n -0.960434   -1.0"
  }
]